{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Using Data Caterer, you have the ability to generate production like data based on any source/target system whether it be a CSV file, database table, etc. anywhere you want the data to be. Whether it be in a test environment or even in your local laptop. Just define your data source connections and data will be generated. It can also be manually altered to produce data or scenarios the way you want.</p> <p>Main features of the data generator include:</p> <ul> <li>Ability to gather metadata about data sources</li> <li>Generate data in either batch or real-time</li> <li>Maintain referential integrity across generated data</li> <li>Create custom data generation scenarios</li> <li>Delete generated data</li> </ul> <p></p>"},{"location":"advanced/advanced/","title":"Advanced use cases","text":""},{"location":"advanced/advanced/#special-data-formats","title":"Special data formats","text":"<p>There are many options available for you to use when you have a scenario when data has to be a certain format.</p> <ol> <li>Create expression datafaker<ol> <li>Can be used to create names, addresses, or anything that can be found    under here</li> </ol> </li> <li>Create regex</li> </ol>"},{"location":"advanced/advanced/#foreign-keys-across-data-sets","title":"Foreign keys across data sets","text":"<p>If you have a use case where you require a columns value to match in another data set, this can be achieved in the plan definition. For example, if I have the column <code>account_number</code> in a data source named <code>customer-postgres</code> and column <code>account_id</code> in <code>transaction-cassandra</code>,</p> <pre><code>sinkOptions:\n  foreignKeys:\n    #The foreign key name with naming convention [dataSourceName].[taskName].[columnName]\n    \"customer-postgres.accounts.account_number\":\n      #List of columns to match with same naming convention\n      - \"transaction-cassandra.transactions.account_id\"\n</code></pre> <p>Sample can be found here. You can define any number of foreign key relationships as you want.</p>"},{"location":"advanced/advanced/#edge-cases","title":"Edge cases","text":"<p>For each given data type, there are edge cases which can cause issues when your application processes the data. This can be controlled at a column level by including the following flag in the generator options:</p> <pre><code>fields:\n  - name: \"amount\"\n    type: \"double\"\n    generator:\n      type: \"random\"\n      options:\n        enableEdgeCases: \"true\" \n</code></pre> <p>If you want to know all the possible edge cases for each data type, can check the documentation here.</p>"},{"location":"advanced/advanced/#scenario-testing","title":"Scenario testing","text":"<p>You can create specific scenarios by adjusting the metadata found in the plan and tasks to your liking. For example, if you had two data sources, a Postgres database and a parquet file, and you wanted to save account data into Postgres and transactions related to those accounts into a parquet file. You can alter the <code>status</code> column in the account data to only generate <code>open</code> accounts and define a foreign key between Postgres and parquet to ensure the same <code>account_id</code> is being used. Then in the parquet task, define 1 to 10 transactions per <code>account_id</code> to be generated.</p> <p>Postgres account generation example task Parquet transaction generation example task Plan</p>"},{"location":"advanced/advanced/#storing-plantasks-in-cloud-storage","title":"Storing plan/task(s) in cloud storage","text":"<p>You can generate and store the plan/task files inside either AWS S3, Azure Blob Storage or Google GCS. This can be controlled via configuration set in the <code>application.conf</code> file where you can set something like the below:</p> <pre><code>folders {\n   generatedPlanAndTaskFolderPath = \"s3a://my-bucket/data-caterer/generated\"\n   planFilePath = \"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\"\n   taskFolderPath = \"s3a://my-bucket/data-caterer/generated/task\"\n}\n\nspark {\n    config {\n        ...\n        #S3\n        \"spark.hadoop.fs.s3a.directory.marker.retention\" = \"keep\"\n        \"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" = \"true\"\n        \"spark.hadoop.fs.defaultFS\" = \"s3a://my-bucket\"\n        #can change to other credential providers as shown here\n        #https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n        \"spark.hadoop.fs.s3a.aws.credentials.provider\" = \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n        \"spark.hadoop.fs.s3a.access.key\" = \"access_key\"\n        \"spark.hadoop.fs.s3a.secret.key\" = \"secret_key\"\n   }\n}\n</code></pre>"},{"location":"connections/connections/","title":"Data Source Connections","text":"<p>Details of all the connection configuration supported can be found in the below subsections for each type of connection.</p>"},{"location":"connections/connections/#supported-data-connections","title":"Supported Data Connections","text":"Data Source Type Data Source Database Postgres, MySQL, Cassandra File CSV, JSON, ORC, Parquet Kafka Kafka JMS Solace HTTP GET, PUT, POST, DELETE, PATCH, HEAD, TRACE, OPTIONS <p>All connection details follow the same pattern.</p> <pre><code>&lt;connection format&gt; {\n    &lt;connection name&gt; {\n        &lt;key&gt; = &lt;value&gt;\n    }\n}\n</code></pre> <p>When defining a configuration value that can be defined by a system property or environment variable at runtime, you can define that via the following:</p> <pre><code>url = \"localhost\"\nurl = ${?POSTGRES_URL}\n</code></pre> <p>The above defines that if there is a system property or environment variable named <code>POSTGRES_URL</code>, then that value will be used for the <code>url</code>, otherwise, it will default to <code>localhost</code>.</p>"},{"location":"connections/connections/#example-task-per-data-source","title":"Example task per data source","text":"<p>To find examples of a task for each type of data source, please check out this page.</p>"},{"location":"connections/connections/#file","title":"File","text":"<p>Linked here is a list of generic options that can be included as part of your file data source configuration if required. Links to specific file type configurations can be found below.</p>"},{"location":"connections/connections/#csv","title":"CSV","text":"<pre><code>csv {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?CSV_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for CSV can be found here</p>"},{"location":"connections/connections/#json","title":"JSON","text":"<pre><code>json {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?JSON_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for JSON can be found here</p>"},{"location":"connections/connections/#orc","title":"ORC","text":"<pre><code>orc {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?ORC_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for ORC can be found here</p>"},{"location":"connections/connections/#parquet","title":"Parquet","text":"<pre><code>parquet {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?PARQUET_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for Parquet can be found here</p>"},{"location":"connections/connections/#delta-not-supported-yet","title":"Delta (not supported yet)","text":"<pre><code>delta {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?DELTA_PATH}\n  }\n}\n</code></pre>"},{"location":"connections/connections/#jdbc","title":"JDBC","text":"<p>Follows the same configuration used by Spark as found here. Sample can be found below</p> <pre><code>jdbc {\n    postgres {\n        url = \"jdbc:postgresql://localhost:5432/customer\"\n        url = ${?POSTGRES_URL}\n        user = \"postgres\"\n        user = ${?POSTGRES_USERNAME}\n        password = \"postgres\"\n        password = ${?POSTGRES_PASSWORD}\n        driver = \"org.postgresql.Driver\"\n    }\n}\n</code></pre> <p>Ensure that the user has write permission so it is able to save the table to the target tables.</p> <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre>"},{"location":"connections/connections/#postgres","title":"Postgres","text":""},{"location":"connections/connections/#permissions","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> <pre><code>GRANT SELECT ON information_schema.tables TO &lt; user &gt;;\nGRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\nGRANT SELECT ON information_schema.table_constraints TO &lt; user &gt;;\nGRANT SELECT ON information_schema.constraint_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"connections/connections/#mysql","title":"MySQL","text":""},{"location":"connections/connections/#permissions_1","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> <pre><code>GRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.statistics TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"connections/connections/#cassandra","title":"Cassandra","text":"<p>Follows same configuration as defined by the Spark Cassandra Connector as found here</p> <pre><code>org.apache.spark.sql.cassandra {\n    cassandra {\n        spark.cassandra.connection.host = \"localhost\"\n        spark.cassandra.connection.host = ${?CASSANDRA_HOST}\n        spark.cassandra.connection.port = \"9042\"\n        spark.cassandra.connection.port = ${?CASSANDRA_PORT}\n        spark.cassandra.auth.username = \"cassandra\"\n        spark.cassandra.auth.username = ${?CASSANDRA_USERNAME}\n        spark.cassandra.auth.password = \"cassandra\"\n        spark.cassandra.auth.password = ${?CASSANDRA_PASSWORD}\n    }\n}\n</code></pre> <p>Ensure that the user has write permission so it is able to save the table to the target tables.</p> <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre>"},{"location":"connections/connections/#kafka","title":"Kafka","text":"<p>Define your Kafka bootstrap server to connect and send generated data to corresponding topics. Topic gets set at a step level. Further details can be found here</p> <pre><code>kafka {\n    kafka {\n        kafka.bootstrap.servers = \"localhost:9092\"\n        kafka.bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}\n    }\n}\n</code></pre> <p>When defining your schema for pushing data to Kafka, it follows a specific top level schema. An example can be found here. You can define the key, value, headers, partition or topic by following the linked schema.</p>"},{"location":"connections/connections/#jms","title":"JMS","text":"<p>Uses JNDI lookup to send messages to JMS queue. Ensure that the messaging system you are using has your queue/topic registered via JNDI otherwise a connection cannot be created.</p> <pre><code>jms {\n    solace {\n        initialContextFactory = \"com.solacesystems.jndi.SolJNDIInitialContextFactory\"\n        connectionFactory = \"/jms/cf/default\"\n        url = \"smf://localhost:55555\"\n        url = ${?SOLACE_URL}\n        user = \"admin\"\n        user = ${?SOLACE_USER}\n        password = \"admin\"\n        password = ${?SOLACE_PASSWORD}\n        vpnName = \"default\"\n        vpnName = ${?SOLACE_VPN}\n    }\n}\n</code></pre>"},{"location":"connections/connections/#http","title":"HTTP","text":"<p>Define any username and/or password needed for the HTTP requests. The url is defined in the tasks to allow for generated data to be populated in the url.</p> <pre><code>http {\n    customer_api {\n        user = \"admin\"\n        user = ${?HTTP_USER}\n        password = \"admin\"\n        password = ${?HTTP_PASSWORD}\n    }\n}\n</code></pre>"},{"location":"generators/count/","title":"Record Count","text":"<p>There are options related to controlling the number of records generated that can help in generating the scenarios or data required.</p>"},{"location":"generators/count/#total-count","title":"Total Count","text":"<p>Total count is the simplest as you define the total number of records you require for that particular step. For example, in the below step, it will generate 1000 records for the CSV file  </p> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      total: 1000\n</code></pre>"},{"location":"generators/count/#generated-count","title":"Generated Count","text":"<p>As like most things in data-caterer, the count can be generated based on some metadata. For example, if I wanted to generate between 1000 and 2000 records, I could define that by the below configuration:</p> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      generator:\n        type: \"random\"\n        options:\n          min: 1000\n          max: 2000\n</code></pre>"},{"location":"generators/count/#per-column-count","title":"Per Column Count","text":"<p>When defining a per column count, this allows you to generate records \"per set of columns\". This means that for a given set of columns, it will generate a particular amount of records per combination of values for those columns.  </p> <p>One example of this would be when generating transactions relating to a customer. A customer may be defined by columns <code>account_id, name</code>. A number of transactions would be generated per <code>account_id,name</code>.  </p> <p>You can also use a combination of the above two methods to generate the number of records per column.</p>"},{"location":"generators/count/#total","title":"Total","text":"<p>When defining a total count within the <code>perColumn</code> configuration, it translates to only creating <code>(count.total * count.perColumn.total)</code> records. This is a fixed number of records that will be generated each time, with no variation between runs.</p> <p>In the example below, we have <code>count.total = 1000</code> and <code>count.perColumn.total = 2</code>. Which means that <code>1000 * 2 = 2000</code> records will be generated for this CSV file every time data gets generated.</p> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      total: 1000\n      perColumn:\n        total: 2\n        columnNames:\n          - \"account_id\"\n          - \"name\"\n</code></pre>"},{"location":"generators/count/#generated","title":"Generated","text":"<p>You can also define a generator for the count per column. This can be used in scenarios where you want a variable number of records per set of columns.</p> <p>In the example below, it will generate between <code>(count.total * count.perColumn.generator.options.minValue) = (1000 * 1) = 1000</code> and <code>(count.total * count.perColumn.generator.options.maxValue) = (1000 * 2) = 2000</code> records.</p> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      total: 1000\n      perColumn:\n        columnNames:\n          - \"account_id\"\n          - \"name\"\n        generator:\n          type: \"random\"\n          options:\n            maxValue: 2\n            minValue: 1\n</code></pre>"},{"location":"generators/generators/","title":"Data Generators","text":""},{"location":"generators/generators/#data-types","title":"Data Types","text":"<p>Below is a list of all supported data types for generating data:</p> Data Type Spark Data Type Options Description string StringType minLen, maxLen, expression, enableNull integer IntegerType min, minValue, max, maxValue long LongType min, minValue, max, maxValue short ShortType min, minValue, max, maxValue decimal(precision, scale) DecimalType(precision, scale) min, minValue, max, maxValue double DoubleType min, minValue, max, maxValue float FloatType min, minValue, max, maxValue date DateType min, max, enableNull timestamp TimestampType min, max, enableNull boolean BooleanType binary BinaryType minLen, maxLen, enableNull byte ByteType array ArrayType listMinLen, listMaxLen _ StructType Implicitly supported when a schema is defined for a field"},{"location":"generators/generators/#options","title":"Options","text":""},{"location":"generators/generators/#all-data-types","title":"All data types","text":"<p>Some options are available to use for all types of data generators. Below is the list along with example and descriptions:</p> Option Default Example Description enableEdgeCases false enableEdgeCases: \"true\" Enable/disable generated data to contain edge cases based on the data type. For example, integer data type has edge cases of (Int.MaxValue, Int.MinValue and 0) isUnique false isUnique: \"true\" Enable/disable generated data to be unique for that column. Errors will be thrown when it is unable to generate unique data seed seed: \"1\" Defines the random seed for generating data for that particular column. It will override any seed defined at a global level sql sql: \"CASE WHEN amount &lt; 10 THEN true ELSE false END\" Define any SQL statement for generating that columns value. Computation occurs after all non-SQL fields are generated. This means any columns used in the SQL cannot be based on other SQL generated columns. Data type of generated value from SQL needs to match data type defined for the field"},{"location":"generators/generators/#string","title":"String","text":"Option Default Example Description minLen 1 minLen: \"2\" Ensures that all generated strings have at least length <code>minLen</code> maxLen 10 maxLen: \"15\" Ensures that all generated strings have at most length <code>maxLen</code> expression expression: \"#{Name.name}\" expression:\"#{Address.city}/#{Demographic.maritalStatus}\" Will generate a string based on the faker expression provided. All possible faker expressions can be found here Expression has to be in format <code>#{&lt;faker expression name&gt;}</code> enableNull false enableNull: \"true\" Enable/disable null values being generated <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\")</p>"},{"location":"generators/generators/#numeric","title":"Numeric","text":"<p>For all the numeric data types, there are 4 options to choose from: min, minValue, max and maxValue. Generally speaking, you only need to define one of min or minValue, similarly with max or maxValue. The reason why there are 2 options for each is because of when metadata is automatically gathered, we gather the statistics of the observed min and max values. Also, it will attempt to gather any restriction on the min or max value as defined by the data source (i.e. max value as per database type).</p>"},{"location":"generators/generators/#integerlongshortdecimal","title":"Integer/Long/Short/Decimal","text":"Option Default Example Description minValue 0 minValue: \"2\" Ensures that all generated values are greater than or equal to <code>minValue</code> min 0 min: \"2\" Ensures that all generated values are greater than or equal to <code>min</code>. If <code>minValue</code> is defined, <code>minValue</code> will define the lowest possible generated value maxValue 1000 maxValue: \"25\" Ensures that all generated values are less than or equal to <code>maxValue</code> max 1000 max: \"25\" Ensures that all generated values are less than or equal to <code>maxValue</code>. If <code>maxValue</code> is defined, <code>maxValue</code> will define the largest possible generated value <p>Edge cases Integer: (2147483647, -2147483648, 0) Edge cases Long/Decimal: (9223372036854775807, -9223372036854775808, 0) Edge cases Short: (32767, -32768, 0)</p>"},{"location":"generators/generators/#doublefloat","title":"Double/Float","text":"Option Default Example Description minValue 0.0 minValue: \"2.1\" Ensures that all generated values are greater than or equal to <code>minValue</code> min 0.0 min: \"2.1\" Ensures that all generated values are greater than or equal to <code>min</code>. If <code>minValue</code> is defined, <code>minValue</code> will define the lowest possible generated value maxValue 1000.0 maxValue: \"25.9\" Ensures that all generated values are less than or equal to <code>maxValue</code> max 1000.0 max: \"25.9\" Ensures that all generated values are less than or equal to <code>maxValue</code>. If <code>maxValue</code> is defined, <code>maxValue</code> will define the largest possible generated value <p>Edge cases Double: (+infinity, 1.7976931348623157e+308, 4.9e-324, 0.0, -0.0, -1.7976931348623157e+308, -infinity, NaN) Edge cases Float: (+infinity, 3.4028235e+38, 1.4e-45, 0.0, -0.0, -3.4028235e+38, -infinity, NaN)</p>"},{"location":"generators/generators/#date","title":"Date","text":"Option Default Example Description min now() - 365 days min: \"2023-01-31\" Ensures that all generated values are greater than or equal to <code>min</code> max now() max: \"2023-12-31\" Ensures that all generated values are less than or equal to <code>max</code> enableNull false enableNull: \"true\" Enable/disable null values being generated <p>Edge cases: (0001-01-01, 1582-10-15, 1970-01-01, 9999-12-31) (reference)</p>"},{"location":"generators/generators/#timestamp","title":"Timestamp","text":"Option Default Example Description min now() - 365 days min: \"2023-01-31 23:10:10\" Ensures that all generated values are greater than or equal to <code>min</code> max now() max: \"2023-12-31 23:10:10\" Ensures that all generated values are less than or equal to <code>max</code> enableNull false enableNull: \"true\" Enable/disable null values being generated <p>Edge cases: (0001-01-01 00:00:00, 1582-10-15 23:59:59, 1970-01-01 00:00:00, 9999-12-31 23:59:59)</p>"},{"location":"generators/generators/#binary","title":"Binary","text":"Option Default Example Description minLen 1 minLen: \"2\" Ensures that all generated array of bytes have at least length <code>minLen</code> maxLen 20 maxLen: \"15\" Ensures that all generated array of bytes have at most length <code>maxLen</code> enableNull false enableNull: \"true\" Enable/disable null values being generated <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\", -128, 127)</p>"},{"location":"generators/generators/#list","title":"List","text":"Option Default Example Description listMinLen 0 listMinLen: \"2\" Ensures that all generated lists have at least length <code>listMinLen</code> listMaxLen 5 listMaxLen: \"15\" Ensures that all generated lists have at most length <code>listMaxLen</code> enableNull false enableNull: \"true\" Enable/disable null values being generated"},{"location":"get-started/docker/","title":"Run Data Caterer","text":""},{"location":"get-started/docker/#docker","title":"Docker","text":""},{"location":"get-started/docker/#quick-start","title":"Quick start","text":"<pre><code>git clone git@github.com:pflooky/data-caterer-docs.git\ncd docs/sample/docker\nDATA_SOURCE=postgres docker-compose up -d datacaterer\n</code></pre> <p>You can change <code>DATA_SOURCE</code> to one of the following:</p> <ul> <li>postgres</li> <li>mysql</li> <li>cassandra</li> <li>solace</li> <li>kafka</li> </ul> <p>If you want to test it out with your own setup, you can alter the corresponding files under docs/sample/docker/data</p>"},{"location":"get-started/docker/#run-with-multiple-data-sources-postgres-and-csv-file","title":"Run with multiple data sources (Postgres and CSV File)","text":"<pre><code>PLAN=plan/scenario-based DATA_SOURCE=postgres docker-compose up -d datacaterer\nhead data/custom/csv/transactions/part-00000*\nsample_account=$(head -1 data/custom/csv/transactions/part-00000* | awk -F \",\" '{print $1}')\ndocker exec docker-postgres-1 psql -Upostgres -d customer -c \"SELECT * FROM account.accounts WHERE account_number='$sample_account'\"\n</code></pre> <p>You should be able to see the linked data between Postgres and the CSV file created along with 1 to 10 records per account_id, name combination in the CSV file.</p>"},{"location":"get-started/docker/#run-with-custom-data-sources","title":"Run with custom data sources","text":"<ol> <li>Create/alter plan under <code>data/custom/plan</code></li> <li>Create/alter tasks under <code>data/custom/task</code></li> <li>Define your schemas and generator configurations such as record count</li> <li>Create/alter application configuration <code>data/custom/application.conf</code></li> <li>This is where you define your connection properties and other flags/configurations</li> </ol> <pre><code>DATA_SOURCE=&lt;data source name&gt; docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#generate-plan-and-tasks","title":"Generate plan and tasks","text":"<pre><code>APPLICATION_CONFIG_PATH=/opt/app/custom/application-dvd.conf ENABLE_GENERATE_DATA=false ENABLE_GENERATE_PLAN_AND_TASKS=true DATA_SOURCE=postgresdvd docker-compose up -d datacaterer\ncat data/custom/generated/plan/plan_*\n</code></pre>"},{"location":"get-started/docker/#generate-data-with-record-tracking","title":"Generate data with record tracking","text":"<pre><code>APPLICATION_CONFIG_PATH=/opt/app/custom/application-dvd.conf ENABLE_GENERATE_DATA=true ENABLE_GENERATE_PLAN_AND_TASKS=false ENABLE_RECORD_TRACKING=true DATA_SOURCE=postgresdvd PLAN=generated/plan/$(ls data/custom/generated/plan/ | grep plan | head -1 | awk -F \" \" '{print $NF}' | sed 's/\\.yaml//g') docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#delete-the-generated-data","title":"Delete the generated data","text":"<pre><code>APPLICATION_CONFIG_PATH=/opt/app/custom/application-dvd.conf ENABLE_GENERATE_DATA=false ENABLE_GENERATE_PLAN_AND_TASKS=false ENABLE_DELETE_GENERATED_RECORDS=true DATA_SOURCE=postgresdvd docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#helm","title":"Helm","text":"<p>Link to sample helm on GitHub here</p> <p>Update the configuration to your own data connections and configuration.</p> <pre><code>git clone git@github.com:pflooky/data-caterer-docs.git\nhelm install data-caterer ./data-caterer-docs/helm/data-caterer\n</code></pre>"},{"location":"sample/","title":"Samples","text":"<p>Below are examples of different types of plans and tasks that can be helpful when trying to create your own. You can use these as a template or to search for something related to your particular use case.</p>"},{"location":"sample/#base-concept","title":"Base Concept","text":"<p>The execution of the data generator is based on the concept of plans and tasks. A plan represent the set of tasks that need to be executed,  along with other information that spans across tasks, such as foreign keys between data sources. A task represent the component(s) of a data source and its associated metadata so that it understands what the data should look like  and how many steps (sub data sources) there are (i.e. tables in a database, topics in Kafka). Tasks can define one or more steps.</p>"},{"location":"sample/#plan","title":"Plan","text":""},{"location":"sample/#foreign-keys","title":"Foreign Keys","text":"<p>Define foreign keys across data sources in your plan to ensure generated data can match Link to associated task 1 Link to associated task 2</p>"},{"location":"sample/#task","title":"Task","text":"Data Source Type Data Source Sample Task Notes Database Postgres Sample Database MySQL Sample Database Cassandra Sample File CSV Sample File JSON Sample Contains nested schemas and use of SQL for generated values File Parquet Sample Partition by year column Kafka Kafka Sample Specific base schema to be used, define headers, key, value, etc. JMS Solace Sample JSON formatted message HTTP PUT Sample JSON formatted PUT body"},{"location":"sample/#configuration","title":"Configuration","text":"<p>Basic configuration</p>"},{"location":"sample/docker/","title":"Data Caterer - Docker Compose","text":"<p>If you want to try out data caterer generating data for various data sources, you do use the following docker-compose file.</p> <p>All you need to do is define which data source you want to run with via a command like below:</p> <pre><code>DATA_SOURCE=postgres docker-compose up -d datacaterer\n</code></pre> <p>You can change <code>DATA_SOURCE</code> to one of the following: - postgres - mysql - cassandra - solace - kafka - http</p>"}]}