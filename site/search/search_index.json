{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Using Data Caterer, you have the ability to generate production like data based on any source/target system whether it be a CSV file, database table, etc. anywhere you want the data to be. Whether it be in a test environment or even in your local laptop. Just define your data source connections and data will be generated. It can also be manually altered to produce data or scenarios the way you want.</p> <p></p>"},{"location":"advanced/advanced/","title":"Advanced use cases","text":""},{"location":"advanced/advanced/#special-data-formats","title":"Special data formats","text":"<p>There are many options available for you to use when you have a scenario when data has to be a certain format.</p> <ol> <li>Create expression datafaker</li> <li>Can be used to create names, addresses, or anything that can be found       under here</li> <li>Create regex</li> </ol>"},{"location":"advanced/advanced/#foreign-keys-across-data-sets","title":"Foreign keys across data sets","text":"<p>If you have a use case where you require a columns value to match in another data set, this can be achieved in the plan definition. For example, if I have the column <code>account_number</code> in a data source named <code>customer-postgres</code> and column <code>account_id</code> in <code>transaction-cassandra</code>,</p> <pre><code>sinkOptions:\n  foreignKeys:\n    #The foreign key name with naming convention [dataSourceName].[taskName].[columnName]\n    \"customer-postgres.accounts.account_number\":\n      #List of columns to match with same naming convention\n      - \"transaction-cassandra.transactions.account_id\"\n</code></pre> <p>Sample can be found here. You can define any number of foreign key relationships as you want.</p>"},{"location":"advanced/advanced/#edge-cases","title":"Edge cases","text":"<p>For each given data type, there are edge cases which can cause issues when your application processes the data. This can be controlled at a column level by including the following flag in the generator options:</p> <pre><code>fields:\n  - name: \"amount\"\n    type: \"double\"\n    generator:\n      type: \"random\"\n      options:\n        enableEdgeCases: \"true\" \n</code></pre> <p>If you want to know all the possible edge cases for each data type, can check the documentation here.</p>"},{"location":"advanced/advanced/#scenario-testing","title":"Scenario testing","text":"<p>You can create specific scenarios by adjusting the metadata found in the plan and tasks to your liking. For example, if you had two data sources, a Postgres database and a parquet file, and you wanted to save account data into Postgres and transactions related to those accounts into a parquet file. You can alter the <code>status</code> column in the account data to only generate <code>open</code> accounts and define a foreign key between Postgres and parquet to ensure the same <code>account_id</code> is being used. Then in the parquet task, define 1 to 10 transactions per <code>account_id</code> to be generated.</p> <p>Postgres account generation example task Parquet transaction generation example task Plan</p>"},{"location":"advanced/advanced/#generating-json-data","title":"Generating JSON data","text":""},{"location":"connections/connections/","title":"Data Source Connections","text":"<p>Details of all the connection configuration supported can be found in the below subsections for each type of connection.</p>"},{"location":"connections/connections/#supported-data-connections","title":"Supported Data Connections","text":"Data Source Type Data Source Database Postgres, MySQL, Cassandra File CSV, JSON, ORC, Parquet, Delta JMS Solace HTTP GET, PUT, POST <p>All connection details follow the same pattern.</p> <pre><code>&lt;connection format&gt; {\n    &lt;connection name&gt; {\n        &lt;key&gt; = &lt;value&gt;\n    }\n}\n</code></pre> <p>When defining a configuration value that can be defined by a system property or environment variable at runtime, you can define that via the following:</p> <pre><code>url = \"localhost\"\nurl = ${?POSTGRES_URL}\n</code></pre> <p>The above defines that if there is a system property or environment variable named <code>POSTGRES_URL</code>, then that value will be used for the <code>url</code>, otherwise, it will default to <code>localhost</code>.</p>"},{"location":"connections/connections/#file","title":"File","text":"<p>Linked here is a list of generic options that can be included as part of your file data source configuration if required. Links to specific file type configurations can be found below.</p>"},{"location":"connections/connections/#csv","title":"CSV","text":"<pre><code>csv {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n  }\n}\n</code></pre> <p>Other available configuration for CSV can be found here</p>"},{"location":"connections/connections/#json","title":"JSON","text":"<pre><code>json {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n  }\n}\n</code></pre> <p>Other available configuration for JSON can be found here</p>"},{"location":"connections/connections/#orc","title":"ORC","text":"<pre><code>orc {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n  }\n}\n</code></pre> <p>Other available configuration for ORC can be found here</p>"},{"location":"connections/connections/#parquet","title":"Parquet","text":"<pre><code>parquet {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n  }\n}\n</code></pre> <p>Other available configuration for Parquet can be found here</p>"},{"location":"connections/connections/#delta","title":"Delta","text":"<pre><code>delta {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n  }\n}\n</code></pre>"},{"location":"connections/connections/#jdbc","title":"JDBC","text":"<p>Follows the same configuration used by Spark as found here. Sample can be found below</p> <pre><code>jdbc {\n    postgres {\n        url = \"jdbc:postgresql://localhost:5432/customer\"\n        url = ${?POSTGRES_URL}\n        user = \"postgres\"\n        user = ${?POSTGRES_USERNAME}\n        password = \"postgres\"\n        password = ${?POSTGRES_PASSWORD}\n        driver = \"org.postgresql.Driver\"\n    }\n}\n</code></pre> <p>Ensure that the user has write permission so it is able to save the table to the target tables.</p> <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre>"},{"location":"connections/connections/#postgres","title":"Postgres","text":""},{"location":"connections/connections/#permissions","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> <pre><code>GRANT SELECT ON information_schema.tables TO &lt; user &gt;;\nGRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\nGRANT SELECT ON information_schema.table_constraints TO &lt; user &gt;;\nGRANT SELECT ON information_schema.constraint_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"connections/connections/#mysql","title":"MySQL","text":""},{"location":"connections/connections/#permissions_1","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> <pre><code>GRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.statistics TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"connections/connections/#cassandra","title":"Cassandra","text":"<p>Follows same configuration as defined by the Spark Cassandra Connector as found here</p> <pre><code>org.apache.spark.sql.cassandra {\n    cassandra {\n        spark.cassandra.connection.host = \"localhost\"\n        spark.cassandra.connection.host = ${?CASSANDRA_HOST}\n        spark.cassandra.connection.port = \"9042\"\n        spark.cassandra.connection.port = ${?CASSANDRA_PORT}\n        spark.cassandra.auth.username = \"cassandra\"\n        spark.cassandra.auth.username = ${?CASSANDRA_USERNAME}\n        spark.cassandra.auth.password = \"cassandra\"\n        spark.cassandra.auth.password = ${?CASSANDRA_PASSWORD}\n    }\n}\n</code></pre> <p>Ensure that the user has write permission so it is able to save the table to the target tables.</p> <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre>"},{"location":"connections/connections/#jms","title":"JMS","text":"<p>Uses JNDI lookup to send messages to JMS queue. Ensure that the messaging system you are using has your queue/topic registered via JNDI otherwise a connection cannot be created.</p> <pre><code>jms {\n    solace {\n        initialContextFactory = \"com.solacesystems.jndi.SolJNDIInitialContextFactory\"\n        connectionFactory = \"/jms/cf/default\"\n        url = \"smf://localhost:55555\"\n        vpnName = \"default\"\n        user = \"admin\"\n        password = \"admin\"  \n    }\n}\n</code></pre>"},{"location":"connections/connections/#http","title":"HTTP","text":"<p>Define a URL to connect to when sending HTTP requests. Later, can have the ability to define generated data as part of the URL.</p> <pre><code>http {\n    customer_api {\n        url = \"http://localhost:80/get\"\n        user = \"admin\"      #optional\n        password = \"admin\"  #optional\n    }\n}\n</code></pre>"},{"location":"generators/count/","title":"Record Count","text":"<p>There are options related to controlling the number of records generated that can help in generating the scenarios or data required.</p>"},{"location":"generators/count/#total-count","title":"Total Count","text":"<p>Total count is the simplest as you define the total number of records you require for that particular step. For example, in the below step, it will generate 1000 records for the CSV file  </p> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      total: 1000\n</code></pre>"},{"location":"generators/count/#generated-count","title":"Generated Count","text":"<p>As like most things in data-caterer, the count can be generated based on some metadata. For example, if I wanted to generate between 1000 and 2000 records, I could define that by the below configuration:</p> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      generator:\n        type: \"random\"\n        options:\n          min: 1000\n          max: 2000\n</code></pre>"},{"location":"generators/count/#per-column-count","title":"Per Column Count","text":"<p>When defining a per column count, this allows you to generate records \"per set of columns\". This means that for a given set of columns, it will generate a particular amount of records per combination of values for those columns.  </p> <p>One example of this would be when generating transactions relating to a customer. A customer may be defined by columns <code>account_id, name</code>. A number of transactions would be generated per <code>account_id,name</code>.  </p> <p>You can also use a combination of the above two methods to generate the number of records per column.</p>"},{"location":"generators/count/#total","title":"Total","text":"<p>When defining a total count within the <code>perColumn</code> configuration, it translates to only creating <code>(count.total * count.perColumn.total)</code> records. This is a fixed number of records that will be generated each time, with no variation between runs.</p> <p>In the example below, we have <code>count.total=1000</code> and <code>count.perColumn.total=2</code>. Which means that <code>1000 * 2=2000</code> records will be generated for this CSV file every time data gets generated.</p> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      total: 1000\n      perColumn:\n        total: 2\n        columnNames:\n          - \"account_id\"\n          - \"name\"\n</code></pre>"},{"location":"generators/count/#generated","title":"Generated","text":"<p>You can also define a generator for the count per column. This can be used in scenarios where you want a variable number of records per set of columns.</p> <p>In the example below, it will generate between <code>(count.total * count.perColumn.generator.options.minValue) = (1000 * 1) = 1000</code> and <code>(count.total * count.perColumn.generator.options.maxValue) = (1000 * 2) = 2000</code> records.</p> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      total: 1000\n      perColumn:\n        columnNames:\n          - \"account_id\"\n          - \"name\"\n        generator:\n          type: \"random\"\n          options:\n            maxValue: 2\n            minValue: 1\n</code></pre>"},{"location":"generators/generators/","title":"Data Generators","text":""},{"location":"generators/generators/#data-types","title":"Data Types","text":"<p>Below is a list of all supported data types for generating data:</p> Data Type Spark Data Type Options Description string StringType minLen, maxLen, expression, enableNull integer IntegerType min, minValue, max, maxValue long LongType min, minValue, max, maxValue short ShortType min, minValue, max, maxValue decimal(precision, scale) DecimalType(precision, scale) min, minValue, max, maxValue double DoubleType min, minValue, max, maxValue float FloatType min, minValue, max, maxValue date DateType min, max, enableNull timestamp TimestampType min, max, enableNull boolean BooleanType binary BinaryType minLen, maxLen, enableNull byte ByteType array ArrayType listMinLen, listMaxLen _ StructType Implicitly supported when a schema is defined for a field"},{"location":"generators/generators/#options","title":"Options","text":""},{"location":"generators/generators/#all-data-types","title":"All data types","text":"<p>Some options are available to use for all types of data generators. Below is the list along with example and descriptions:</p> Option Default Example Description enableEdgeCases false enableEdgeCases: \"true\" Enable/disable generated data to contain edge cases based on the data type. For example, integer data type has edge cases of (Int.MaxValue, Int.MinValue and 0) isUnique false isUnique: \"true\" Enable/disable generated data to be unique for that column. Errors will be thrown when it is unable to generate unique data seed seed: \"1\" Defines the random seed for generating data for that particular column. It will override any seed defined at a global level sql sql: \"CASE WHEN amount &lt; 10 THEN true ELSE false END\" Define any SQL statement for generating that columns value. Computation occurs after all non-SQL fields are generated. This means any columns used in the SQL cannot be based on other SQL generated columns. Data type of generated value from SQL needs to match data type defined for the field"},{"location":"generators/generators/#string","title":"String","text":"Option Default Example Description minLen 1 minLen: \"2\" Ensures that all generated strings have at least length <code>minLen</code> maxLen 10 maxLen: \"15\" Ensures that all generated strings have at most length <code>maxLen</code> expression expression: \"#{Name.name}\" expression:\"#{Address.city}/#{Demographic.maritalStatus}\" Will generate a string based on the faker expression provided. All possible faker expressions can be found here Expression has to be in format <code>#{&lt;faker expression name&gt;}</code> enableNull false enableNull: \"true\" Enable/disable null values being generated <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\")</p>"},{"location":"generators/generators/#numeric","title":"Numeric","text":"<p>For all the numeric data types, there are 4 options to choose from: min, minValue, max and maxValue. Generally speaking, you only need to define one of min or minValue, similarly with max or maxValue. The reason why there are 2 options for each is because of when metadata is automatically gathered, we gather the statistics of the observed min and max values. Also, it will attempt to gather any restriction on the min or max value as defined by the data source (i.e. max value as per database type).</p>"},{"location":"generators/generators/#integerlongshortdecimal","title":"Integer/Long/Short/Decimal","text":"Option Default Example Description minValue 0 minValue: \"2\" Ensures that all generated values are greater than or equal to <code>minValue</code> min 0 min: \"2\" Ensures that all generated values are greater than or equal to <code>min</code>. If <code>minValue</code> is defined, <code>minValue</code> will define the lowest possible generated value maxValue 1000 maxValue: \"25\" Ensures that all generated values are less than or equal to <code>maxValue</code> max 1000 max: \"25\" Ensures that all generated values are less than or equal to <code>maxValue</code>. If <code>maxValue</code> is defined, <code>maxValue</code> will define the largest possible generated value <p>Edge cases Integer: (2147483647, -2147483648, 0) Edge cases Long/Decimal: (9223372036854775807, -9223372036854775808, 0) Edge cases Short: (32767, -32768, 0)</p>"},{"location":"generators/generators/#doublefloat","title":"Double/Float","text":"Option Default Example Description minValue 0.0 minValue: \"2.1\" Ensures that all generated values are greater than or equal to <code>minValue</code> min 0.0 min: \"2.1\" Ensures that all generated values are greater than or equal to <code>min</code>. If <code>minValue</code> is defined, <code>minValue</code> will define the lowest possible generated value maxValue 1000.0 maxValue: \"25.9\" Ensures that all generated values are less than or equal to <code>maxValue</code> max 1000.0 max: \"25.9\" Ensures that all generated values are less than or equal to <code>maxValue</code>. If <code>maxValue</code> is defined, <code>maxValue</code> will define the largest possible generated value <p>Edge cases Double: (+infinity, 1.7976931348623157e+308, 4.9e-324, 0.0, -0.0, -1.7976931348623157e+308, -infinity, NaN) Edge cases Float: (+infinity, 3.4028235e+38, 1.4e-45, 0.0, -0.0, -3.4028235e+38, -infinity, NaN)</p>"},{"location":"generators/generators/#date","title":"Date","text":"Option Default Example Description min now() - 365 days min: \"2023-01-31\" Ensures that all generated values are greater than or equal to <code>min</code> max now() max: \"2023-12-31\" Ensures that all generated values are less than or equal to <code>max</code> enableNull false enableNull: \"true\" Enable/disable null values being generated <p>Edge cases: (0001-01-01, 1582-10-15, 1970-01-01, 9999-12-31) (reference)</p>"},{"location":"generators/generators/#timestamp","title":"Timestamp","text":"Option Default Example Description min now() - 365 days min: \"2023-01-31 23:10:10\" Ensures that all generated values are greater than or equal to <code>min</code> max now() max: \"2023-12-31 23:10:10\" Ensures that all generated values are less than or equal to <code>max</code> enableNull false enableNull: \"true\" Enable/disable null values being generated <p>Edge cases: (0001-01-01 00:00:00, 1582-10-15 23:59:59, 1970-01-01 00:00:00, 9999-12-31 23:59:59)</p>"},{"location":"generators/generators/#binary","title":"Binary","text":"Option Default Example Description minLen 1 minLen: \"2\" Ensures that all generated array of bytes have at least length <code>minLen</code> maxLen 20 maxLen: \"15\" Ensures that all generated array of bytes have at most length <code>maxLen</code> enableNull false enableNull: \"true\" Enable/disable null values being generated <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\", -128, 127)</p>"},{"location":"generators/generators/#list","title":"List","text":"Option Default Example Description listMinLen 0 listMinLen: \"2\" Ensures that all generated lists have at least length <code>listMinLen</code> listMaxLen 5 listMaxLen: \"15\" Ensures that all generated lists have at most length <code>listMaxLen</code> enableNull false enableNull: \"true\" Enable/disable null values being generated"},{"location":"get-started/docker/","title":"Run Data Caterer","text":""},{"location":"get-started/docker/#docker","title":"Docker","text":""},{"location":"get-started/docker/#quick-start","title":"Quick start","text":"<p>Generate 1000 records of JSON data in local file system.</p> <pre><code>mkdir /tmp/datagen\ndocker run -v /tmp/datagen:/opt/app/data-caterer pflookyy/data-caterer:0.1\nhead /tmp/datagen/sample/json/account-gen/part-0000*\n</code></pre>"},{"location":"get-started/docker/#run-with-custom-data-connections","title":"Run with custom data connections","text":"<pre><code>cp sample/conf/application.conf /tmp/datagen\nvi /tmp/datagen/application.conf\ndocker run -v /tmp/datagen:/opt/app/data-caterer -e APPLICATION_CONFIG_PATH=/opt/app/datagen/application.conf pflookyy/data-caterer:0.1\n</code></pre>"},{"location":"get-started/docker/#run-with-plan-and-task-auto-generation","title":"Run with plan and task auto generation","text":""},{"location":"get-started/docker/#run-with-custom-plan-and-tasks","title":"Run with custom plan and task(s)","text":"<ol> <li>Create plan like here</li> <li>Create tasks like here</li> <li>Create application configuration like here</li> </ol> <pre><code>cp sample/conf/application.conf /tmp/datagen\ncp sample/plan/simple-json-plan.yaml /tmp/datagen\ncp -R sample/task/ /tmp/datagen\n#alter plan and task file(s) as required\ndocker run -v /tmp/datagen:/opt/app/data-caterer \\\n  -e APPLICATION_CONFIG_PATH=/opt/app/datagen/application.conf \\\n  -e PLAN_FILE_PATH=/opt/app/datagen/plan/simple-json-plan.yaml \\\n  -e TASK_FOLDER_PATH=/opt/app/datagen/task \\\n  pflookyy/data-caterer:0.1\n</code></pre>"},{"location":"get-started/docker/#helm","title":"Helm","text":"<p>Link to sample helm on Github here</p>"},{"location":"sample/","title":"Samples","text":"<p>Below are examples of different types of plans and tasks that can be helpful when trying to create your own. You can use these as a template or to search for something related to your particular use case.</p>"},{"location":"sample/#plan","title":"Plan","text":""},{"location":"sample/#foreign-keys","title":"Foreign Keys","text":"<p>Define foreign keys across data sources in your plan to ensure generated data can match Link to associated task 1 Link to associated task 2</p>"},{"location":"sample/#task","title":"Task","text":"Data Source Type Data Source Sample Task Notes Database Postgres Sample Database Cassandra Sample File CSV Sample File JSON Sample Contains nested schemas and use of SQL for generated values File Parquet Sample HTTP PUT Sample JSON formatted PUT body JMS Solace Sample JSON formatted message"},{"location":"sample/#configuration","title":"Configuration","text":"<p>Basic configuration</p>"}]}