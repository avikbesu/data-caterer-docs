{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Data Caterer is a metadata driven data generation tool that aids in creating production like data across batch and event data systems. You can then clean up the generated data or run data validations to ensure your systems have ingested it as expected. Use the Java, Scala API, or YAML files to help with setup or customisation that are all run via Docker.</p> <p>Main features of the data generator include:</p> <ul> <li> Metadata discovery</li> <li> Batch or  event data generation</li> <li> Maintain referential integrity across any dataset</li> <li> Create custom data generation scenarios</li> <li> Clean up generated data</li> <li> Validate data</li> </ul>"},{"location":"about/","title":"About","text":"<p>Hi, my name is Peter. I am a independent Software Developer, mainly focussing on data related services. My experience can be found on my LinkedIn.</p> <p>I have created Data Caterer to help serve individuals and companies with data testing. It is a complex area that has many edge cases or intricacies that are hard to summarise or turn into something actionable and repeatable. Through the use of metadata, Data Caterer can help simplify your data testing, simulating production environment data, aid in data debugging, or whatever your data use case may be.</p> <p>Given that it is going to save you and your team time and money, please help in considering financial support. This will help the product grow into a sustainable and feature-full service.</p>"},{"location":"about/#terms-of-service","title":"Terms of service","text":"<p>Terms of service can be found here.</p>"},{"location":"about/#privacy-policy","title":"Privacy policy","text":"<p>Privacy policy can be found here.</p>"},{"location":"pricing/","title":"Pricing","text":"<p>To have access to the paid features of Data Caterer, you can subscribe according to your situation. You will not be  charged by usage. As you continue to subscribe, you will have access to the latest version of Data Caterer as new  bug fixes and features get published.</p>"},{"location":"pricing/#paid-features","title":"Paid Features","text":"<ul> <li> Metadata discovery</li> <li> All data sources (see here for all data sources)</li> <li> Batch and  Event generation</li> <li> Auto generation from data connections</li> <li> Clean up generated data</li> </ul>"},{"location":"pricing/#pricing-table","title":"Pricing Table","text":""},{"location":"pricing/#manage-subscription","title":"Manage Subscription","text":"<p>Manage via this link</p>"},{"location":"pricing/#contact","title":"Contact","text":"<p>Please contact me at <code>peter.flook@data.catering</code> if you have any questions or queries.</p>"},{"location":"get-started/docker/","title":"Run Data Caterer","text":""},{"location":"get-started/docker/#docker","title":"Docker","text":""},{"location":"get-started/docker/#quick-start","title":"Quick start","text":"<p>Ensure you have <code>docker</code> installed and running.</p> <pre><code>git clone git@github.com:pflooky/data-caterer-example.git\n./run.sh\n#check results under docker/sample/report/index.html folder\n</code></pre> <p>To run for another data source, you can run using <code>docker-compose</code> and set <code>DATA_SOURCE</code> like below</p> <pre><code>./gradlew build\ncd docker\nDATA_SOURCE=postgres docker-compose up -d datacaterer\n</code></pre> <p>Can set it to one of the following:</p> <ul> <li>postgres</li> <li>mysql</li> <li>cassandra</li> <li>solace</li> <li>kafka</li> <li>http</li> </ul> <p>If you want to test it out with your own setup, you can alter the corresponding files under docker/data</p>"},{"location":"get-started/docker/#report","title":"Report","text":"<p>Check the report generated under <code>docker/data/custom/report/index.html</code>.</p> <p>Sample report can also be seen here</p>"},{"location":"get-started/docker/#api","title":"API","text":"<p>A Java and Scala API is available to use to create your own data generation scenario. There are various guides you can  follow here.</p> <p>You can check out the example project found here via:</p> <pre><code>git clone git@github.com:pflooky/data-caterer-example.git\n#for Scala example\n#src/main/scala/com/github/pflooky/plan/DocumentationPlanRun.scala\n#for Java example\n#src/main/java/com/github/pflooky/plan/DocumentationJavaPlanRun.java\n</code></pre> JavaScala <pre><code>import com.github.pflooky.datacaterer.api.java.PlanRun;\nimport com.github.pflooky.datacaterer.api.model.ArrayType;\nimport com.github.pflooky.datacaterer.api.model.DateType;\nimport com.github.pflooky.datacaterer.api.model.DoubleType;\nimport com.github.pflooky.datacaterer.api.model.IntegerType;\n\nimport java.sql.Date;\n\npublic class DocumentationJavaPlanRun extends PlanRun {\n{\nvar myJson = json(\"account_info\", \"/tmp/data-caterer/json\")\n.schema(\nfield().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\nfield().name(\"year\").type(IntegerType.instance()).min(2022).max(2023),\nfield().name(\"name\").expression(\"#{Name.name}\"),\nfield().name(\"amount\").type(DoubleType.instance()).max(1000.0),\nfield().name(\"date\").type(DateType.instance()).min(Date.valueOf(\"2022-01-01\")),\nfield().name(\"status\").oneOf(\"open\", \"closed\"),\nfield().name(\"txn_list\").type(ArrayType.instance())\n.arrayMaxLength(2)\n.schema(schema().addFields(\nfield().name(\"id\").regex(\"ID[0-9]{4}\"),\nfield().name(\"date\").type(DateType.instance()).min(Date.valueOf(\"2022-01-01\")),\nfield().name(\"amount\").type(DoubleType.instance()).max(1000.0)\n))\n)\n.count(count().records(100));\n\nexecute(myJson);\n}\n}\n</code></pre> <pre><code>import com.github.pflooky.datacaterer.api.PlanRun\nimport com.github.pflooky.datacaterer.api.model.{ArrayType, DateType, DoubleType, IntegerType}\n\nimport java.sql.Date\n\nclass ExamplePlanRun extends PlanRun {\n\nval jsonTask = json(\"account_info\", \"/tmp/data-caterer/json\")\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"year\").`type`(IntegerType).min(2022),\nfield.name(\"name\").expression(\"#{Name.name}\"),\nfield.name(\"amount\").`type`(DoubleType).max(1000.0),\nfield.name(\"date\").`type`(DateType).min(Date.valueOf(\"2022-01-01\")),\nfield.name(\"status\").oneOf(\"open\", \"closed\"),\nfield.name(\"txn_list\")\n.`type`(ArrayType)\n.schema(schema.addFields(\nfield.name(\"id\"),\nfield.name(\"date\").`type`(DateType).min(Date.valueOf(\"2022-01-01\")),\nfield.name(\"amount\").`type`(DoubleType),\n))\n)\n.count(count.records(100))\n\nexecute(jsonTask)\n}\n</code></pre>"},{"location":"get-started/docker/#run-with-api","title":"Run with API","text":"<pre><code>./run.sh\n#&lt;input class name&gt;\n#check results under docker/sample/report/index.html folder\n</code></pre> <p>Once you have cloned the data-caterer-example repo, it is easiest to run via the <code>run.sh</code> script that helps package up  the API into a jar, then mount it to the docker image, ready to be run. It will prompt you for the class name you want to run for.</p>"},{"location":"get-started/docker/#run-with-multiple-sub-data-sources","title":"Run with multiple sub data sources","text":"<p>In the context of Postgres data sources, tables are sub data sources that data can be generated for.</p> <p>Try to run the following command:</p> <pre><code>PLAN=plan/postgres-multiple-tables docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#run-with-multiple-data-sources","title":"Run with multiple data sources","text":""},{"location":"get-started/docker/#postgres-and-csv-file","title":"Postgres and CSV File","text":"<pre><code>PLAN=plan/scenario-based docker-compose up -d datacaterer\nhead data/custom/csv/transactions/part-00000*\nsample_account=$(head -1 data/custom/csv/transactions/part-00000* | awk -F \",\" '{print $1}')\ndocker exec docker-postgres-1 psql -Upostgres -d customer -c \"SELECT * FROM account.accounts WHERE account_number='$sample_account'\"\n</code></pre> <p>You should be able to see the linked data between Postgres and the CSV file created along with 1 to 10 records per account_id, name combination in the CSV file.</p>"},{"location":"get-started/docker/#run-with-custom-data-sources","title":"Run with custom data sources","text":"<ol> <li>Create/alter plan    under <code>data/custom/plan</code></li> <li>Create/alter tasks    under <code>data/custom/task</code><ol> <li>Define your schemas and generator configurations such as record count</li> </ol> </li> <li>Create/alter application    configuration <code>data/custom/application.conf</code><ol> <li>This is where you define your connection properties and other flags/configurations</li> </ol> </li> </ol> <pre><code>DATA_SOURCE=&lt;data source name&gt; docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#generate-plan-and-tasks","title":"Generate plan and tasks","text":"<pre><code>APPLICATION_CONFIG_PATH=/opt/app/custom/application-dvd.conf ENABLE_GENERATE_DATA=false ENABLE_GENERATE_PLAN_AND_TASKS=true DATA_SOURCE=postgresdvd docker-compose up -d datacaterer\ncat data/custom/generated/plan/plan_*\n</code></pre>"},{"location":"get-started/docker/#generate-data-with-record-tracking","title":"Generate data with record tracking","text":"<pre><code>APPLICATION_CONFIG_PATH=/opt/app/custom/application-dvd.conf ENABLE_GENERATE_DATA=true ENABLE_GENERATE_PLAN_AND_TASKS=false ENABLE_RECORD_TRACKING=true DATA_SOURCE=postgresdvd PLAN=generated/plan/$(ls data/custom/generated/plan/ | grep plan | head -1 | awk -F \" \" '{print $NF}' | sed 's/\\.yaml//g') docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#delete-the-generated-data","title":"Delete the generated data","text":"<pre><code>APPLICATION_CONFIG_PATH=/opt/app/custom/application-dvd.conf ENABLE_GENERATE_DATA=false ENABLE_GENERATE_PLAN_AND_TASKS=false ENABLE_DELETE_GENERATED_RECORDS=true DATA_SOURCE=postgresdvd PLAN=generated/plan/$(ls data/custom/generated/plan/ | grep plan | head -1 | awk -F \" \" '{print $NF}' | sed 's/\\.yaml//g') docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#helm","title":"Helm","text":"<p>Link to sample helm on GitHub here</p> <p>Update the configuration to your own data connections and configuration.</p> <pre><code>git clone git@github.com:pflooky/data-caterer-example.git\nhelm install data-caterer ./data-caterer-example/helm/data-caterer\n</code></pre>"},{"location":"legal/privacy-policy/","title":"Privacy Policy","text":"<p>Last updated September 25, 2023</p>"},{"location":"legal/privacy-policy/#data-caterer-policy-on-privacy-of-customer-personal-information","title":"Data Caterer Policy on Privacy of Customer Personal Information","text":"<p>Peter John Flook is committed to protecting the privacy and security of your personal information obtained by reason of your use of Data Caterer. This policy explains the types of customer personal information we collect, how it is used, and the steps we take to ensure your personal information is handled appropriately.</p>"},{"location":"legal/privacy-policy/#who-is-peter-john-flook","title":"Who is Peter John Flook?","text":"<p>For purposes of this Privacy Policy, \u201cPeter John Flook\u201d means Peter John Flook, the company developing and providing Data Caterer and related websites and services.</p>"},{"location":"legal/privacy-policy/#what-is-personal-information","title":"What is personal information?","text":"<p>Personal information is information that refers to an individual specifically and is recorded in any form. Personal information includes such things as age, income, date of birth, ethnic origin and credit records. Information about individuals contained in the following documents is not considered personal information:</p> <ul> <li>public telephone directories, where the subscriber can refuse to be listed</li> <li>professional and business directories available to the public</li> <li>public registries and court records</li> <li>other publicly available printed and electronic publications</li> </ul>"},{"location":"legal/privacy-policy/#we-are-accountable-to-you","title":"We are accountable to you","text":"<p>Peter John Flook is responsible for all personal information under its control. Our team is accountable for compliance with these privacy and security principles.</p>"},{"location":"legal/privacy-policy/#we-let-you-know-why-we-collect-and-use-your-personal-information-and-get-your-consent","title":"We let you know why we collect and use your personal information and get your consent","text":"<p>Peter John Flook identifies the purpose for which your personal information is collected and will be used or disclosed. If that purpose is not listed below we will do this before or at the time the information is actually being collected. You will be deemed to consent to our use of your personal information for the purpose of:</p> <ul> <li>communicating with you generally</li> <li>processing your purchases</li> <li>processing and keeping track of transactions and reporting back to you</li> <li>protecting against fraud or error</li> <li>providing product and services requested by you</li> <li>recommending products and services that Peter John Flook believes will be of interest and provide value to you</li> <li>fulfilling any other purpose that would be reasonably apparent to the average person at the time we collect it from   you</li> </ul> <p>Otherwise, Peter John Flook will obtain your express consent (by verbal, written or electronic agreement) to collect, use or disclose your personal information. You can change your consent preferences at any time by contacting Peter John Flook (please refer to the \u201cHow to contact us\u201d section below).</p>"},{"location":"legal/privacy-policy/#we-limit-collection-of-your-personal-information","title":"We limit collection of your personal information","text":"<p>Peter John Flook collects only the information required to provide products and services to you. Peter John Flook will collect personal information only by clear, fair and lawful means.</p> <p>We receive and store any information you enter on our website or give us in any other way. You can choose not to provide certain information, but then you might not be able to take advantage of many of our features.</p> <p>Peter John Flook does not receive or store personal content saved to your local device while using Data Caterer.</p> <p>We also receive and store certain types of information whenever you interact with us.</p>"},{"location":"legal/privacy-policy/#information-provided-to-stripe","title":"Information provided to Stripe","text":"<p>All purchases that are made through this site are processed securely and externally by Stripe. Unless you expressly consent otherwise, we do not see or have access to any personal information that you may provide to Stripe, other than information that is required in order to process your order and deliver your purchased items to you (eg, your name, email address and billing/postal address).</p>"},{"location":"legal/privacy-policy/#we-limit-disclosure-and-retention-of-your-personal-information","title":"We limit disclosure and retention of your personal information","text":"<p>Peter John Flook does not disclose personal information to any organization or person for any reason except the following:</p> <p>We employ other companies and individuals to perform functions on our behalf. Examples include fulfilling orders, delivering packages, sending postal mail and e-mail, removing repetitive information from customer lists, analyzing data, providing marketing assistance, processing credit card payments, and providing customer service. They have access to personal information needed to perform their functions, but may not use it for other purposes. We may use service providers located outside of Canada, and, if applicable, your personal information may be processed and stored in other countries and therefore may be subject to disclosure under the laws of those countries. As we continue to develop our business, we might sell or buy stores, subsidiaries, or business units. In such transactions, customer information generally is one of the transferred business assets but remains subject to the promises made in any pre-existing Privacy Notice (unless, of course, the customer consents otherwise). Also, in the unlikely event that Peter John Flook or substantially all of its assets are acquired, customer information of course will be one of the transferred assets. You are deemed to consent to disclosure of your personal information for those purposes. If your personal information is shared with third parties, those third parties are bound by appropriate agreements with Peter John Flook to secure and protect the confidentiality of your personal information.</p> <p>Peter John Flook retains your personal information only as long as it is required for our business relationship or as required by federal and provincial laws.</p>"},{"location":"legal/privacy-policy/#we-keep-your-personal-information-up-to-date-and-accurate","title":"We keep your personal information up to date and accurate","text":"<p>Peter John Flook keeps your personal information up to date, accurate and relevant for its intended use.</p> <p>You may request access to the personal information we have on record in order to review and amend the information, as appropriate. In circumstances where your personal information has been provided by a third party, we will refer you to that party (e.g. credit bureaus). To access your personal information, refer to the \u201cHow to contact us\u201d section below.</p>"},{"location":"legal/privacy-policy/#the-security-of-your-personal-information-is-a-priority-for-peter-john-flook","title":"The security of your personal information is a priority for Peter John Flook","text":"<p>We take steps to safeguard your personal information, regardless of the format in which it is held, including:</p> <p>physical security measures such as restricted access facilities and locked filing cabinets electronic security measures for computerized personal information such as password protection, database encryption and personal identification numbers. We work to protect the security of your information during transmission by using \u201cTransport Layer Security\u201d (TLS) protocol. organizational processes such as limiting access to your personal information to a selected group of individuals contractual obligations with third parties who need access to your personal information requiring them to protect and secure your personal information It\u2019s important for you to protect against unauthorized access to your password and your computer. Be sure to sign off when you\u2019ve finished using any shared computer.</p>"},{"location":"legal/privacy-policy/#what-about-third-party-advertisers-and-links-to-other-websites","title":"What About Third-Party Advertisers and Links to Other Websites?","text":"<p>Our site may include third-party advertising and links to other websites. We do not provide any personally identifiable customer information to these advertisers or third-party websites.</p> <p>These third-party websites and advertisers, or Internet advertising companies working on their behalf, sometimes use technology to send (or \u201cserve\u201d) the advertisements that appear on our website directly to your browser. They automatically receive your IP address when this happens. They may also use cookies, JavaScript, web beacons (also known as action tags or single-pixel gifs), and other technologies to measure the effectiveness of their ads and to personalize advertising content. We do not have access to or control over cookies or other features that they may use, and the information practices of these advertisers and third-party websites are not covered by this Privacy Notice. Please contact them directly for more information about their privacy practices. In addition, the Network Advertising Initiative offers useful information about Internet advertising companies (also called \u201cad networks\u201d or \u201cnetwork advertisers\u201d), including information about how to opt-out of their information collection. You can access the Network Advertising Initiative at http://www.networkadvertising.org.</p>"},{"location":"legal/privacy-policy/#redirection-to-stripe","title":"Redirection to Stripe","text":"<p>In particular, when you submit an order to us, you may be automatically redirected to Stripe in order to complete the required payment. The payment page that is provided by Stripe is not part of this site. As noted above, we are not privy to any of the bank account, credit card or other personal information that you may provide to Stripe, other than information that is required in order to process your order and deliver your purchased items to you (eg, your name, email address and billing/postal address). We recommend that you refer to Stripe\u2019s privacy statement if you would like more information about how Stripe collects and handles your personal information.</p>"},{"location":"legal/privacy-policy/#we-are-open-about-our-privacy-and-security-policy","title":"We are open about our privacy and security policy","text":"<p>We are committed to providing you with understandable and easily available information about our policy and practices related to management of your personal information. This policy and any related information is available at all times on our website, https://pflooky.github.io/data-caterer-docs/ under Privacy or on request. To contact us, refer to the \u201cHow to contact us\u201d section below.</p>"},{"location":"legal/privacy-policy/#we-provide-access-to-your-personal-information-stored-by-peter-john-flook","title":"We provide access to your personal information stored by Peter John Flook","text":"<p>You can request access to your personal information stored by Peter John Flook. To contact us, refer to the \u201cHow to contact us\u201d section below. Upon receiving such a request, Peter John Flook will:</p> <p>inform you about what type of personal information we have on record or in our control, how it is used and to whom it may have been disclosed provide you with access to your information so you can review and verify the accuracy and completeness and request changes to the information make any necessary updates to your personal information We respond to your questions, concerns and complaints about privacy Peter John Flook responds in a timely manner to your questions, concerns and complaints about the privacy of your personal information and our privacy policies and procedures.</p>"},{"location":"legal/privacy-policy/#how-to-contact-us","title":"How to contact us","text":"<ul> <li>by email at <code>peter.flook@data.catering</code></li> </ul> <p>Our business changes constantly, and this privacy notice will change also. We may e-mail periodic reminders of our notices and conditions, unless you have instructed us not to, but you should check our website frequently to see recent changes. We are, however, committed to protecting your information and will never materially change our policies and practices to make them less protective of customer information collected in the past without the consent of affected customers.</p>"},{"location":"legal/terms-of-service/","title":"Terms and Conditions","text":"<p>Last updated: September 25, 2023</p> <p>Please read these terms and conditions carefully before using Our Service.</p>"},{"location":"legal/terms-of-service/#interpretation-and-definitions","title":"Interpretation and Definitions","text":""},{"location":"legal/terms-of-service/#interpretation","title":"Interpretation","text":"<p>The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.</p>"},{"location":"legal/terms-of-service/#definitions","title":"Definitions","text":"<p>For the purposes of these Terms and Conditions:</p> <ul> <li>Application means the software program provided by the Company downloaded by You on any electronic device, named   Data Caterer</li> <li>Application Store means the digital distribution service operated and developed by Docker Inc. (\u201cDocker\u201d) in which   the Application has been downloaded.</li> <li>Affiliate means an entity that controls, is controlled by or is under common control with a party, where \"control\"   means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of   directors or other managing authority.</li> <li>Country refers to: New South Wales, Australia</li> <li>Company (referred to as either \"the Company\", \"We\", \"Us\" or \"Our\" in this Agreement) refers to Peter John Flook (   ABN: 65153160916), 30 Anne William Drive, West Pennant Hills, 2125, NSW, Australia.</li> <li>Device means any device that can access the Service such as a computer, a cellphone or a digital tablet.</li> <li>Service refers to the Application.</li> <li>Terms and Conditions (also referred as \"Terms\") mean these Terms and Conditions that form the entire agreement   between You and the Company regarding the use of the Service.</li> <li>Third-party Social Media Service means any services or content (including data, information, products or services)   provided by a third party that may be displayed, included or made available by the Service.</li> <li>You means the individual accessing or using the Service, or the company, or other legal entity on behalf of which   such individual is accessing or using the Service, as applicable.</li> </ul>"},{"location":"legal/terms-of-service/#acknowledgment","title":"Acknowledgment","text":"<p>These are the Terms and Conditions governing the use of this Service and the agreement that operates between You and the Company. These Terms and Conditions set out the rights and obligations of all users regarding the use of the Service.</p> <p>Your access to and use of the Service is conditioned on Your acceptance of and compliance with these Terms and Conditions. These Terms and Conditions apply to all visitors, users and others who access or use the Service.</p> <p>By accessing or using the Service You agree to be bound by these Terms and Conditions. If You disagree with any part of these Terms and Conditions then You may not access the Service.</p> <p>You represent that you are over the age of 18. The Company does not permit those under 18 to use the Service.</p> <p>Your access to and use of the Service is also conditioned on Your acceptance of and compliance with the Privacy Policy of the Company. Our Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your personal information when You use the Application or the Website and tells You about Your privacy rights and how the law protects You. Please read Our Privacy Policy carefully before using Our Service.</p>"},{"location":"legal/terms-of-service/#links-to-other-websites","title":"Links to Other Websites","text":"<p>Our Service may contain links to third-party websites or services that are not owned or controlled by the Company.</p> <p>The Company has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third party websites or services. You further acknowledge and agree that the Company shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with the use of or reliance on any such content, goods or services available on or through any such websites or services.</p> <p>We strongly advise You to read the terms and conditions and privacy policies of any third-party websites or services that You visit.</p>"},{"location":"legal/terms-of-service/#termination","title":"Termination","text":"<p>We may terminate or suspend Your access immediately, without prior notice or liability, for any reason whatsoever, including without limitation if You breach these Terms and Conditions.</p> <p>Upon termination, Your right to use the Service will cease immediately.</p>"},{"location":"legal/terms-of-service/#limitation-of-liability","title":"Limitation of Liability","text":"<p>Notwithstanding any damages that You might incur, the entire liability of the Company and any of its suppliers under any provision of these Terms and Your exclusive remedy for all the foregoing shall be limited to the amount actually paid by You through the Service or 100 USD if You haven't purchased anything through the Service.</p> <p>To the maximum extent permitted by applicable law, in no event shall the Company or its suppliers be liable for any special, incidental, indirect, or consequential damages whatsoever (including, but not limited to, damages for loss of profits, loss of data or other information, for business interruption, for personal injury, loss of privacy arising out of or in any way related to the use of or inability to use the Service, third-party software and/or third-party hardware used with the Service, or otherwise in connection with any provision of these Terms), even if the Company or any supplier has been advised of the possibility of such damages and even if the remedy fails of its essential purpose.</p> <p>Some states do not allow the exclusion of implied warranties or limitation of liability for incidental or consequential damages, which means that some of the above limitations may not apply. In these states, each party's liability will be limited to the greatest extent permitted by law.</p>"},{"location":"legal/terms-of-service/#as-is-and-as-available-disclaimer","title":"\"AS IS\" and \"AS AVAILABLE\" Disclaimer","text":"<p>The Service is provided to You \"AS IS\" and \"AS AVAILABLE\" and with all faults and defects without warranty of any kind. To the maximum extent permitted under applicable law, the Company, on its own behalf and on behalf of its Affiliates and its and their respective licensors and service providers, expressly disclaims all warranties, whether express, implied, statutory or otherwise, with respect to the Service, including all implied warranties of merchantability, fitness for a particular purpose, title and non-infringement, and warranties that may arise out of course of dealing, course of performance, usage or trade practice. Without limitation to the foregoing, the Company provides no warranty or undertaking, and makes no representation of any kind that the Service will meet Your requirements, achieve any intended results, be compatible or work with any other software, applications, systems or services, operate without interruption, meet any performance or reliability standards or be error free or that any errors or defects can or will be corrected.</p> <p>Without limiting the foregoing, neither the Company nor any of the company's provider makes any representation or warranty of any kind, express or implied: (i) as to the operation or availability of the Service, or the information, content, and materials or products included thereon; (ii) that the Service will be uninterrupted or error-free; (iii) as to the accuracy, reliability, or currency of any information or content provided through the Service; or (iv) that the Service, its servers, the content, or e-mails sent from or on behalf of the Company are free of viruses, scripts, trojan horses, worms, malware, time-bombs or other harmful components.</p> <p>Some jurisdictions do not allow the exclusion of certain types of warranties or limitations on applicable statutory rights of a consumer, so some or all of the above exclusions and limitations may not apply to You. But in such a case the exclusions and limitations set forth in this section shall be applied to the greatest extent enforceable under applicable law.</p>"},{"location":"legal/terms-of-service/#governing-law","title":"Governing Law","text":"<p>The laws of the Country, excluding its conflicts of law rules, shall govern this Terms and Your use of the Service. Your use of the Application may also be subject to other local, state, national, or international laws.</p>"},{"location":"legal/terms-of-service/#disputes-resolution","title":"Disputes Resolution","text":"<p>If You have any concern or dispute about the Service, You agree to first try to resolve the dispute informally by contacting the Company.</p>"},{"location":"legal/terms-of-service/#for-european-union-eu-users","title":"For European Union (EU) Users","text":"<p>If You are a European Union consumer, you will benefit from any mandatory provisions of the law of the country in which you are resident in.</p>"},{"location":"legal/terms-of-service/#united-states-legal-compliance","title":"United States Legal Compliance","text":"<p>You represent and warrant that (i) You are not located in a country that is subject to the United States government embargo, or that has been designated by the United States government as a \"terrorist supporting\" country, and (ii) You are not listed on any United States government list of prohibited or restricted parties.</p>"},{"location":"legal/terms-of-service/#severability-and-waiver","title":"Severability and Waiver","text":""},{"location":"legal/terms-of-service/#severability","title":"Severability","text":"<p>If any provision of these Terms is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect.</p>"},{"location":"legal/terms-of-service/#waiver","title":"Waiver","text":"<p>Except as provided herein, the failure to exercise a right or to require performance of an obligation under these Terms shall not affect a party's ability to exercise such right or require such performance at any time thereafter nor shall the waiver of a breach constitute a waiver of any subsequent breach.</p>"},{"location":"legal/terms-of-service/#translation-interpretation","title":"Translation Interpretation","text":"<p>These Terms and Conditions may have been translated if We have made them available to You on our Service. You agree that the original English text shall prevail in the case of a dispute.</p>"},{"location":"legal/terms-of-service/#changes-to-these-terms-and-conditions","title":"Changes to These Terms and Conditions","text":"<p>We reserve the right, at Our sole discretion, to modify or replace these Terms at any time. If a revision is material We will make reasonable efforts to provide at least 30 days' notice prior to any new terms taking effect. What constitutes a material change will be determined at Our sole discretion.</p> <p>By continuing to access or use Our Service after those revisions become effective, You agree to be bound by the revised terms. If You do not agree to the new terms, in whole or in part, please stop using the website and the Service.</p>"},{"location":"legal/terms-of-service/#contact-us","title":"Contact Us","text":"<p>If you have any questions about these Terms and Conditions, You can contact us:</p> <ul> <li>By email: peter.flook@data.catering</li> </ul>"},{"location":"setup/","title":"Setup","text":"<p>All the configurations and customisation related to Data Caterer can be found under here.</p>"},{"location":"setup/#guide","title":"Guide","text":"<p>If you want a guided tour of using the Java or Scala API, you can follow one of the guides found here.</p>"},{"location":"setup/#specific-configuration","title":"Specific Configuration","text":"<ul> <li> Configurations - Configurations relating to feature flags, folder pathways, metadata   analysis</li> <li> Connections - Explore the data source connections available</li> <li> Generators - Choose and configure the type of generator you want used for   fields</li> <li> Validations - How to validate data to ensure your system is performing as expected</li> <li> Foreign Keys - Define links between data elements across data sources</li> <li> Advanced - Advanced usage of Data Caterer</li> </ul>"},{"location":"setup/configuration/","title":"Configuration","text":"<p>A number of configurations can be made and customised within Data Caterer to help control what gets run and/or where any metadata gets saved.</p> <p>These configurations are defined from within your Java or Scala class via <code>configuration</code> or for YAML file setup, <code>application.conf</code> file as seen  here.</p>"},{"location":"setup/configuration/#flags","title":"Flags","text":"<p>Flags are used to control which processes are executed when you run Data Caterer.</p> Config Default Paid Description <code>enableGenerateData</code> true N Enable/disable data generation <code>enableCount</code> true N Count the number of records generated. Can be disabled to improve performance <code>enableFailOnError</code> true N Whilst saving generated data, if there is an error, it will stop any further data from being generated <code>enableSaveReports</code> true N Enable/disable HTML reports summarising data generated, metadata of data generated (if <code>enableSinkMetadata</code> is enabled) and validation results (if <code>enableValidation</code> is enabled). Sample here <code>enableSinkMetadata</code> true N Run data profiling for the generated data. Shown in HTML reports if <code>enableSaveSinkMetadata</code> is enabled <code>enableValidation</code> false N Run validations as described in plan. Results can be viewed from logs or from HTML report if <code>enableSaveSinkMetadata</code> is enabled. Sample here <code>enableGeneratePlanAndTasks</code> false Y Enable/disable plan and task auto generation based off data source connections <code>enableRecordTracking</code> false Y Enable/disable which data records have been generated for any data source <code>enableDeleteGeneratedRecords</code> false Y Delete all generated records based off record tracking (if <code>enableRecordTracking</code> has been set to true) JavaScalaapplication.conf <pre><code>configuration()\n.enableGenerateData(true)\n.enableCount(true)\n.enableFailOnError(true)\n.enableSaveReports(true)\n.enableSinkMetadata(true)\n.enableValidation(false)\n.enableGeneratePlanAndTasks(false)\n.enableRecordTracking(false)\n.enableDeleteGeneratedRecords(false);\n</code></pre> <pre><code>configuration\n.enableGenerateData(true)\n.enableCount(true)\n.enableFailOnError(true)\n.enableSaveReports(true)\n.enableSinkMetadata(true)\n.enableValidation(false)\n.enableGeneratePlanAndTasks(false)\n.enableRecordTracking(false)\n.enableDeleteGeneratedRecords(false)\n</code></pre> <pre><code>flags {\n  enableCount = false\n  enableCount = ${?ENABLE_COUNT}\n  enableGenerateData = true\n  enableGenerateData = ${?ENABLE_GENERATE_DATA}\n  enableFailOnError = true\n  enableFailOnError = ${?ENABLE_FAIL_ON_ERROR}\n  enableGeneratePlanAndTasks = false\n  enableGeneratePlanAndTasks = ${?ENABLE_GENERATE_PLAN_AND_TASKS}\n  enableRecordTracking = false\n  enableRecordTracking = ${?ENABLE_RECORD_TRACKING}\n  enableDeleteGeneratedRecords = false\n  enableDeleteGeneratedRecords = ${?ENABLE_DELETE_GENERATED_RECORDS}\n}\n</code></pre>"},{"location":"setup/configuration/#folders","title":"Folders","text":"<p>Depending on which flags are enabled, there are folders that get used to save metadata, store HTML reports or track the records generated.</p> <p>These folder pathways can be defined as a cloud storage pathway (i.e. <code>s3a://my-bucket/task</code>).</p> Config Default Paid Description <code>planFilePath</code> /opt/app/plan/customer-create-plan.yaml N Plan file path to use when generating and/or validating data <code>taskFolderPath</code> /opt/app/task N Task folder path that contains all the task files (can have nested directories) <code>validationFolderPath</code> /opt/app/validation N Validation folder path that contains all the validation files (can have nested directories) <code>generatedReportsFolderPath</code> /opt/app/report N Where HTML reports get generated that contain information about data generated along with any validations performed <code>generatedPlanAndTaskFolderPath</code> /tmp Y Folder path where generated plan and task files will be saved <code>recordTrackingFolderPath</code> /opt/app/record-tracking Y Where record tracking parquet files get saved JavaScalaapplication.conf <pre><code>configuration()\n.planFilePath(\"/opt/app/custom/plan/postgres-plan.yaml\")\n.taskFolderPath(\"/opt/app/custom/task\")\n.validationFolderPath(\"/opt/app/custom/validation\")\n.generatedReportsFolderPath(\"/opt/app/custom/report\")\n.generatedPlanAndTaskFolderPath(\"/opt/app/custom/generated\")\n.recordTrackingFolderPath(\"/opt/app/custom/record-tracking\");\n</code></pre> <pre><code>configuration\n.planFilePath(\"/opt/app/custom/plan/postgres-plan.yaml\")\n.taskFolderPath(\"/opt/app/custom/task\")\n.validationFolderPath(\"/opt/app/custom/validation\")\n.generatedReportsFolderPath(\"/opt/app/custom/report\")\n.generatedPlanAndTaskFolderPath(\"/opt/app/custom/generated\")\n.recordTrackingFolderPath(\"/opt/app/custom/record-tracking\")\n</code></pre> <pre><code>folders {\n  planFilePath = \"/opt/app/custom/plan/postgres-plan.yaml\"\n  planFilePath = ${?PLAN_FILE_PATH}\n  taskFolderPath = \"/opt/app/custom/task\"\n  taskFolderPath = ${?TASK_FOLDER_PATH}\n  validationFolderPath = \"/opt/app/custom/validation\"\n  validationFolderPath = ${?VALIDATION_FOLDER_PATH}\n  generatedReportsFolderPath = \"/opt/app/custom/report\"\n  generatedReportsFolderPath = ${?GENERATED_REPORTS_FOLDER_PATH}\n  generatedPlanAndTaskFolderPath = \"/opt/app/custom/generated\"\n  generatedPlanAndTaskFolderPath = ${?GENERATED_PLAN_AND_TASK_FOLDER_PATH}\n  recordTrackingFolderPath = \"/opt/app/custom/record-tracking\"\n  recordTrackingFolderPath = ${?RECORD_TRACKING_FOLDER_PATH}\n}\n</code></pre>"},{"location":"setup/configuration/#metadata","title":"Metadata","text":"<p>When metadata gets generated, there are some configurations that can be altered to help with performance or accuracy related issues. Metadata gets generated from two processes: 1) if <code>enableGeneratePlanAndTasks</code> or 2) if <code>enableSinkMetadata</code> are enabled.</p> <p>During the generation of plan and tasks, data profiling is used to create the metadata for each of the fields defined in the data source. You may face issues if the number of records in the data source is large as data profiling is an expensive task. Similarly, it can be expensive when analysing the generated data if the number of records generated is large.</p> Config Default Paid Description <code>numRecordsFromDataSource</code> 10000 Y Number of records read in from the data source that could be used for data profiling <code>numRecordsForAnalysis</code> 10000 Y Number of records used for data profiling from the records gathered in <code>numRecordsFromDataSource</code> <code>oneOfMinCount</code> 1000 Y Minimum number of records required before considering if a field can be of type <code>oneOf</code> <code>oneOfDistinctCountVsCountThreshold</code> 0.2 Y Threshold ratio to determine if a field is of type <code>oneOf</code> (i.e. a field called <code>status</code> that only contains <code>open</code> or <code>closed</code>. Distinct count = 2, total count = 10, ratio = 2 / 10 = 0.2 therefore marked as <code>oneOf</code>) <code>numGeneratedSamples</code> 10 N Number of sample records from generated data to take. Shown in HTML report JavaScalaapplication.conf <pre><code>configuration()\n.numRecordsFromDataSourceForDataProfiling(10000)\n.numRecordsForAnalysisForDataProfiling(10000)\n.oneOfMinCount(1000)\n.oneOfDistinctCountVsCountThreshold(1000)\n.numGeneratedSamples(10);\n</code></pre> <pre><code>configuration\n.numRecordsFromDataSourceForDataProfiling(10000)\n.numRecordsForAnalysisForDataProfiling(10000)\n.oneOfMinCount(1000)\n.oneOfDistinctCountVsCountThreshold(1000)\n.numGeneratedSamples(10)\n</code></pre> <pre><code>metadata {\n  numRecordsFromDataSource = 10000\n  numRecordsForAnalysis = 10000\n  oneOfMinCount = 1000\n  oneOfDistinctCountVsCountThreshold = 0.2\n  numGeneratedSamples = 10\n}\n</code></pre>"},{"location":"setup/configuration/#generation","title":"Generation","text":"<p>When generating data, you may have some limitations such as limited CPU or memory, large number of data sources, or data sources prone to failure under load. To help alleviate these issues or speed up performance, you can control the number of records that get generated in each batch.</p> Config Default Paid Description <code>numRecordsPerBatch</code> 100000 N Number of records across all data sources to generate per batch <code>numRecordsPerStep</code> N Overrides the count defined in each step with this value if defined (i.e. if set to 1000, for each step, 1000 records will be generated) ScalaScalaapplication.conf <pre><code>configuration()\n.numRecordsPerBatch(100000)\n.numRecordsPerStep(1000);\n</code></pre> <pre><code>configuration\n.numRecordsPerBatch(100000)\n.numRecordsPerStep(1000)\n</code></pre> <pre><code>generation {\n  numRecordsPerBatch = 100000\n  numRecordsPerStep = 1000\n}\n</code></pre>"},{"location":"setup/configuration/#runtime","title":"Runtime","text":"<p>Given Data Caterer uses Spark as the base framework for data processing, you can configure the job as to your  specifications via configuration as seen here.</p> JavaScalaapplication.conf <pre><code>configuration()\n.master(\"local[*]\")\n.runtimeConfig(Map.of(\"spark.driver.cores\", \"5\"))\n.addRuntimeConfig(\"spark.driver.memory\", \"10g\");\n</code></pre> <pre><code>configuration\n.master(\"local[*]\")\n.runtimeConfig(Map(\"spark.driver.cores\" -&gt; \"5\"))\n.addRuntimeConfig(\"spark.driver.memory\" -&gt; \"10g\")\n</code></pre> <pre><code>runtime {\n  master = \"local[*]\"\n  master = ${?DATA_CATERER_MASTER}\n  config {\n    \"spark.driver.cores\" = \"5\"\n    \"spark.driver.memory\" = \"10g\"\n  }\n}\n</code></pre>"},{"location":"setup/advanced/advanced/","title":"Advanced use cases","text":""},{"location":"setup/advanced/advanced/#special-data-formats","title":"Special data formats","text":"<p>There are many options available for you to use when you have a scenario when data has to be a certain format.</p> <ol> <li>Create expression datafaker<ol> <li>Can be used to create names, addresses, or anything that can be found    under here</li> </ol> </li> <li>Create regex</li> </ol>"},{"location":"setup/advanced/advanced/#foreign-keys-across-data-sets","title":"Foreign keys across data sets","text":"<p>Details for how you can configure foreign keys can be found here.</p>"},{"location":"setup/advanced/advanced/#edge-cases","title":"Edge cases","text":"<p>For each given data type, there are edge cases which can cause issues when your application processes the data. This can be controlled at a column level by including the following flag in the generator options:</p> JavaScalaYAML <pre><code>field()\n.name(\"amount\")\n.type(DoubleType.instance())\n.enableEdgeCases(true)\n.edgeCaseProbability(0.1)\n</code></pre> <pre><code>field\n.name(\"amount\")\n.`type`(DoubleType)\n.enableEdgeCases(true)\n.edgeCaseProbability(0.1)\n</code></pre> <pre><code>fields:\n- name: \"amount\"\ntype: \"double\"\ngenerator:\ntype: \"random\"\noptions:\nenableEdgeCases: \"true\"\nedgeCaseProb: 0.1\n</code></pre> <p>If you want to know all the possible edge cases for each data type, can check the documentation here.</p>"},{"location":"setup/advanced/advanced/#scenario-testing","title":"Scenario testing","text":"<p>You can create specific scenarios by adjusting the metadata found in the plan and tasks to your liking. For example, if you had two data sources, a Postgres database and a parquet file, and you wanted to save account data into Postgres and transactions related to those accounts into a parquet file. You can alter the <code>status</code> column in the account data to only generate <code>open</code> accounts and define a foreign key between Postgres and parquet to ensure the same <code>account_id</code> is being used. Then in the parquet task, define 1 to 10 transactions per <code>account_id</code> to be generated.</p> <p>Postgres account generation example task Parquet transaction generation example task Plan</p>"},{"location":"setup/advanced/advanced/#cloud-storage","title":"Cloud storage","text":""},{"location":"setup/advanced/advanced/#data-source","title":"Data source","text":"<p>If you want to save the file types CSV, JSON, Parquet or ORC into cloud storage, you can do so via adding extra configurations. Below is an example for S3.</p> JavaScalaYAML <pre><code>var csvTask = csv(\"my_csv\", \"s3a://my-bucket/csv/accounts\")\n.schema(\nfield().name(\"account_id\"),\n...\n);\n\nvar s3Configuration = configuration()\n.runtimeConfig(Map.of(\n\"spark.hadoop.fs.s3a.directory.marker.retention\", \"keep\",\n\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\", \"true\",\n\"spark.hadoop.fs.defaultFS\", \"s3a://my-bucket\",\n//can change to other credential providers as shown here\n//https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n\"spark.hadoop.fs.s3a.access.key\", \"access_key\",\n\"spark.hadoop.fs.s3a.secret.key\", \"secret_key\"\n));\n\nexecute(s3Configuration, csvTask);\n</code></pre> <pre><code>val csvTask = csv(\"my_csv\", \"s3a://my-bucket/csv/accounts\")\n.schema(\nfield.name(\"account_id\"),\n...\n)\n\nval s3Configuration = configuration\n.runtimeConfig(Map(\n\"spark.hadoop.fs.s3a.directory.marker.retention\" -&gt; \"keep\",\n\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" -&gt; \"true\",\n\"spark.hadoop.fs.defaultFS\" -&gt; \"s3a://my-bucket\",\n//can change to other credential providers as shown here\n//https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n\"spark.hadoop.fs.s3a.aws.credentials.provider\" -&gt; \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n\"spark.hadoop.fs.s3a.access.key\" -&gt; \"access_key\",\n\"spark.hadoop.fs.s3a.secret.key\" -&gt; \"secret_key\"\n))\n\nexecute(s3Configuration, csvTask)\n</code></pre> <pre><code>folders {\ngeneratedPlanAndTaskFolderPath = \"s3a://my-bucket/data-caterer/generated\"\nplanFilePath = \"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\"\ntaskFolderPath = \"s3a://my-bucket/data-caterer/generated/task\"\n}\n\nruntime {\nconfig {\n...\n#S3\n\"spark.hadoop.fs.s3a.directory.marker.retention\" = \"keep\"\n\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" = \"true\"\n\"spark.hadoop.fs.defaultFS\" = \"s3a://my-bucket\"\n#can change to other credential providers as shown here\n#https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n\"spark.hadoop.fs.s3a.aws.credentials.provider\" = \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n\"spark.hadoop.fs.s3a.access.key\" = \"access_key\"\n\"spark.hadoop.fs.s3a.secret.key\" = \"secret_key\"\n}\n}\n</code></pre>"},{"location":"setup/advanced/advanced/#storing-plantasks","title":"Storing plan/task(s)","text":"<p>You can generate and store the plan/task files inside either AWS S3, Azure Blob Storage or Google GCS. This can be controlled via configuration set in the <code>application.conf</code> file where you can set something like the below:</p> JavaScalaYAML <pre><code>configuration()\n.generatedReportsFolderPath(\"s3a://my-bucket/data-caterer/generated\")\n.planFilePath(\"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\")\n.taskFolderPath(\"s3a://my-bucket/data-caterer/generated/task\")\n.runtimeConfig(Map.of(\n\"spark.hadoop.fs.s3a.directory.marker.retention\", \"keep\",\n\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\", \"true\",\n\"spark.hadoop.fs.defaultFS\", \"s3a://my-bucket\",\n//can change to other credential providers as shown here\n//https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n\"spark.hadoop.fs.s3a.access.key\", \"access_key\",\n\"spark.hadoop.fs.s3a.secret.key\", \"secret_key\"\n));\n</code></pre> <pre><code>configuration\n.generatedReportsFolderPath(\"s3a://my-bucket/data-caterer/generated\")\n.planFilePath(\"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\")\n.taskFolderPath(\"s3a://my-bucket/data-caterer/generated/task\")\n.runtimeConfig(Map(\n\"spark.hadoop.fs.s3a.directory.marker.retention\" -&gt; \"keep\",\n\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" -&gt; \"true\",\n\"spark.hadoop.fs.defaultFS\" -&gt; \"s3a://my-bucket\",\n//can change to other credential providers as shown here\n//https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n\"spark.hadoop.fs.s3a.aws.credentials.provider\" -&gt; \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n\"spark.hadoop.fs.s3a.access.key\" -&gt; \"access_key\",\n\"spark.hadoop.fs.s3a.secret.key\" -&gt; \"secret_key\"\n))\n</code></pre> <pre><code>folders {\ngeneratedPlanAndTaskFolderPath = \"s3a://my-bucket/data-caterer/generated\"\nplanFilePath = \"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\"\ntaskFolderPath = \"s3a://my-bucket/data-caterer/generated/task\"\n}\n\nruntime {\nconfig {\n...\n#S3\n\"spark.hadoop.fs.s3a.directory.marker.retention\" = \"keep\"\n\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" = \"true\"\n\"spark.hadoop.fs.defaultFS\" = \"s3a://my-bucket\"\n#can change to other credential providers as shown here\n#https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n\"spark.hadoop.fs.s3a.aws.credentials.provider\" = \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n\"spark.hadoop.fs.s3a.access.key\" = \"access_key\"\n\"spark.hadoop.fs.s3a.secret.key\" = \"secret_key\"\n}\n}\n</code></pre>"},{"location":"setup/connection/connection/","title":"Data Source Connections","text":"<p>Details of all the connection configuration supported can be found in the below subsections for each type of connection.</p> <p>These configurations can be done via API or from configuration. Examples of both are shown for each data source below.</p>"},{"location":"setup/connection/connection/#supported-data-connections","title":"Supported Data Connections","text":"Data Source Type Data Source Paid Database Postgres, MySQL, Cassandra N (Postgres), Y (rest) File CSV, JSON, ORC, Parquet N Kafka Kafka Y JMS Solace Y HTTP GET, PUT, POST, DELETE, PATCH, HEAD, TRACE, OPTIONS Y"},{"location":"setup/connection/connection/#api","title":"API","text":"<p>All connection details require a name. Depending on the data source, you can define additional options which may be used by the driver or connector for connecting to the data source.</p>"},{"location":"setup/connection/connection/#configuration-file","title":"Configuration file","text":"<p>All connection details follow the same pattern.</p> <pre><code>&lt;connection format&gt; {\n    &lt;connection name&gt; {\n        &lt;key&gt; = &lt;value&gt;\n    }\n}\n</code></pre> <p>Overriding configuration</p> <p>When defining a configuration value that can be defined by a system property or environment variable at runtime, you can define that via the following:</p> <pre><code>```\nurl = \"localhost\"\nurl = ${?POSTGRES_URL}\n```\n\nThe above defines that if there is a system property or environment variable named `POSTGRES_URL`, then that value will\nbe used for the `url`, otherwise, it will default to `localhost`.\n</code></pre>"},{"location":"setup/connection/connection/#data-sources","title":"Data sources","text":"<p>To find examples of a task for each type of data source, please check out this page.</p>"},{"location":"setup/connection/connection/#file","title":"File","text":"<p>Linked here is a list of generic options that can be included as part of your file data source configuration if required. Links to specific file type configurations can be found below.</p>"},{"location":"setup/connection/connection/#csv","title":"CSV","text":"JavaScalaapplication.conf <pre><code>csv(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>csv(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>csv {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?CSV_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for CSV can be found here</p>"},{"location":"setup/connection/connection/#json","title":"JSON","text":"JavaScalaapplication.conf <pre><code>json(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>json(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>json {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?JSON_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for JSON can be found here</p>"},{"location":"setup/connection/connection/#orc","title":"ORC","text":"JavaScalaapplication.conf <pre><code>orc(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>orc(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>orc {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?ORC_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for ORC can be found here</p>"},{"location":"setup/connection/connection/#parquet","title":"Parquet","text":"JavaScalaapplication.conf <pre><code>parquet(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>parquet(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>parquet {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?PARQUET_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for Parquet can be found here</p>"},{"location":"setup/connection/connection/#delta-not-supported-yet","title":"Delta (not supported yet)","text":"JavaScalaapplication.conf <pre><code>delta(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>delta(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>delta {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?DELTA_PATH}\n  }\n}\n</code></pre>"},{"location":"setup/connection/connection/#jdbc","title":"JDBC","text":"<p>Follows the same configuration used by Spark as found here. Sample can be found below</p> JavaScalaapplication.conf <pre><code>postgres(\n\"customer_postgres\",                            #name\n\"jdbc:postgresql://localhost:5432/customer\",    #url\n\"postgres\",                                     #username\n\"postgres\"                                      #password\n)\n</code></pre> <pre><code>postgres(\n\"customer_postgres\",                            #name\n\"jdbc:postgresql://localhost:5432/customer\",    #url\n\"postgres\",                                     #username\n\"postgres\"                                      #password\n)\n</code></pre> <pre><code>jdbc {\n    customer_postgres {\n        url = \"jdbc:postgresql://localhost:5432/customer\"\n        url = ${?POSTGRES_URL}\n        user = \"postgres\"\n        user = ${?POSTGRES_USERNAME}\n        password = \"postgres\"\n        password = ${?POSTGRES_PASSWORD}\n        driver = \"org.postgresql.Driver\"\n    }\n}\n</code></pre> <p>Ensure that the user has write permission, so it is able to save the table to the target tables.</p> SQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre>"},{"location":"setup/connection/connection/#postgres","title":"Postgres","text":"<p>Can see example API or Config definition for Postgres connection above.</p>"},{"location":"setup/connection/connection/#permissions","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> SQL Permission Statements <pre><code>GRANT SELECT ON information_schema.tables TO &lt; user &gt;;\nGRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\nGRANT SELECT ON information_schema.table_constraints TO &lt; user &gt;;\nGRANT SELECT ON information_schema.constraint_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"setup/connection/connection/#mysql","title":"MySQL","text":"JavaScalaapplication.conf <pre><code>mysql(\n\"customer_mysql\",                       #name\n\"jdbc:mysql://localhost:3306/customer\", #url\n\"root\",                                 #username\n\"root\"                                  #password\n)\n</code></pre> <pre><code>mysql(\n\"customer_mysql\",                       #name\n\"jdbc:mysql://localhost:3306/customer\", #url\n\"root\",                                 #username\n\"root\"                                  #password\n)\n</code></pre> <pre><code>jdbc {\n    customer_mysql {\n        url = \"jdbc:mysql://localhost:3306/customer\"\n        user = \"root\"\n        password = \"root\"\n        driver = \"com.mysql.cj.jdbc.Driver\"\n    }\n}\n</code></pre>"},{"location":"setup/connection/connection/#permissions_1","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> SQL Permission Statements <pre><code>GRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.statistics TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"setup/connection/connection/#cassandra","title":"Cassandra","text":"<p>Follows same configuration as defined by the Spark Cassandra Connector as found here</p> JavaScalaapplication.conf <pre><code>cassandra(\n\"customer_cassandra\",   #name\n\"localhost:9042\",       #url\n\"cassandra\",            #username\n\"cassandra\",            #password\nMap.of()                #optional additional connection options\n)\n</code></pre> <pre><code>cassandra(\n\"customer_cassandra\",   #name\n\"localhost:9042\",       #url\n\"cassandra\",            #username\n\"cassandra\",            #password\nMap()                #optional additional connection options\n)\n</code></pre> <pre><code>org.apache.spark.sql.cassandra {\n    customer_cassandra {\n        spark.cassandra.connection.host = \"localhost\"\n        spark.cassandra.connection.host = ${?CASSANDRA_HOST}\n        spark.cassandra.connection.port = \"9042\"\n        spark.cassandra.connection.port = ${?CASSANDRA_PORT}\n        spark.cassandra.auth.username = \"cassandra\"\n        spark.cassandra.auth.username = ${?CASSANDRA_USERNAME}\n        spark.cassandra.auth.password = \"cassandra\"\n        spark.cassandra.auth.password = ${?CASSANDRA_PASSWORD}\n    }\n}\n</code></pre>"},{"location":"setup/connection/connection/#permissions_2","title":"Permissions","text":"<p>Ensure that the user has write permission, so it is able to save the table to the target tables.</p> CQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre> <p>Following permissions are required when enabling <code>configuration.enableGeneratePlanAndTasks(true)</code> as it will gather metadata information about tables and columns from the below tables.</p> CQL Permission Statements <pre><code>GRANT SELECT ON system_schema.tables TO &lt;user&gt;;\nGRANT SELECT ON system_schema.columns TO &lt;user&gt;;\n</code></pre>"},{"location":"setup/connection/connection/#kafka","title":"Kafka","text":"<p>Define your Kafka bootstrap server to connect and send generated data to corresponding topics. Topic gets set at a step level. Further details can be found here</p> JavaScalaapplication.conf <pre><code>kafka(\n\"customer_kafka\",   #name\n\"localhost:9092\"    #url\n)\n</code></pre> <pre><code>kafka(\n\"customer_kafka\",   #name\n\"localhost:9092\"    #url\n)\n</code></pre> <pre><code>kafka {\n    customer_kafka {\n        kafka.bootstrap.servers = \"localhost:9092\"\n        kafka.bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}\n    }\n}\n</code></pre> <p>When defining your schema for pushing data to Kafka, it follows a specific top level schema. An example can be found here . You can define the key, value, headers, partition or topic by following the linked schema.</p>"},{"location":"setup/connection/connection/#jms","title":"JMS","text":"<p>Uses JNDI lookup to send messages to JMS queue. Ensure that the messaging system you are using has your queue/topic registered via JNDI otherwise a connection cannot be created.</p> JavaScalaapplication.conf <pre><code>solace(\n\"customer_solace\",                                      #name\n\"smf://localhost:55554\",                                #url\n\"admin\",                                                #username\n\"admin\",                                                #password\n\"default\",                                              #vpn name\n\"/jms/cf/default\",                                      #connection factory\n\"com.solacesystems.jndi.SolJNDIInitialContextFactory\"   #initial context factory\n)\n</code></pre> <pre><code>solace(\n\"customer_solace\",                                      #name\n\"smf://localhost:55554\",                                #url\n\"admin\",                                                #username\n\"admin\",                                                #password\n\"default\",                                              #vpn name\n\"/jms/cf/default\",                                      #connection factory\n\"com.solacesystems.jndi.SolJNDIInitialContextFactory\"   #initial context factory\n)\n</code></pre> <pre><code>jms {\n    customer_solace {\n        initialContextFactory = \"com.solacesystems.jndi.SolJNDIInitialContextFactory\"\n        connectionFactory = \"/jms/cf/default\"\n        url = \"smf://localhost:55555\"\n        url = ${?SOLACE_URL}\n        user = \"admin\"\n        user = ${?SOLACE_USER}\n        password = \"admin\"\n        password = ${?SOLACE_PASSWORD}\n        vpnName = \"default\"\n        vpnName = ${?SOLACE_VPN}\n    }\n}\n</code></pre>"},{"location":"setup/connection/connection/#http","title":"HTTP","text":"<p>Define any username and/or password needed for the HTTP requests. The url is defined in the tasks to allow for generated data to be populated in the url.</p> JavaScalaapplication.conf <pre><code>http(\n\"customer_api\", #name\n\"admin\",        #username\n\"admin\"         #password\n)\n</code></pre> <pre><code>http(\n\"customer_api\", #name\n\"admin\",        #username\n\"admin\"         #password\n)\n</code></pre> <pre><code>http {\n    customer_api {\n        user = \"admin\"\n        user = ${?HTTP_USER}\n        password = \"admin\"\n        password = ${?HTTP_PASSWORD}\n    }\n}\n</code></pre>"},{"location":"setup/foreign-key/foreign-key/","title":"Foreign Keys","text":"<p>Foreign keys can be defined to represent the relationships between datasets where values are required to match for particular columns.</p>"},{"location":"setup/foreign-key/foreign-key/#single-column","title":"Single column","text":"<p>Define a column in one data source to match against another column. Below example shows a <code>postgres</code> data source with two tables, <code>accounts</code> and <code>transactions</code> that have a foreign key for <code>account_id</code>.</p> JavaScalaYAML <pre><code>var postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n.table(\"public.accounts\")\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"name\"),\n...\n);\nvar postgresTxn = postgres(postgresAcc)\n.table(\"public.transactions\")\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"full_name\"),\n...\n);\n\nplan().addForeignKeyRelationship(\npostgresAcc, \"account_id\",\nList.of(Map.entry(postgresTxn, \"account_id\"))\n);\n</code></pre> <pre><code>val postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n.table(\"public.accounts\")\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"name\"),\n...\n)\nval postgresTxn = postgres(postgresAcc)\n.table(\"public.transactions\")\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"full_name\"),\n...\n)\n\nplan.addForeignKeyRelationship(\npostgresAcc, \"account_id\",\nList(postgresTxn -&gt; \"account_id\")\n)\n</code></pre> <pre><code>---\nname: \"postgres_data\"\nsteps:\n- name: \"accounts\"\ntype: \"postgres\"\noptions:\ndbtable: \"account.accounts\"\nschema:\nfields:\n- name: \"account_id\"\n- name: \"name\"\n- name: \"transactions\"\ntype: \"postgres\"\noptions:\ndbtable: \"account.transactions\"\nschema:\nfields:\n- name: \"account_id\"\n- name: \"full_name\"\n---\nname: \"customer_create_plan\"\ndescription: \"Create customers in JDBC\"\ntasks:\n- name: \"postgres_data\"\ndataSourceName: \"my_postgres\"\n\nsinkOptions:\nforeignKeys:\n\"postgres.accounts.account_id\":\n- \"postgres.transactions.account_id\"\n</code></pre>"},{"location":"setup/foreign-key/foreign-key/#multiple-columns","title":"Multiple columns","text":"<p>You may have a scenario where multiple columns need to be aligned. From the same example, we want <code>account_id</code> and <code>name</code> from <code>accounts</code> to match with <code>account_id</code> and <code>full_name</code> to match in <code>transactions</code> respectively.</p> JavaScalaYAML <pre><code>var postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n.table(\"public.accounts\")\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"name\"),\n...\n);\nvar postgresTxn = postgres(postgresAcc)\n.table(\"public.transactions\")\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"full_name\"),\n...\n);\n\nplan().addForeignKeyRelationship(\npostgresAcc, List.of(\"account_id\", \"name\"),\nList.of(Map.entry(postgresTxn, List.of(\"account_id\", \"full_name\")))\n);\n</code></pre> <pre><code>val postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n.table(\"public.accounts\")\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"name\"),\n...\n)\nval postgresTxn = postgres(postgresAcc)\n.table(\"public.transactions\")\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"full_name\"),\n...\n)\n\nplan.addForeignKeyRelationship(\npostgresAcc, List(\"account_id\", \"name\"),\nList(postgresTxn -&gt; List(\"account_id\", \"full_name\"))\n)\n</code></pre> <pre><code>---\nname: \"postgres_data\"\nsteps:\n- name: \"accounts\"\ntype: \"postgres\"\noptions:\ndbtable: \"account.accounts\"\nschema:\nfields:\n- name: \"account_id\"\n- name: \"name\"\n- name: \"transactions\"\ntype: \"postgres\"\noptions:\ndbtable: \"account.transactions\"\nschema:\nfields:\n- name: \"account_id\"\n- name: \"full_name\"\n---\nname: \"customer_create_plan\"\ndescription: \"Create customers in JDBC\"\ntasks:\n- name: \"postgres_data\"\ndataSourceName: \"my_postgres\"\n\nsinkOptions:\nforeignKeys:\n\"my_postgres.accounts.account_id,name\":\n- \"my_postgres.transactions.account_id,full_name\"\n</code></pre>"},{"location":"setup/foreign-key/foreign-key/#nested-column","title":"Nested column","text":"<p>Your schema structure can have nested fields which can also be referenced as foreign keys. But to do so, you need to create a proxy field that gets omitted from the final saved data.</p> <p>In the example below, the nested <code>customer_details.name</code> field inside the <code>json</code> task needs to match with <code>name</code> from <code>postgres</code>. A new field in the <code>json</code> called <code>_txn_name</code> is used as a temporary column to facilitate the foreign key definition.</p> JavaScalaYAML <pre><code>var postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n.table(\"public.accounts\")\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"name\"),\n...\n);\nvar jsonTask = json(\"my_json\", \"/tmp/json\")\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"customer_details\")\n.schema(\nfield().name(\"name\").sql(\"_txn_name\"), #nested field will get value from '_txn_name'\n...\n),\nfield().name(\"_txn_name\").omit(true)       #value will not be included in output\n);\n\nplan().addForeignKeyRelationship(\npostgresAcc, List.of(\"account_id\", \"name\"),\nList.of(Map.entry(jsonTask, List.of(\"account_id\", \"_txn_name\")))\n);\n</code></pre> <pre><code>val postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n.table(\"public.accounts\")\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"name\"),\n...\n)\nvar jsonTask = json(\"my_json\", \"/tmp/json\")\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"customer_details\")\n.schema(\nfield.name(\"name\").sql(\"_txn_name\"), #nested field will get value from '_txn_name'\n...\n), field.name(\"_txn_name\").omit(true)       #value will not be included in output\n)\n\nplan.addForeignKeyRelationship(\npostgresAcc, List(\"account_id\", \"name\"),\nList(jsonTask -&gt; List(\"account_id\", \"_txn_name\"))\n)\n</code></pre> <pre><code>---\n#postgres task yaml\nname: \"postgres_data\"\nsteps:\n- name: \"accounts\"\ntype: \"postgres\"\noptions:\ndbtable: \"account.accounts\"\nschema:\nfields:\n- name: \"account_id\"\n- name: \"name\"\n---\n#json task yaml\nname: \"json_data\"\nsteps:\n- name: \"transactions\"\ntype: \"json\"\noptions:\ndbtable: \"account.transactions\"\nschema:\nfields:\n- name: \"account_id\"\n- name: \"_txn_name\"\ngenerator:\noptions:\nomit: true\n- name: \"cusotmer_details\"\nschema:\nfields:\nname: \"name\"\ngenerator:\ntype: \"sql\"\noptions:\nsql: \"_txn_name\"\n\n---\n#plan yaml\nname: \"customer_create_plan\"\ndescription: \"Create customers in JDBC\"\ntasks:\n- name: \"postgres_data\"\ndataSourceName: \"my_postgres\"\n- name: \"json_data\"\ndataSourceName: \"my_json\"\n\nsinkOptions:\nforeignKeys:\n\"my_postgres.accounts.account_id,name\":\n- \"my_json.transactions.account_id,_txn_name\"\n</code></pre>"},{"location":"setup/generator/count/","title":"Record Count","text":"<p>There are options related to controlling the number of records generated that can help in generating the scenarios or data required.</p>"},{"location":"setup/generator/count/#record-count_1","title":"Record Count","text":"<p>Record count is the simplest as you define the total number of records you require for that particular step. For example, in the below step, it will generate 1000 records for the CSV file  </p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(1000);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(1000)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\ncount:\nrecords: 1000\n</code></pre>"},{"location":"setup/generator/count/#generated-count","title":"Generated Count","text":"<p>As like most things in Data Caterer, the count can be generated based on some metadata. For example, if I wanted to generate between 1000 and 2000 records, I could define that by the below configuration:</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(generator().min(1000).max(2000));\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(generator.min(1000).max(2000))\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\ncount:\ngenerator:\ntype: \"random\"\noptions:\nmin: 1000\nmax: 2000\n</code></pre>"},{"location":"setup/generator/count/#per-column-count","title":"Per Column Count","text":"<p>When defining a per column count, this allows you to generate records \"per set of columns\". This means that for a given set of columns, it will generate a particular amount of records per combination of values for those columns.  </p> <p>One example of this would be when generating transactions relating to a customer, a customer may be defined by columns <code>account_id, name</code>. A number of transactions would be generated per <code>account_id, name</code>.  </p> <p>You can also use a combination of the above two methods to generate the number of records per column.</p>"},{"location":"setup/generator/count/#records","title":"Records","text":"<p>When defining a base number of records within the <code>perColumn</code> configuration, it translates to creating <code>(count.records * count.recordsPerColumn)</code> records. This is a fixed number of records that will be generated each time, with no variation between runs.</p> <p>In the example below, we have <code>count.records = 1000</code> and <code>count.recordsPerColumn = 2</code>. Which means that <code>1000 * 2 = 2000</code> records will be generated in total.</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(\ncount()\n.records(1000)\n.recordsPerColumn(2, \"account_id\", \"name\")\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(\ncount\n.records(1000)\n.recordsPerColumn(2, \"account_id\", \"name\")\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\ncount:\nrecords: 1000\nperColumn:\nrecords: 2\ncolumnNames:\n- \"account_id\"\n- \"name\"\n</code></pre>"},{"location":"setup/generator/count/#generated","title":"Generated","text":"<p>You can also define a generator for the count per column. This can be used in scenarios where you want a variable number of records per set of columns.</p> <p>In the example below, it will generate between <code>(count.records * count.perColumnGenerator.generator.min) = (1000 * 1) = 1000</code> and <code>(count.records * count.perColumnGenerator.generator.max) = (1000 * 2) = 2000</code> records.</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(\ncount()\n.records(1000)\n.recordsPerColumnGenerator(generator().min(1).max(2), \"account_id\", \"name\")\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(\ncount\n.records(1000)\n.recordsPerColumnGenerator(generator.min(1).max(2), \"account_id\", \"name\")\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\ncount:\nrecords: 1000\nperColumn:\ncolumnNames:\n- \"account_id\"\n- \"name\"\ngenerator:\ntype: \"random\"\noptions:\nmin: 1\nmax: 2\n</code></pre>"},{"location":"setup/generator/generator/","title":"Data Generators","text":""},{"location":"setup/generator/generator/#data-types","title":"Data Types","text":"<p>Below is a list of all supported data types for generating data:</p> Data Type Spark Data Type Options Description string StringType <code>minLen, maxLen, expression, enableNull</code> integer IntegerType <code>min, max, stddev, mean</code> long LongType <code>min, max, stddev, mean</code> short ShortType <code>min, max, stddev, mean</code> decimal(precision, scale) DecimalType(precision, scale) <code>min, max, stddev, mean</code> double DoubleType <code>min, max, stddev, mean</code> float FloatType <code>min, max, stddev, mean</code> date DateType <code>min, max, enableNull</code> timestamp TimestampType <code>min, max, enableNull</code> boolean BooleanType binary BinaryType <code>minLen, maxLen, enableNull</code> byte ByteType array ArrayType <code>arrayMinLen, arrayMaxLen, arrayType</code> _ StructType Implicitly supported when a schema is defined for a field"},{"location":"setup/generator/generator/#options","title":"Options","text":""},{"location":"setup/generator/generator/#all-data-types","title":"All data types","text":"<p>Some options are available to use for all types of data generators. Below is the list along with example and descriptions:</p> Option Default Example Description <code>enableEdgeCase</code> false <code>enableEdgeCase: \"true\"</code> Enable/disable generated data to contain edge cases based on the data type. For example, integer data type has edge cases of (Int.MaxValue, Int.MinValue and 0) <code>edgeCaseProbability</code> 0.0 <code>edgeCaseProb: \"0.1\"</code> Probability of generating a random edge case value if <code>enableEdgeCase</code> is true <code>isUnique</code> false <code>isUnique: \"true\"</code> Enable/disable generated data to be unique for that column. Errors will be thrown when it is unable to generate unique data <code>seed</code> <code>seed: \"1\"</code> Defines the random seed for generating data for that particular column. It will override any seed defined at a global level <code>sql</code> <code>sql: \"CASE WHEN amount &lt; 10 THEN true ELSE false END\"</code> Define any SQL statement for generating that columns value. Computation occurs after all non-SQL fields are generated. This means any columns used in the SQL cannot be based on other SQL generated columns. Data type of generated value from SQL needs to match data type defined for the field"},{"location":"setup/generator/generator/#string","title":"String","text":"Option Default Example Description <code>minLen</code> 1 <code>minLen: \"2\"</code> Ensures that all generated strings have at least length <code>minLen</code> <code>maxLen</code> 10 <code>maxLen: \"15\"</code> Ensures that all generated strings have at most length <code>maxLen</code> <code>expression</code> <code>expression: \"#{Name.name}\"</code><code>expression:\"#{Address.city}/#{Demographic.maritalStatus}\"</code> Will generate a string based on the faker expression provided. All possible faker expressions can be found here Expression has to be in format <code>#{&lt;faker expression name&gt;}</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\", \"\u0130yi g\u00fcnler\", \"\u0421\u043f\u0430\u0441\u0438\u0431\u043e\", \"\u039a\u03b1\u03bb\u03b7\u03bc\u03ad\u03c1\u03b1\", \"\u0635\u0628\u0627\u062d \u0627\u0644\u062e\u064a\u0631\", \" F\u00f6rl\u00e5t\", \"\u4f60\u597d\u5417\", \"Nh\u00e0 v\u1ec7 sinh \u1edf \u0111\u00e2u\", \"\u3053\u3093\u306b\u3061\u306f\", \"\u0928\u092e\u0938\u094d\u0924\u0947\", \"\u0532\u0561\u0580\u0565\u0582\", \"\u0417\u0434\u0440\u0430\u0432\u0435\u0439\u0442\u0435\")</p>"},{"location":"setup/generator/generator/#sample","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield()\n.name(\"name\")\n.type(StringType.instance())\n.expression(\"#{Name.name}\")\n.enableNull(true)\n.nullProbability(0.1)\n.minLength(4)\n.maxLength(20)\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield\n.name(\"name\")\n.`type`(StringType)\n.expression(\"#{Name.name}\")\n.enableNull(true)\n.nullProbability(0.1)\n.minLength(4)\n.maxLength(20)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\nschema:\nfields:\n- name: \"name\"\ntype: \"string\"\ngenerator:\noptions:\nexpression: \"#{Name.name}\"\nenableNull: true\nnullProb: 0.1\nminLength: 4\nmaxLength: 20\n</code></pre>"},{"location":"setup/generator/generator/#numeric","title":"Numeric","text":"<p>For all the numeric data types, there are 4 options to choose from: min, max and maxValue. Generally speaking, you only need to define one of min or minValue, similarly with max or maxValue. The reason why there are 2 options for each is because of when metadata is automatically gathered, we gather the statistics of the observed min and max values. Also, it will attempt to gather any restriction on the min or max value as defined by the data source (i.e. max value as per database type).</p>"},{"location":"setup/generator/generator/#integerlongshort","title":"Integer/Long/Short","text":"Option Default Example Description <code>min</code> 0 <code>min: \"2\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> 1000 <code>max: \"25\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>stddev</code> 1.0 <code>stddev: \"2.0\"</code> Standard deviation for normal distributed data <code>mean</code> <code>max - min</code> <code>mean: \"5.0\"</code> Mean for normal distributed data <p>Edge cases Integer: (2147483647, -2147483648, 0) Edge cases Long: (9223372036854775807, -9223372036854775808, 0) Edge cases Short: (32767, -32768, 0)</p>"},{"location":"setup/generator/generator/#sample_1","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"year\").type(IntegerType.instance()).min(2020).max(2023),\nfield().name(\"customer_id\").type(LongType.instance()),\nfield().name(\"customer_group\").type(ShortType.instance())\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"year\").`type`(IntegerType).min(2020).max(2023),\nfield.name(\"customer_id\").`type`(LongType),\nfield.name(\"customer_group\").`type`(ShortType)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"year\"\ntype: \"integer\"\ngenerator:\noptions:\nmin: 2020\nmax: 2023\n- name: \"customer_id\"\ntype: \"long\"\n- name: \"customer_group\"\ntype: \"short\"\n</code></pre>"},{"location":"setup/generator/generator/#decimal","title":"Decimal","text":"Option Default Example Description <code>min</code> 0 <code>min: \"2\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> 1000 <code>max: \"25\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>stddev</code> 1.0 <code>stddev: \"2.0\"</code> Standard deviation for normal distributed data <code>mean</code> <code>max - min</code> <code>mean: \"5.0\"</code> Mean for normal distributed data <code>numericPrecision</code> 10 <code>precision: \"25\"</code> The maximum number of digits <code>numericScale</code> 0 <code>scale: \"25\"</code> The number of digits on the right side of the decimal point (has to be less than or equal to precision) <p>Edge cases Decimal: (9223372036854775807, -9223372036854775808, 0)</p>"},{"location":"setup/generator/generator/#sample_2","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"balance\").type(DecimalType.instance()).numericPrecision(10).numericScale(5)\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"balance\").`type`(DecimalType).numericPrecision(10).numericScale(5)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"balance\"\ntype: \"decimal\"\ngenerator:\noptions:\nprecision: 10\nscale: 5\n</code></pre>"},{"location":"setup/generator/generator/#doublefloat","title":"Double/Float","text":"Option Default Example Description <code>min</code> 0.0 <code>min: \"2.1\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> 1000.0 <code>max: \"25.9\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>stddev</code> 1.0 <code>stddev: \"2.0\"</code> Standard deviation for normal distributed data <code>mean</code> <code>max - min</code> <code>mean: \"5.0\"</code> Mean for normal distributed data <p>Edge cases Double: (+infinity, 1.7976931348623157e+308, 4.9e-324, 0.0, -0.0, -1.7976931348623157e+308, -infinity, NaN) Edge cases Float: (+infinity, 3.4028235e+38, 1.4e-45, 0.0, -0.0, -3.4028235e+38, -infinity, NaN)</p>"},{"location":"setup/generator/generator/#sample_3","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"amount\").type(DoubleType.instance())\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"amount\").`type`(DoubleType)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"amount\"\ntype: \"double\"\n</code></pre>"},{"location":"setup/generator/generator/#date","title":"Date","text":"Option Default Example Description <code>min</code> now() - 365 days <code>min: \"2023-01-31\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> now() <code>max: \"2023-12-31\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (0001-01-01, 1582-10-15, 1970-01-01, 9999-12-31) (reference)</p>"},{"location":"setup/generator/generator/#sample_4","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"created_date\").type(DateType.instance()).min(java.sql.Date.valueOf(\"2020-01-01\"))\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"created_date\").`type`(DateType).min(java.sql.Date.valueOf(\"2020-01-01\"))\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"created_date\"\ntype: \"date\"\ngenerator:\noptions:\nmin: \"2020-01-01\"\n</code></pre>"},{"location":"setup/generator/generator/#timestamp","title":"Timestamp","text":"Option Default Example Description <code>min</code> now() - 365 days <code>min: \"2023-01-31 23:10:10\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> now() <code>max: \"2023-12-31 23:10:10\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (0001-01-01 00:00:00, 1582-10-15 23:59:59, 1970-01-01 00:00:00, 9999-12-31 23:59:59)</p>"},{"location":"setup/generator/generator/#sample_5","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"created_time\").type(TimestampType.instance()).min(java.sql.Timestamp.valueOf(\"2020-01-01 00:00:00\"))\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"created_time\").`type`(TimestampType).min(java.sql.Timestamp.valueOf(\"2020-01-01 00:00:00\"))\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"created_time\"\ntype: \"timestamp\"\ngenerator:\noptions:\nmin: \"2020-01-01 00:00:00\"\n</code></pre>"},{"location":"setup/generator/generator/#binary","title":"Binary","text":"Option Default Example Description <code>minLen</code> 1 <code>minLen: \"2\"</code> Ensures that all generated array of bytes have at least length <code>minLen</code> <code>maxLen</code> 20 <code>maxLen: \"15\"</code> Ensures that all generated array of bytes have at most length <code>maxLen</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\", -128, 127)</p>"},{"location":"setup/generator/generator/#sample_6","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"payload\").type(BinaryType.instance())\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"payload\").`type`(BinaryType)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"payload\"\ntype: \"binary\"\n</code></pre>"},{"location":"setup/generator/generator/#array","title":"Array","text":"Option Default Example Description <code>arrayMinLen</code> 0 <code>arrayMinLen: \"2\"</code> Ensures that all generated arrays have at least length <code>arrayMinLen</code> <code>arrayMaxLen</code> 5 <code>arrayMaxLen: \"15\"</code> Ensures that all generated arrays have at most length <code>arrayMaxLen</code> <code>arrayType</code> <code>arrayType: \"double\"</code> Inner data type of the array. Optional when using Java/Scala API. Allows for nested data types to be defined like struct <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true"},{"location":"setup/generator/generator/#sample_7","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"last_5_amounts\").type(ArrayType.instance()).arrayType(\"double\")\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"last_5_amounts\").`type`(ArrayType).arrayType(\"double\")\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"last_5_amounts\"\ntype: \"array&lt;double&gt;\"\n</code></pre>"},{"location":"setup/generator/report/","title":"Report","text":"<p>Data Caterer can be configured to produce a report of the data generated to help users understand what was run, how much  data was generated, where it was generated, validation results and any associated metadata. </p>"},{"location":"setup/generator/report/#sample","title":"Sample","text":"<p>Once run, it will produce a report like this.</p>"},{"location":"setup/guide/","title":"Guides","text":"<p>Below are a list of guides you can follow to create your data generation for your use case.</p> <p>Checkout this repo for example Java and Scala API usage.</p>"},{"location":"setup/guide/#javascala-api","title":"Java/Scala API","text":""},{"location":"setup/guide/#scenarios","title":"Scenarios","text":"<p>Free tier scenarios</p> <ul> <li>First Data Generation - If you are new, this is the place to start</li> <li>Multiple Records Per Column Value - How you can generate multiple records per set of columns</li> <li>Foreign Keys Across Data Sources - (Soon to document) Generate matching values across generated data sets</li> <li>Data Validations - (Soon to document) Run data validations after generating data</li> </ul> <p>Paid tier scenarios</p> <ul> <li>Auto Generate From Data Connection - Automatically generating data from just defining data sources</li> <li>Delete Generated Data - Delete the generated data whilst leaving other data</li> <li>Generate Batch and Event Data - (Soon to document) Generate matching batch and event data</li> </ul>"},{"location":"setup/guide/#data-sources","title":"Data Sources","text":"<p>Free tier data sources</p> <ul> <li>Files (CSV, JSON, ORC, Parquet) - Generate data for files with separator (comma separated by default)</li> <li>Files (Fixed width) - A variant of CSV but with no separator</li> <li>Postgres - JDBC Postgres tables</li> </ul> <p>Paid tier data sources</p> <ul> <li>MySql - JDBC MySql tables</li> <li>Cassandra - Cassandra tables</li> <li>Kafka - Kafka topics</li> <li>Solace - Solace messages</li> <li>HTTP - HTTP requests</li> </ul>"},{"location":"setup/guide/#yaml-files","title":"YAML Files","text":""},{"location":"setup/guide/#base-concept","title":"Base Concept","text":"<p>The execution of the data generator is based on the concept of plans and tasks. A plan represent the set of tasks that need to be executed,  along with other information that spans across tasks, such as foreign keys between data sources. A task represent the component(s) of a data source and its associated metadata so that it understands what the data should look like  and how many steps (sub data sources) there are (i.e. tables in a database, topics in Kafka). Tasks can define one or more steps.</p>"},{"location":"setup/guide/#plan","title":"Plan","text":""},{"location":"setup/guide/#foreign-keys","title":"Foreign Keys","text":"<p>Define foreign keys across data sources in your plan to ensure generated data can match Link to associated task 1 Link to associated task 2</p>"},{"location":"setup/guide/#task","title":"Task","text":"Data Source Type Data Source Sample Task Notes Database Postgres Sample Database MySQL Sample Database Cassandra Sample File CSV Sample File JSON Sample Contains nested schemas and use of SQL for generated values File Parquet Sample Partition by year column Kafka Kafka Sample Specific base schema to be used, define headers, key, value, etc. JMS Solace Sample JSON formatted message HTTP PUT Sample JSON formatted PUT body"},{"location":"setup/guide/#configuration","title":"Configuration","text":"<p>Basic configuration</p>"},{"location":"setup/guide/data-source/cassandra/","title":"Cassandra","text":"<p>Info</p> <p>Writing data to Cassandra is a paid feature.</p> <p>Creating a data generator for Cassandra. You will build a Docker image that will be able to populate data in Cassandra for the tables you configure.</p>"},{"location":"setup/guide/data-source/cassandra/#requirements","title":"Requirements","text":"<ul> <li>20 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> <li>Cassandra</li> </ul>"},{"location":"setup/guide/data-source/cassandra/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> <pre><code>git clone git@github.com:pflooky/data-caterer-example.git\n</code></pre> <p>If you already have a Cassandra instance running, you can skip to this step.</p>"},{"location":"setup/guide/data-source/cassandra/#cassandra-setup","title":"Cassandra Setup","text":"<p>Next, let's make sure you have an instance of Cassandra up and running in your local environment. This will make it easy for us to iterate and check our changes.</p> <pre><code>cd docker\ndocker-compose up -d cassandra\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#permissions","title":"Permissions","text":"<p>Let's make a new user that has the required permissions needed to push data into the Cassandra tables we want.</p> CQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO data_caterer_user;\n</code></pre> <p>Following permissions are required when enabling <code>configuration.enableGeneratePlanAndTasks(true)</code> as it will gather metadata information about tables and columns from the below tables.</p> CQL Permission Statements <pre><code>GRANT SELECT ON system_schema.tables TO data_caterer_user;\nGRANT SELECT ON system_schema.columns TO data_caterer_user;\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/com/github/pflooky/plan/MyCassandraJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/com/github/pflooky/plan/MyCassandraPlan.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import com.github.pflooky.datacaterer.java.api.PlanRun;\n\npublic class MyCassandraJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import com.github.pflooky.datacaterer.api.PlanRun\n\nclass MyCassandraPlan extends PlanRun {\n}\n</code></pre> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"setup/guide/data-source/cassandra/#connection-configuration","title":"Connection Configuration","text":"<p>Within our <code>MyCassandraPlan</code> class, we can start by defining the connection properties to connect to Cassandra.</p> JavaScala <pre><code>var accountTask = cassandra(\n\"customer_cassandra\",   //name\n\"localhost:9042\",       //url\n\"cassandra\",            //username\n\"cassandra\",            //password\nMap.of()                //optional additional connection options\n)\n</code></pre> <p>Additional options such as SSL configuration, etc can be found here.</p> <pre><code>val accountTask = cassandra(\n\"customer_cassandra\",   //name\n\"localhost:9042\",       //url\n\"cassandra\",            //username\n\"cassandra\",            //password\nMap()                   //optional additional connection options\n)\n</code></pre> <p>Additional options such as SSL configuration, etc can be found here.</p>"},{"location":"setup/guide/data-source/cassandra/#schema","title":"Schema","text":"<p>Let's create a task for inserting data into the <code>account.accounts</code> and <code>account.account_status_history</code> tables as defined under<code>docker/data/cql/customer.cql</code>. This table should already be setup for you if you followed this step. We can check if the table is setup already via the following command:</p> <pre><code>docker exec host.docker.internal cqlsh -e 'describe account.accounts; describe account.account_status_history;'\n</code></pre> <p>Here we should see some output that looks like the below. This tells us what schema we need to follow when generating data. We need to define that alongside any metadata that is useful to add constraints on what are possible values the generated data should contain.</p> <pre><code>CREATE TABLE account.accounts (\naccount_id text PRIMARY KEY,\n    amount double,\n    created_by text,\n    name text,\n    open_time timestamp,\n    status text\n)...\n\nCREATE TABLE account.account_status_history (\naccount_id text,\n    eod_date date,\n    status text,\n    updated_by text,\n    updated_time timestamp,\n    PRIMARY KEY (account_id, eod_date)\n)...\n</code></pre> <p>Trimming the connection details to work with the docker-compose Cassandra, we have a base Cassandra connection to define the table and schema required. Let's define each field along with their corresponding data type. You will notice that the <code>text</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code> which corresponds to <code>text</code> in Cassandra.</p> JavaScala <pre><code>{\nvar accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n.table(\"account\", \"accounts\")\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"amount\").type(DoubleType.instance()),\nfield().name(\"created_by\"),\nfield().name(\"name\"),\nfield().name(\"open_time\").type(TimestampType.instance()),\nfield().name(\"status\")\n);\n}\n</code></pre> <pre><code>val accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n.table(\"account\", \"accounts\")\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"amount\").`type`(DoubleType),\nfield.name(\"created_by\"),\nfield.name(\"name\"),\nfield.name(\"open_time\").`type`(TimestampType),\nfield.name(\"status\")\n)\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#field-metadata","title":"Field Metadata","text":"<p>We could stop here and generate random data for the accounts table. But wouldn't it be more useful if we produced data that is closer to the structure of the data that would come in production? We can do this by defining various metadata that add guidelines that the data generator will understand when generating data.</p>"},{"location":"setup/guide/data-source/cassandra/#account_id","title":"account_id","text":"<p><code>account_id</code> follows a particular pattern that where it starts with <code>ACC</code> and has 8 digits after it. This can be defined via a regex like below. Alongside, we also mention that it is the primary key to prompt ensure that unique values are generated.</p> JavaScala <pre><code>field().name(\"account_id\").regex(\"ACC[0-9]{8}\").primaryKey(true),\n</code></pre> <pre><code>field.name(\"account_id\").regex(\"ACC[0-9]{8}\").primaryKey(true),\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#amount","title":"amount","text":"<p><code>amount</code> the numbers shouldn't be too large, so we can define a min and max for the generated numbers to be between <code>1</code> and <code>1000</code>.</p> JavaScala <pre><code>field().name(\"amount\").type(DoubleType.instance()).min(1).max(1000),\n</code></pre> <pre><code>field.name(\"amount\").`type`(DoubleType).min(1).max(1000),\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#name","title":"name","text":"<p><code>name</code> is a string that also follows a certain pattern, so we could also define a regex but here we will choose to leverage the DataFaker library and create an <code>expression</code> to generate real looking name. All possible faker expressions can be found here</p> JavaScala <pre><code>field().name(\"name\").expression(\"#{Name.name}\"),\n</code></pre> <pre><code>field.name(\"name\").expression(\"#{Name.name}\"),\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#open_time","title":"open_time","text":"<p><code>open_time</code> is a timestamp that we want to have a value greater than a specific date. We can define a min date by using <code>java.sql.Date</code> like below.</p> JavaScala <pre><code>field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n</code></pre> <pre><code>field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#status","title":"status","text":"<p><code>status</code> is a field that can only obtain one of four values, <code>open, closed, suspended or pending</code>.</p> JavaScala <pre><code>field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n</code></pre> <pre><code>field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#created_by","title":"created_by","text":"<p><code>created_by</code> is a field that is based on the <code>status</code> field where it follows the logic: <code>if status is open or closed, then it is created_by eod else created_by event</code>. This can be achieved by defining a SQL expression like below.</p> JavaScala <pre><code>field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n</code></pre> <pre><code>field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n</code></pre> <p>Putting it all the fields together, our class should now look like this.</p> JavaScala <pre><code>var accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n.table(\"account\", \"accounts\")\n.schema(\nfield().name(\"account_id\").regex(\"ACC[0-9]{8}\").primaryKey(true),\nfield().name(\"amount\").type(DoubleType.instance()).min(1).max(1000),\nfield().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield().name(\"name\").expression(\"#{Name.name}\"),\nfield().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n);\n</code></pre> <pre><code>val accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n.table(\"account\", \"accounts\")\n.schema(\nfield.name(\"account_id\").primaryKey(true),\nfield.name(\"amount\").`type`(DoubleType).min(1).max(1000),\nfield.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield.name(\"name\").expression(\"#{Name.name}\"),\nfield.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n)\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScala <pre><code>var config = configuration()\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true);\n</code></pre> <pre><code>val config = configuration\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true)\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>accountTask</code>, we have to call <code>execute</code> . So our full plan run will look like this.</p> JavaScala <pre><code>public class AdvancedCassandraJavaPlanRun extends PlanRun {\n{\nvar accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n.table(\"account\", \"accounts\")\n.schema(\nfield().name(\"account_id\").regex(\"ACC[0-9]{8}\").primaryKey(true),\nfield().name(\"amount\").type(DoubleType.instance()).min(1).max(1000),\nfield().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield().name(\"name\").expression(\"#{Name.name}\"),\nfield().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n);\n\nvar config = configuration()\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true);\n\nexecute(config, accountTask);\n}\n}\n</code></pre> <pre><code>class AdvancedCassandraPlanRun extends PlanRun {\nval accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n.table(\"account\", \"accounts\")\n.schema(\nfield.name(\"account_id\").primaryKey(true),\nfield.name(\"amount\").`type`(DoubleType).min(1).max(1000),\nfield.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield.name(\"name\").expression(\"#{Name.name}\"),\nfield.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n)\n\nval config = configuration\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true)\n\nexecute(config, accountTask)\n}\n</code></pre>"},{"location":"setup/guide/data-source/cassandra/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> <pre><code>./run.sh\n#input class AdvancedCassandraJavaPlanRun or AdvancedCassandraPlanRun\n#after completing\ndocker exec docker-cassandraserver-1 cqlsh -e 'select count(1) from account.accounts;select * from account.accounts limit 10;'\n</code></pre> <p>Your output should look like this.</p> <pre><code> count\n-------\n  1000\n\n(1 rows)\n\nWarnings :\nAggregation query used without partition key\n\n\n account_id  | amount    | created_by         | name                   | open_time                       | status\n-------------+-----------+--------------------+------------------------+---------------------------------+-----------\n ACC13554145 | 917.00418 | zb CVvbBTTzitjo5fK |          Jan Sanford I | 2023-06-21 21:50:10.463000+0000 | suspended\n ACC19154140 |  46.99177 |             VH88H9 |       Clyde Bailey PhD | 2023-07-18 11:33:03.675000+0000 |      open\n ACC50587836 |  774.9872 |         GENANwPm t |           Sang Monahan | 2023-03-21 00:16:53.308000+0000 |    closed\n ACC67619387 | 452.86706 |       5msTpcBLStTH |         Jewell Gerlach | 2022-10-18 19:13:07.606000+0000 | suspended\n ACC69889784 |  14.69298 |           WDmOh7NT |          Dale Schulist | 2022-10-25 12:10:52.239000+0000 | suspended\n ACC41977254 |  51.26492 |          J8jAKzvj2 |           Norma Nienow | 2023-08-19 18:54:39.195000+0000 | suspended\n ACC40932912 | 349.68067 |   SLcJgKZdLp5ALMyg | Vincenzo Considine III | 2023-05-16 00:22:45.991000+0000 |    closed\n ACC20642011 | 658.40713 |          clyZRD4fI |  Lannie McLaughlin DDS | 2023-05-11 23:14:30.249000+0000 |      open\n ACC74962085 | 970.98218 |       ZLETTSnj4NpD |          Ima Jerde DVM | 2023-05-07 10:01:56.218000+0000 |   pending\n ACC72848439 | 481.64267 |                 cc |        Kyla Deckow DDS | 2023-08-16 13:28:23.362000+0000 | suspended\n\n(10 rows)\n</code></pre> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed.</p> <p></p>"},{"location":"setup/guide/scenario/auto-generate-connection/","title":"Auto Generate From Data Connection","text":"<p>Info</p> <p>Auto data generation from data connection is a paid feature.</p> <p>Creating a data generator based on only a data connection to Postgres.</p>"},{"location":"setup/guide/scenario/auto-generate-connection/#requirements","title":"Requirements","text":"<ul> <li>5 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"setup/guide/scenario/auto-generate-connection/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> <pre><code>git clone git@github.com:pflooky/data-caterer-example.git\n</code></pre>"},{"location":"setup/guide/scenario/auto-generate-connection/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/com/github/pflooky/plan/MyAdvancedAutomatedJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/com/github/pflooky/plan/MyAdvancedAutomatedPlanRun.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import com.github.pflooky.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedAutomatedJavaPlanRun extends PlanRun {\n{\nvar autoRun = configuration()\n.postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")  (1)\n.enableGeneratePlanAndTasks(true)                                                 (2)\n.generatedPlanAndTaskFolderPath(\"/opt/app/data/generated\")                        (3)\n.enableUniqueCheck(true)                                                          (4)\n.generatedReportsFolderPath(\"/opt/app/data/report\");\n\nexecute(autoRun);\n}\n}\n</code></pre> <pre><code>import com.github.pflooky.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedAutomatedPlanRun extends PlanRun {\n\nval autoRun = configuration\n.postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")  (1)\n.enableGeneratePlanAndTasks(true)                                                 (2)\n.generatedPlanAndTaskFolderPath(\"/opt/app/data/generated\")                        (3)\n.enableUniqueCheck(true)                                                          (4)\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n\nexecute(configuration = autoRun)\n}\n</code></pre> <p>In the above code, we note the following:</p> <ol> <li>Data source configuration to a Postgres data source called <code>my_postgres</code></li> <li>We have enabled the flag <code>enableGeneratePlanAndTasks</code> which tells Data Caterer to go to <code>my_postgres</code> and generate    data for all the tables found under the database <code>customer</code> (which is defined in the connection string).</li> <li>The config <code>generatedPlanAndTaskFolderPath</code> defines where the metadata that is gathered from <code>my_postgres</code> should be    saved at so that we could re-use it later.</li> <li><code>enableUniqueCheck</code> is set to true to ensure that generated data is unique based on primary key or foreign key    definitions.</li> </ol> <p>Note</p> <p>Unique check will only ensure generated data is unique. Any existing data in your data source is not taken into  account, so generated data may fail to insert depending on the data source restrictions</p>"},{"location":"setup/guide/scenario/auto-generate-connection/#postgres-setup","title":"Postgres Setup","text":"<p>If you don't have your own Postgres up and running, you can set up and run an instance configured in the <code>docker</code> folder via.</p> <pre><code>cd docker\ndocker-compose up -d postgres\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c '\\dt+ account.*'\n</code></pre> <p>This will create the tables found under <code>docker/data/sql/postgres/customer.sql</code>. You can change this file to contain your own tables. We can see there are 4 tables created for us, <code>accounts, balances, transactions and mapping</code>.</p>"},{"location":"setup/guide/scenario/auto-generate-connection/#run","title":"Run","text":"<p>Let's try run.</p> <pre><code>cd ..\n./run.sh\n#input class MyAdvancedAutomatedJavaPlanRun or MyAdvancedAutomatedPlanRun\n#after completing\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select * from account.accounts limit 1;'\n</code></pre> <p>It should look something like this.</p> <pre><code>   id   | account_number  | account_status | created_by | created_by_fixed_length | customer_id_int | customer_id_smallint | customer_id_bigint |   customer_id_decimal    | customer_id_real | customer_id_double | open_date  |     open_timestamp      | last_opened_time |                                                           payload_bytes\n--------+-----------------+----------------+------------+-------------------------+-----------------+----------------------+--------------------+--------------------------+------------------+--------------------+------------+-------------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------\n 100414 | 5uROOVOUyQUbubN | h3H            | SfA0eZJcTm | CuRw                    |              13 |                   42 |               6041 | 76987.745612542900000000 |         91866.78 |  66400.37433202339 | 2023-03-05 | 2023-08-14 11:33:11.343 | 23:58:01.736     | \\x604d315d4547616e6a233050415373317274736f5e682d516132524f3d23233c37463463322f342d34376d597e665d6b3d395b4238284028622b7d6d2b4f5042\n(1 row)\n</code></pre> <p>The data that gets inserted will follow the foreign keys that are defined within Postgres and also ensure the insertion order is correct.</p> <p>Also check the HTML report that gets generated under <code>docker/sample/report/index.html</code>. You can see a summary of what was generated along with other metadata.</p> <p>You can now look to play around with other tables or data sources and auto generate for them.</p>"},{"location":"setup/guide/scenario/auto-generate-connection/#additional-topics","title":"Additional Topics","text":""},{"location":"setup/guide/scenario/auto-generate-connection/#learn-from-existing-data","title":"Learn From Existing Data","text":"<p>If you have any existing data within your data source, Data Caterer will gather metadata about the existing data to help guide it when generating new data. There are configurations that can help tune the metadata analysis found here.</p>"},{"location":"setup/guide/scenario/auto-generate-connection/#filter-out-schematables","title":"Filter Out Schema/Tables","text":"<p>As part of your connection definition, you can define any schemas and/or tables your don't want to generate data for. In the example below, it will not generate any data for any tables under the <code>history</code> and <code>audit</code> schemas. Also, any table with the name <code>balances</code> or <code>transactions</code> in any schema will also not have data generated.</p> JavaScala <pre><code>var autoRun = configuration()\n.postgres(\n\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\",\nMap.of(\n\"filterOutSchema\", \"history, audit\",\n\"filterOutTable\", \"balances, transactions\")\n)\n)\n</code></pre> <pre><code>val autoRun = configuration\n.postgres(\n\"my_postgres\",\n\"jdbc:postgresql://host.docker.internal:5432/customer\",\nMap(\n\"filterOutSchema\" -&gt; \"history, audit\",\n\"filterOutTable\" -&gt; \"balances, transactions\")\n)\n)\n</code></pre>"},{"location":"setup/guide/scenario/auto-generate-connection/#define-record-count","title":"Define record count","text":"<p>You can control the record count per sub data source via <code>numRecordsPerStep</code>.</p> JavaScala <pre><code>var autoRun = configuration()\n...\n.numRecordsPerStep(100)\n\nexecute(autoRun)\n</code></pre> <pre><code>val autoRun = configuration\n...\n.numRecordsPerStep(100)\n\nexecute(configuration = autoRun)\n</code></pre>"},{"location":"setup/guide/scenario/delete-generated-data/","title":"Delete Generated Data","text":"<p>Info</p> <p>Delete generated data is a paid feature.</p> <p>Creating a data generator for Postgres and delete the generated data after using it.</p>"},{"location":"setup/guide/scenario/delete-generated-data/#requirements","title":"Requirements","text":"<ul> <li>5 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"setup/guide/scenario/delete-generated-data/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> <pre><code>git clone git@github.com:pflooky/data-caterer-example.git\n</code></pre>"},{"location":"setup/guide/scenario/delete-generated-data/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/com/github/pflooky/plan/MyAdvancedDeleteJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/com/github/pflooky/plan/MyAdvancedDeletePlanRun.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import com.github.pflooky.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedDeleteJavaPlanRun extends PlanRun {\n{\nvar autoRun = configuration()\n.postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")  (1)\n.enableGeneratePlanAndTasks(true)                                                 (2)\n.enableRecordTracking(true)                                                       (3)\n.enableDeleteGeneratedRecords(false)                                              (4)\n.enableUniqueCheck(true)\n.generatedPlanAndTaskFolderPath(\"/opt/app/data/generated\")                        (5)\n.generatedReportsFolderPath(\"/opt/app/data/report\");                              (6)\n\nexecute(autoRun);\n}\n}\n</code></pre> <pre><code>import com.github.pflooky.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedDeletePlanRun extends PlanRun {\n\nval autoRun = configuration\n.postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")  (1)\n.enableGeneratePlanAndTasks(true)                                                 (2)\n.enableRecordTracking(true)                                                       (3)\n.enableDeleteGeneratedRecords(false)                                              (4)\n.enableUniqueCheck(true)\n.generatedPlanAndTaskFolderPath(\"/opt/app/data/generated\")                        (5)\n.recordTrackingFolderPath(\"/opt/app/data/recordTracking\")                         (6)\n\nexecute(configuration = autoRun)\n}\n</code></pre> <p>In the above code we note the following:</p> <ol> <li>We have defined a Postgres connection called <code>my_postgres</code></li> <li><code>enableGeneratePlanAndTasks</code> is enabled to auto generate data for all tables under <code>customer</code> database</li> <li><code>enableRecordTracking</code> is enabled to ensure that all generated records are tracked. This will get used when we want    to delete data afterwards</li> <li><code>enableDeleteGeneratedRecords</code> is disabled for now. We want to see the generated data first and delete sometime after</li> <li><code>generatedPlanAndTaskFolderPath</code> is the folder path where we saved the metadata we have gathered from <code>my_postgres</code></li> <li><code>recordTrackingFolderPath</code> is the folder path where record tracking is maintained. We need to persist this data to    ensure it is still available when we want to delete data</li> </ol>"},{"location":"setup/guide/scenario/delete-generated-data/#postgres-setup","title":"Postgres Setup","text":"<p>If you don't have your own Postgres up and running, you can set up and run an instance configured in the <code>docker</code> folder via.</p> <pre><code>cd docker\ndocker-compose up -d postgres\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c '\\dt+ account.*'\n</code></pre> <p>This will create the tables found under <code>docker/data/sql/postgres/customer.sql</code>. You can change this file to contain your own tables. We can see there are 4 tables created for us, <code>accounts, balances, transactions and mapping</code>.</p>"},{"location":"setup/guide/scenario/delete-generated-data/#run","title":"Run","text":"<p>Let's try run.</p> <pre><code>cd ..\n./run.sh\n#input class MyAdvancedDeleteJavaPlanRun or MyAdvancedDeletePlanRun\n#after completing\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select * from account.accounts limit 1'\n</code></pre> <p>It should look something like this.</p> <pre><code>   id   | account_number  | account_status | created_by | created_by_fixed_length | customer_id_int | customer_id_smallint | customer_id_bigint |   customer_id_decimal    | customer_id_real | customer_id_double | open_date  |     open_timestamp      | last_opened_time |                                                           payload_bytes\n--------+-----------------+----------------+------------+-------------------------+-----------------+----------------------+--------------------+--------------------------+------------------+--------------------+------------+-------------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------\n 100414 | 5uROOVOUyQUbubN | h3H            | SfA0eZJcTm | CuRw                    |              13 |                   42 |               6041 | 76987.745612542900000000 |         91866.78 |  66400.37433202339 | 2023-03-05 | 2023-08-14 11:33:11.343 | 23:58:01.736     | \\x604d315d4547616e6a233050415373317274736f5e682d516132524f3d23233c37463463322f342d34376d597e665d6b3d395b4238284028622b7d6d2b4f5042\n(1 row)\n</code></pre> <p>The data that gets inserted will follow the foreign keys that are defined within Postgres and also ensure the insertion order is correct.</p> <p>Check the number of records via:</p> <pre><code>docker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select count(1) from account.accounts'\n#open report under docker/sample/report/index.html\n</code></pre>"},{"location":"setup/guide/scenario/delete-generated-data/#delete","title":"Delete","text":"<p>We are now at a stage where we want to delete the data that was generated. All we need to do is flip two flags.</p> <pre><code>.enableDeleteGeneratedRecords(true)\n.enableGenerateData(false)  //this flag is newly defined in our class, we explicitly disable generating data\n</code></pre> <p>Enable delete generated records and disable generating data. </p> <p>Before we run again, let us insert a record manually to see if that data will survive after running the job to delete the generated data.</p> <pre><code>docker exec docker-postgresserver-1 psql -Upostgres -d customer -c \"insert into account.accounts (account_number) values ('my_account_number')\"\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c \"select count(1) from account.accounts\"\n</code></pre> <p>We now should have 1001 records in our <code>account.accounts</code> table. Let's delete the generated data now.</p> <pre><code>./run.sh\n#input class MyAdvancedDeleteJavaPlanRun or MyAdvancedDeletePlanRun\n#after completing\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select * from account.accounts limit 1'\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select count(1) from account.accounts'\n</code></pre> <p>You should see that only 1 record is left, the one that we manually inserted. Great, now we can generate data reliably  and also be able to clean it up.</p>"},{"location":"setup/guide/scenario/delete-generated-data/#additional-topics","title":"Additional Topics","text":""},{"location":"setup/guide/scenario/delete-generated-data/#one-class-for-generating-another-for-deleting","title":"One class for generating, another for deleting?","text":"<p>Yes, this is possible. The only requirement is that the connection names used need to be the same across both classes.</p>"},{"location":"setup/guide/scenario/delete-generated-data/#define-record-count","title":"Define record count","text":"<p>You can control the record count per sub data source via <code>numRecordsPerStep</code>.</p> JavaScala <pre><code>var autoRun = configuration()\n...\n.numRecordsPerStep(100)\n\nexecute(autoRun)\n</code></pre> <pre><code>val autoRun = configuration\n...\n.numRecordsPerStep(100)\n\nexecute(configuration = autoRun)\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/","title":"First Data Generation","text":"<p>Creating a data generator for a CSV file.</p>"},{"location":"setup/guide/scenario/first-data-generation/#requirements","title":"Requirements","text":"<ul> <li>20 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"setup/guide/scenario/first-data-generation/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> <pre><code>git clone git@github.com:pflooky/data-caterer-example.git\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/com/github/pflooky/plan/MyCsvPlan.java</code></li> <li>Scala: <code>src/main/scala/com/github/pflooky/plan/MyCsvPlan.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import com.github.pflooky.datacaterer.java.api.PlanRun;\n\npublic class MyCsvJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import com.github.pflooky.datacaterer.api.PlanRun\n\nclass MyCsvPlan extends PlanRun {\n}\n</code></pre> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"setup/guide/scenario/first-data-generation/#connection-configuration","title":"Connection Configuration","text":"<p>When dealing with CSV files, we need to define a path for our generated CSV files to be saved at, along with any other high level configurations.</p> JavaScala <pre><code>csv(\n\"customer_accounts\",              //name\n\"/opt/app/data/customer/account\", //path\nMap.of(\"header\", \"true\")          //optional additional options\n)\n</code></pre> <p>Other additional options for CSV can be found here</p> <pre><code>csv(\n\"customer_accounts\",              //name\n\"/opt/app/data/customer/account\", //path\nMap(\"header\" -&gt; \"true\")           //optional additional options\n)\n</code></pre> <p>Other additional options for CSV can be found here</p>"},{"location":"setup/guide/scenario/first-data-generation/#schema","title":"Schema","text":"<p>Our CSV file that we generate should adhere to a defined schema where we can also define data types.</p> <p>Let's define each field along with their corresponding data type. You will notice that the <code>string</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code>.</p> JavaScala <pre><code>var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"balance\").type(DoubleType.instance()),\nfield().name(\"created_by\"),\nfield().name(\"name\"),\nfield().name(\"open_time\").type(TimestampType.instance()),\nfield().name(\"status\")\n);\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"balance\").`type`(DoubleType),\nfield.name(\"created_by\"),\nfield.name(\"name\"),\nfield.name(\"open_time\").`type`(TimestampType),\nfield.name(\"status\")\n)\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#field-metadata","title":"Field Metadata","text":"<p>We could stop here and generate random data for the accounts table. But wouldn't it be more useful if we produced data that is closer to the structure of the data that would come in production? We can do this by defining various metadata attributes that add guidelines that the data generator will understand when generating data.</p>"},{"location":"setup/guide/scenario/first-data-generation/#account_id","title":"account_id","text":"<ul> <li><code>account_id</code> follows a particular pattern that where it starts with <code>ACC</code> and has 8 digits after it.   This can be defined via a regex like below. Alongside, we also mention that values are unique ensure that   unique values are generated.</li> </ul> JavaScala <pre><code>field().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n</code></pre> <pre><code>field.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#balance","title":"balance","text":"<ul> <li><code>balance</code> let's make the numbers not too large, so we can define a min and max for the generated numbers to be between   <code>1</code> and <code>1000</code>.</li> </ul> JavaScala <pre><code>field().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\n</code></pre> <pre><code>field.name(\"balance\").`type`(DoubleType).min(1).max(1000),\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#name","title":"name","text":"<ul> <li><code>name</code> is a string that also follows a certain pattern, so we could also define a regex but here we will choose to   leverage the DataFaker library and create an <code>expression</code> to generate real looking name. All possible faker   expressions   can be found here</li> </ul> JavaScala <pre><code>field().name(\"name\").expression(\"#{Name.name}\"),\n</code></pre> <p>```scala field.name(\"name\").expression(\"#{Name.name}\"),</p>"},{"location":"setup/guide/scenario/first-data-generation/#open_time","title":"open_time","text":"<ul> <li><code>open_time</code> is a timestamp that we want to have a value greater than a specific date. We can define a min date by   using   <code>java.sql.Date</code> like below.</li> </ul> JavaScala <pre><code>field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n</code></pre> <pre><code>field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#status","title":"status","text":"<ul> <li><code>status</code> is a field that can only obtain one of four values, <code>open, closed, suspended or pending</code>.</li> </ul> JavaScala <pre><code>field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n</code></pre> <pre><code>field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#created_by","title":"created_by","text":"<ul> <li><code>created_by</code> is a field that is based on the <code>status</code> field where it follows the   logic: <code>if status is open or closed, then   it is created_by eod else created_by event</code>. This can be achieved by defining a SQL expression like below.</li> </ul> JavaScala <pre><code>field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n</code></pre> <pre><code>field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n</code></pre> <p>Putting it all the fields together, our class should now look like this.</p> JavaScala <pre><code>var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n.schema(\nfield().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\nfield().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\nfield().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield().name(\"name\").expression(\"#{Name.name}\"),\nfield().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n);\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n.schema(\nfield.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\nfield.name(\"balance\").`type`(DoubleType).min(1).max(1000),\nfield.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield.name(\"name\").expression(\"#{Name.name}\"),\nfield.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n)\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#record-count","title":"Record Count","text":"<p>We only want to generate 100 records, so that we can see what the output looks like. This is controlled at the  <code>accountTask</code> level like below. If you want to generate more records, set it to the value you want.</p> JavaScala <pre><code>var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n.schema(\n...\n)\n.count(count().records(100));\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n.schema(\n...\n)\n.count(count.records(100))\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScala <pre><code>var config = configuration()\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true);\n</code></pre> <pre><code>val config = configuration\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true)\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>accountTask</code>, we have to call <code>execute</code> . So our full plan run will look like this.</p> JavaScala <pre><code>public class MyCsvJavaPlan extends PlanRun {\n{\nvar accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n.schema(\nfield().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\nfield().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\nfield().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield().name(\"name\").expression(\"#{Name.name}\"),\nfield().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n);\n\nvar config = configuration()\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true);\n\nexecute(config, accountTask);\n}\n}\n</code></pre> <pre><code>class MyCsvPlan extends PlanRun {\n\nval accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n.schema(\nfield.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\nfield.name(\"balance\").`type`(DoubleType).min(1).max(1000),\nfield.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield.name(\"name\").expression(\"#{Name.name}\"),\nfield.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n)\nval config = configuration\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true)\n\nexecute(config, accountTask)\n}\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> <pre><code>./run.sh\n#input class MyCsvJavaPlan or MyCsvPlan\n#after completing\nhead docker/sample/customer/account/part-00000*\n</code></pre> <p>Your output should look like this.</p> <pre><code>account_id,balance,created_by,name,open_time,status\nACC06192462,853.9843359645766,eod,Hoyt Kertzmann MD,2023-07-22T11:17:01.713Z,closed\nACC15350419,632.5969895326234,eod,Dr. Claude White,2022-12-13T21:57:56.840Z,open\nACC25134369,592.0958847218986,eod,Fabian Rolfson,2023-04-26T04:54:41.068Z,open\nACC48021786,656.6413439322964,eod,Dewayne Stroman,2023-05-17T06:31:27.603Z,open\nACC26705211,447.2850352884595,event,Garrett Funk,2023-07-14T03:50:22.746Z,pending\nACC03150585,750.4568929015996,event,Natisha Reichel,2023-04-11T11:13:10.080Z,suspended\nACC29834210,686.4257811608622,event,Gisele Ondricka,2022-11-15T22:09:41.172Z,suspended\nACC39373863,583.5110618128994,event,Thaddeus Ortiz,2022-09-30T06:33:57.193Z,suspended\nACC39405798,989.2623959059525,eod,Shelby Reinger,2022-10-23T17:29:17.564Z,open\n</code></pre> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed.</p> <p></p>"},{"location":"setup/guide/scenario/first-data-generation/#join-with-another-csv","title":"Join With Another CSV","text":"<p>Now that we have generated some accounts, let's also try to generate a set of transactions for those accounts in CSV  format as well. The transactions could be in any other format, but to keep this simple, we will continue using CSV.</p> <p>We can define our schema the same way along with any additional metadata.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"name\"),\nfield().name(\"amount\").type(DoubleType.instance()).min(1).max(100),\nfield().name(\"time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield().name(\"date\").type(DateType.instance()).sql(\"DATE(time)\")\n);\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"full_name\"),\nfield.name(\"amount\").`type`(DoubleType).min(1).max(100),\nfield.name(\"time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield.name(\"date\").`type`(DateType).sql(\"DATE(time)\")\n)\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#records-per-column","title":"Records Per Column","text":"<p>Usually, for a given <code>account_id, full_name</code>, there should be multiple records for it as we want to simulate a customer having multiple transactions. We can achieve this through defining the number of records to generate in the <code>count</code>  function.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n.schema(\n...\n)\n.count(count().recordsPerColumn(5, \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n.schema(\n...\n)\n.count(count.recordsPerColumn(5, \"account_id\", \"full_name\"))\n</code></pre>"},{"location":"setup/guide/scenario/first-data-generation/#random-records-per-column","title":"Random Records Per Column","text":"<p>Here, you will notice that we are generating 5 records per <code>account_id, full_name</code>. This is okay but still not quite  reflective of the real world. Sometimes, people have accounts with no transactions in them, or they could have many. We  can accommodate for this via defining a random number of records per column.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n.schema(\n...\n)\n.count(count().recordsPerColumnGenerator(generator().min(0).max(5), \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n.schema(\n...\n)\n.count(count.recordsPerColumnGenerator(generator.min(0).max(5), \"account_id\", \"full_name\"))\n</code></pre> <p>Here we set the minimum number of records per column to be 0 and the maximum to 5.</p>"},{"location":"setup/guide/scenario/first-data-generation/#foreign-key","title":"Foreign Key","text":"<p>In this scenario, we want to match the <code>account_id</code> in <code>account</code> to match the same column values in <code>transaction</code>. We  also want to match <code>name</code> in <code>account</code> to <code>full_name</code> in <code>transaction</code>. This can be done via plan configuration like  below.</p> JavaScala <pre><code>var myPlan = plan().addForeignKeyRelationship(\naccountTask, List.of(\"account_id\", \"name\"), //the task and columns we want linked\nList.of(Map.entry(transactionTask, List.of(\"account_id\", \"full_name\"))) //list of other tasks and their respective column names we want matched\n);\n</code></pre> <pre><code>val myPlan = plan.addForeignKeyRelationship(\naccountTask, List(\"account_id\", \"name\"),  //the task and columns we want linked\nList(transactionTask -&gt; List(\"account_id\", \"full_name\"))  //list of other tasks and their respective column names we want matched\n)\n</code></pre> <p>Now, stitching it all together for the <code>execute</code> function, our final plan should look like this.</p> JavaScala <pre><code>public class MyCsvJavaPlan extends PlanRun {\n{\nvar accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n.schema(\nfield().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\nfield().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\nfield().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield().name(\"name\").expression(\"#{Name.name}\"),\nfield().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n)\n.count(count().records(100));\n\nvar transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"name\"),\nfield().name(\"amount\").type(DoubleType.instance()).min(1).max(100),\nfield().name(\"time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield().name(\"date\").type(DateType.instance()).sql(\"DATE(time)\")\n)\n.count(count().recordsPerColumnGenerator(generator().min(0).max(5), \"account_id\", \"full_name\"));\n\nvar config = configuration()\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true);\n\nvar myPlan = plan().addForeignKeyRelationship(\naccountTask, List.of(\"account_id\", \"name\"),\nList.of(Map.entry(transactionTask, List.of(\"account_id\", \"full_name\")))\n);\n\nexecute(myPlan, config, accountTask, transactionTask);\n}\n}\n</code></pre> <pre><code>class MyCsvPlan extends PlanRun {\n\nval accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n.schema(\nfield.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\nfield.name(\"balance\").`type`(DoubleType).min(1).max(1000),\nfield.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\nfield.name(\"name\").expression(\"#{Name.name}\"),\nfield.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n)\n.count(count.records(100))\n\nval transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"name\"),\nfield.name(\"amount\").`type`(DoubleType).min(1).max(100),\nfield.name(\"time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield.name(\"date\").`type`(DateType).sql(\"DATE(time)\")\n)\n.count(count.recordsPerColumnGenerator(generator.min(0).max(5), \"account_id\", \"full_name\"))\n\nval config = configuration\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true)\n\nval myPlan = plan.addForeignKeyRelationship(\naccountTask, List(\"account_id\", \"name\"),\nList(transactionTask -&gt; List(\"account_id\", \"full_name\"))\n)\n\nexecute(myPlan, config, accountTask, transactionTask)\n}\n</code></pre> <p>Let's try run again.</p> <pre><code>#clean up old data\nrm -rf docker/sample/customer/account\n./run.sh\n#input class MyCsvJavaPlan or MyCsvPlan\n#after completing, let's pick an account and check the transactions for that account\naccount=$(tail -1 docker/sample/customer/account/part-00000* | awk -F \",\" '{print $1 \",\" $4}')\necho $account\ncat docker/sample/customer/transaction/part-00000* | grep $account\n</code></pre> <p>It should look something like this.</p> <pre><code>ACC29117767,Willodean Sauer\nACC29117767,Willodean Sauer,84.99145871948083,2023-05-14T09:55:51.439Z,2023-05-14\nACC29117767,Willodean Sauer,58.89345733567232,2022-11-22T07:38:20.143Z,2022-11-22\n</code></pre> <p>Congratulations! You have now made a data generator that has simulated a real world data scenario. You can check the  <code>CsvJavaPlan.java</code> or <code>CsvPlan.scala</code> files as well to check that your plan is the same.</p> <p>You can now look to play around with other configurations or data sources to meet your needs. Also, make sure to explore the docs further as it can guide you on what can be configured.</p>"},{"location":"setup/guide/scenario/records-per-column/","title":"Multiple Records Per Column","text":"<p>Creating a data generator for a CSV file where there are multiple records per column values.</p>"},{"location":"setup/guide/scenario/records-per-column/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"setup/guide/scenario/records-per-column/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> <pre><code>git clone git@github.com:pflooky/data-caterer-example.git\n</code></pre>"},{"location":"setup/guide/scenario/records-per-column/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/com/github/pflooky/plan/MyMultipleRecordsPerColJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/com/github/pflooky/plan/MyMultipleRecordsPerColPlan.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import com.github.pflooky.datacaterer.java.api.PlanRun;\n...\n\npublic class MyMultipleRecordsPerColJavaPlan extends PlanRun {\n{\nvar transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n.schema(\nfield().name(\"account_id\"),\nfield().name(\"full_name\"),\nfield().name(\"amount\").type(DoubleType.instance()).min(1).max(100),\nfield().name(\"time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\nfield().name(\"date\").type(DateType.instance()).sql(\"DATE(time)\")\n)\n\nvar config = configuration()\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n.enableUniqueCheck(true);\n\nexecute(config, transactionTask);\n}\n}\n</code></pre> <pre><code>import com.github.pflooky.datacaterer.api.PlanRun\n...\n\nclass MyMultipleRecordsPerColPlan extends PlanRun {\n\nval transactionTask: ConnectionTaskBuilder[FileBuilder] = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n.schema(\nfield.name(\"account_id\").regex(\"ACC[0-9]{8}\"), field.name(\"full_name\").expression(\"#{Name.name}\"), field.name(\"amount\").`type`(DoubleType.instance).min(1).max(100),\nfield.name(\"time\").`type`(TimestampType.instance).min(java.sql.Date.valueOf(\"2022-01-01\")), field.name(\"date\").`type`(DateType.instance).sql(\"DATE(time)\")\n)\n\nval config = configuration\n.generatedReportsFolderPath(\"/opt/app/data/report\")\n\nexecute(config, transactionTask)\n}\n</code></pre>"},{"location":"setup/guide/scenario/records-per-column/#record-count","title":"Record Count","text":"<p>By default, tasks will generate 1000 records. You can alter this value via the <code>count</code> configuration which can be applied to individual tasks. For example, in Scala, <code>csv(...).count(count.records(100))</code> to generate only 100 records.</p>"},{"location":"setup/guide/scenario/records-per-column/#records-per-column","title":"Records Per Column","text":"<p>In this scenario, for a given <code>account_id, full_name</code>, there should be multiple records for it as we want to simulate a customer having multiple transactions. We can achieve this through defining the number of records to generate in the <code>count</code> function.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n.schema(\n...\n)\n.count(count().recordsPerColumn(5, \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n.schema(\n...\n)\n.count(count.recordsPerColumn(5, \"account_id\", \"full_name\"))\n</code></pre> <p>This will generate <code>1000 * 5 = 5000</code> records as the default number of records is set (1000) and per <code>account_id, full_name</code> from the initial 1000 records, 5 records will be generated.</p>"},{"location":"setup/guide/scenario/records-per-column/#random-records-per-column","title":"Random Records Per Column","text":"<p>Generating 5 records per column is okay but still not quite reflective of the real world. Sometimes, people have accounts with no transactions in them, or they could have many. We can accommodate for this via defining a random number of records per column.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n.schema(\n...\n)\n.count(count().recordsPerColumnGenerator(generator().min(0).max(5), \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n.schema(\n...\n)\n.count(count.recordsPerColumnGenerator(generator.min(0).max(5), \"account_id\", \"full_name\"))\n</code></pre> <p>Here we set the minimum number of records per column to be 0 and the maximum to 5. This will follow a uniform distribution so the average number of records per account is 2.5. We could also define other metadata, just like we did with fields, when defining the generator. For example, we could set <code>standardDeviation</code> and <code>mean</code> for the number of records generated per column to follow a normal distribution.</p>"},{"location":"setup/guide/scenario/records-per-column/#run","title":"Run","text":"<p>Let's try run.</p> <pre><code>#clean up old data\nrm -rf docker/sample/customer/account\n./run.sh\n#input class MyMultipleRecordsPerColJavaPlan or MyMultipleRecordsPerColPlan\n#after completing\nhead docker/sample/customer/transaction/part-00000*\n</code></pre> <p>It should look something like this.</p> <pre><code>ACC29117767,Willodean Sauer\nACC29117767,Willodean Sauer,84.99145871948083,2023-05-14T09:55:51.439Z,2023-05-14\nACC29117767,Willodean Sauer,58.89345733567232,2022-11-22T07:38:20.143Z,2022-11-22\n</code></pre> <p>You can now look to play around with other count configurations found here.</p>"},{"location":"setup/validation/validation/","title":"Validations","text":"<p>Validations can be used to run data checks after you have run the data generator or even as a standalone task. A report summarising the success or failure of the validations is produced and can be examined for further investigation.</p> <p>Currently, SQL expression validations are supported (can see here for reference what other expressions are valid), but will later be extended out to supported other validations such as aggregates (group by account_number, sum of amounts should be greater than 100), ordering (transaction dates should be in descending order), relationships (at least one transaction per account_number) or data profiling (how close produced data profile is to expected data profile).</p>"},{"location":"setup/validation/validation/#define-validations","title":"Define Validations","text":""},{"location":"setup/validation/validation/#expression-validation","title":"Expression Validation","text":"JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\", Map.of(\"header\", \"true\"))\n.validations(\nvalidation().expr(\"amount &lt; 100\"),\nvalidation().expr(\"year == 2021\").errorThreshold(0.1),  //equivalent to if error percentage is &gt; 10%, then fail\nvalidation().expr(\"REGEXP_LIKE(name, 'Peter .*')\").errorThreshold(200)  //equivalent to if number of errors is &gt; 200, then fail\n);\n\nvar conf = configuration().enableValidation(true);\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\", Map(\"header\" -&gt; \"true\"))\n.validations(\nvalidation.expr(\"amount &lt; 100\"),\nvalidation.expr(\"year == 2021\").errorThreshold(0.1),  //equivalent to if error percentage is &gt; 10%, then fail\nvalidation.expr(\"REGEXP_LIKE(name, 'Peter .*')\").errorThreshold(200)  //equivalent to if number of errors is &gt; 200, then fail\n)\n\nval conf = configuration.enableValidation(true)\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\ntransactions:\noptions:\npath: \"/tmp/csv\"\nvalidations:\n- expr: \"amount &lt; 100\"\n- expr: \"year == 2021\"\nerrorThreshold: 0.1   #equivalent to if error percentage is &gt; 10%, then fail\n- expr: \"REGEXP_LIKE(name, 'Peter .*')\"\nerrorThreshold: 200   #equivalent to if number of errors is &gt; 200, then fail\ndescription: \"Should be lots of Peters\"\n\n#enableValidation inside application.conf\n</code></pre>"},{"location":"setup/validation/validation/#wait-condition","title":"Wait Condition","text":"<p>Once data has been generated, you may want to wait for a certain condition to be met before starting the data validations. This can be via:</p> <ul> <li>Pause for seconds</li> <li>When file is available</li> <li>Data exists</li> <li>Webhook</li> </ul>"},{"location":"setup/validation/validation/#pause","title":"Pause","text":"JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\", Map.of(\"header\", \"true\"))\n.validationWait(waitCondition().pause(1));\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\", Map(\"header\" -&gt; \"true\"))\n.validationWait(waitCondition.pause(1))\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\ntransactions:\noptions:\npath: \"/tmp/csv\"\nwaitCondition:\npauseInSeconds: 1\n</code></pre>"},{"location":"setup/validation/validation/#data-exists","title":"Data exists","text":"JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\", Map.of(\"header\", \"true\"))\n.validationWaitDataExists(\"updated_date &gt; DATE('2023-01-01')\");\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\", Map(\"header\" -&gt; \"true\"))\n.validationWaitDataExists(\"updated_date &gt; DATE('2023-01-01')\")\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\ntransactions:\noptions:\npath: \"/tmp/csv\"\nwaitCondition:\ndataSourceName: \"transactions\"\noptions:\npath: \"/tmp/csv\"\nexpr: \"updated_date &gt; DATE('2023-01-01')\"\n</code></pre>"},{"location":"setup/validation/validation/#webhook","title":"Webhook","text":"JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\", Map.of(\"header\", \"true\"))\n.validationWait(waitCondition().webhook(\"http://localhost:8080/finished\")); //by default, GET request successful when 200 status code\n\n//or\n\nvar csvTxnsWithStatusCodes = csv(\"transactions\", \"/tmp/csv\", Map.of(\"header\", \"true\"))\n.validationWait(waitCondition().webhook(\"http://localhost:8080/finished\", \"GET\", 200, 202));  //successful if 200 or 202 status code\n\n//or\n\nvar csvTxnsWithExistingHttpConnection = csv(\"transactions\", \"/tmp/csv\", Map.of(\"header\", \"true\"))\n.validationWait(waitCondition().webhook(\"my_http\", \"http://localhost:8080/finished\"));  //use connection configuration from existing 'my_http' connection definition\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\", Map(\"header\" -&gt; \"true\"))\n.validationWait(waitCondition.webhook(\"http://localhost:8080/finished\"))  //by default, GET request successful when 200 status code\n\n//or\n\nval csvTxnsWithStatusCodes = csv(\"transactions\", \"/tmp/csv\", Map(\"header\" -&gt; \"true\"))\n.validationWait(waitCondition.webhook(\"http://localhost:8080/finished\", \"GET\", 200, 202)) //successful if 200 or 202 status code\n\n//or\n\nval csvTxnsWithExistingHttpConnection = csv(\"transactions\", \"/tmp/csv\", Map(\"header\" -&gt; \"true\"))\n.validationWait(waitCondition.webhook(\"my_http\", \"http://localhost:8080/finished\")) //use connection configuration from existing 'my_http' connection definition\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\ntransactions:\noptions:\npath: \"/tmp/csv\"\nwaitCondition:\nurl: \"http://localhost:8080/finished\" #by default, GET request successful when 200 status code\n\n#or\n\n---\nname: \"account_checks\"\ndataSources:\ntransactions:\noptions:\npath: \"/tmp/csv\"\nwaitCondition:\nurl: \"http://localhost:8080/finished\"\nmethod: \"GET\"\nstatusCodes: [200, 202] #successful if 200 or 202 status code\n\n#or\n\n---\nname: \"account_checks\"\ndataSources:\ntransactions:\noptions:\npath: \"/tmp/csv\"\nwaitCondition:\ndataSourceName: \"my_http\" #use connection configuration from existing 'my_http' connection definition\nurl: \"http://localhost:8080/finished\"\n</code></pre>"},{"location":"setup/validation/validation/#file-available","title":"File available","text":"JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\", Map.of(\"header\", \"true\"))\n.validationWait(waitCondition().file(\"/tmp/json\"));\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\", Map.of(\"header\", \"true\"))\n.validationWait(waitCondition.file(\"/tmp/json\"))\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\ntransactions:\noptions:\npath: \"/tmp/csv\"\nwaitCondition:\npath: \"/tmp/json\"\n</code></pre>"},{"location":"setup/validation/validation/#report","title":"Report","text":"<p>Once run, it will produce a report like this.</p>"},{"location":"use-case/business-value/","title":"Business Value","text":"<p>Below is a list of the business related benefits from using Data Caterer which may be applicable for your use case.</p> Problem Data Caterer Solution Resources Effects Reliable test data creation - Profile existing data- Create scenarios- Generate data Software Engineers, QA, Testers Cost reduction in labor, more time spent on development, more bugs caught before production Faster development cycles - Generate data in local, test, UAT, pre-prod- Run different scenarios Software Engineers, QA, Testers More defects caught in lower environments, features pushed to production faster, common framework used across all environments Data compliance - Profiling existing data- Generate based on metadata- No complex masking- No production data used in lower environments Audit and compliance No chance for production data breaches Storage costs - Delete generated data- Test specific scenarios Infrastructure Lower data storage costs, less time spent on data management and clean up Schema evolution - Create metadata from data sources- Generate data based off fresh metadata Software Engineers, QA, Testers Less time spent altering tests due to schema changes, ease of use between environments and application versions"},{"location":"use-case/comparison/","title":"Comparison to similar tools","text":"<p>We have not been able to assess all the below tools which is why some are missing pros and cons. We have collected all information that is publicly available.</p> Tool Description Features Pros Cons DBLDatagen Python based data generation tool - Scalable and predictable data generation across data scenarios- Plugin third-party libraries- Generate from existing data- Generate based on combination of other fields - Open source- Good documentation- Customisable and scalable- Generate from existing data/schemas - Limited support if issues- Code required- No clean up- No validation DataCebo Synthetic Data Vault Python based data generation tool with focus on ML generation, evaluating generated data - Create synthetic data using machine learning- Evaluate and visualize synthetic data- Preprocess, anonymize and define constraints Tonic Platform solution for generating data - Integration with many data sources- UI with RBAC- Quality and security checks- Auditing and alerting- Dashboards and reporting Datafaker Realistic data generation library - Generate realistic data- Push to CSV/JSON format- Create your own data providers- Performant - Simple, easy to use- Extensible- Open source- Realistic values - Have to code for and manage data source connections- No data clean up- No validation- No foreign keys support Gatling HTTP API load testing tool - Load testing- Validating data and responses- Scenario testing- Reporting- Extensive API support- Integration with CI/CD tools - Widely used- Clear documentation- Extensive testing/validation support- Customisable scenarios - Only supports HTTP, JMS and JDBC- No data clean up- Data feeders not based off metadata Tricentis - Data integrity Testing tool that focuses on data integrity - Data testing- Pre-screening data- Reconciliation, profiling and report testing- Support SQL DB, noSQL DB, files, API Broadcom - Test data manager Test data provisioning tool with PII detection and reusable datasets - Identify sensitive data- Generate synthetic data- Store and reuse existing data- Create virtual copies of data"},{"location":"use-case/roadmap/","title":"Roadmap","text":"<ul> <li>Support for other data sources<ul> <li>GCP and Azure related data services, Delta, RabbitMQ, ActiveMQ</li> </ul> </li> <li>Metadata discovery for HTTP, JMS</li> <li> API for developers and testers</li> <li>UI for metadata and data generation</li> <li> Report for data generated and validation rules</li> <li>Metadata stored in database</li> <li>Integration with existing metadata services (i.e. Amundsen, Datahub, Schema Registry)</li> <li>Data dictionary<ul> <li>Business definitions of fields that can be referenced for metadata across all data sources</li> </ul> </li> <li> Verification rules after data generation</li> <li> Validation waiting conditions<ul> <li>Webhook</li> <li>File exists</li> <li>Data exists via SQL expression</li> <li>Pause</li> </ul> </li> <li>Extend validation types<ul> <li>Aggregates (sum of amount per account is &gt; 500)</li> <li>Ordering (transactions are ordered by date)</li> <li>Relationship (at least one account entry in history table per account in accounts table)</li> <li>Data profile (how close the generated data profile is compared to the expected data profile)</li> </ul> </li> <li>Alerting</li> <li>Overriding tasks<ul> <li>Can customise tasks without copying whole schema definitions</li> <li>Easier to create scenarios</li> </ul> </li> <li>Integration with Great Expectations<ul> <li>Data validations can reference expectations</li> </ul> </li> <li>Gradle plugin</li> <li>Metadata improvements<ul> <li>PII detection</li> <li>Relationship detection across data sources</li> <li>SQL generation</li> </ul> </li> <li>Code generation</li> <li>Schema generation from Scala/Java class</li> </ul>"},{"location":"use-case/use-case/","title":"Use cases","text":""},{"location":"use-case/use-case/#replicate-production-in-lower-environment","title":"Replicate production in lower environment","text":"<p>Having a stable and reliable test environment is a challenge for a number of companies, especially where teams are asynchronously deploying and testing changes at faster rates. Data Caterer can help alleviate these issues by doing the following:</p> <ol> <li>Generates data with the latest schema changes and production like field values</li> <li>Run as a job on a daily/regular basis to replicate production traffic or data flows</li> <li>Validate data to ensure your system runs as expected</li> <li>Clean up data to avoid build up of generated data</li> </ol> <p></p>"},{"location":"use-case/use-case/#local-development","title":"Local development","text":"<p>Similar to the above, being able to replicate production like data in your local environment can be key to developing more reliable code as you can test directly against data in your local computer. This has a number of benefits including:</p> <ol> <li>Fewer assumptions or ambiguities when the developer codes</li> <li>Direct feedback loop in local computer rather than waiting for test environment for more reliable test data</li> <li>No domain expertise required to understand the data</li> <li>Easy for new developers to be onboarded and developing/testing code for jobs/services</li> </ol>"},{"location":"use-case/use-case/#systemintegration-testing","title":"System/integration testing","text":"<p>When working with third-party, external or internal data providers, it can be difficult to have all setup ready to produce reliable data that abides by relationship contracts between each of the systems. You have to rely on these data providers in order for you to run your tests which may not align to their priorities. With Data Caterer, you can generate the same data that they would produce, along with maintaining referential integrity across the data providers, so that you can run your tests without relying on their systems being up and reliable in their corresponding lower environments.</p>"},{"location":"use-case/use-case/#scenario-testing","title":"Scenario testing","text":"<p>If you want to set up particular data scenarios, you can customise the generated data to fit your scenario. Once the data gets generated and is consumed, you can also run validations to ensure your system has consumed the data correctly. These scenarios can be put together from existing tasks or data sources can be enabled/disabled based on your requirement. Built into Data Caterer and controlled via feature flags, is the ability to test edge cases based on the data type of the fields used for data generation (<code>enableEdgeCases</code> flag within <code>&lt;field&gt;.generator.options</code>, see more here).</p>"},{"location":"use-case/use-case/#data-debugging","title":"Data debugging","text":"<p>When data related issues occur in production, it may be difficult to replicate in a lower or local environment. It could be related to specific fields not containing expected results, size of data is too large or missing corresponding referenced data. This becomes key to resolving the issue as you can directly code against the exact data scenario and have confidence that your code changes will fix the problem. Data Caterer can be used to generate the appropriate data in whichever environment you want to test your changes against.</p>"},{"location":"use-case/use-case/#data-profiling","title":"Data profiling","text":"<p>When using Data Caterer with the feature flag <code>enableGeneratePlanAndTasks</code> enabled (see here), metadata relating all the fields defined in the data sources you have configured will be generated via data profiling. You can run this as a standalone job (can disable <code>enableGenerateData</code>)  so that you can focus on the profile of the data you are utilising. This can be run against your production data sources  to ensure the metadata can be used to accurately generate data in other environments. This is a key feature of Data  Caterer as no direct production connections need to be maintained to generate data in other environments (which can  lead to serious concerns about data security as seen here).</p>"},{"location":"use-case/use-case/#schema-gathering","title":"Schema gathering","text":"<p>When using Data Caterer with the feature flag <code>enableGeneratePlanAndTasks</code> enabled (see here), all schemas of the data sources defined will be tracked in a common format (as tasks). This data, along with the data profiling metadata, could then feed back into your schema registries to help keep them up to date with your system.</p>"}]}