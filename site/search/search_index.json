{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Using Data Caterer, you have the ability to generate production like data based on any source/target system whether it be a CSV file, database table, etc. anywhere you want the data to be. Whether it be in a test environment or even in your local laptop. Just define your data source connections and data will be generated. It can also be manually altered to produce data or scenarios the way you want.</p> <p>Main features of the data generator include:</p> <ul> <li> Metadata discovery</li> <li> Batch or  event data generation</li> <li> Maintain referential integrity across any dataset</li> <li> Create custom data generation scenarios</li> <li> Clean up generated data</li> <li> Validate data</li> </ul>"},{"location":"advanced/advanced/","title":"Advanced use cases","text":""},{"location":"advanced/advanced/#special-data-formats","title":"Special data formats","text":"<p>There are many options available for you to use when you have a scenario when data has to be a certain format.</p> <ol> <li>Create expression datafaker<ol> <li>Can be used to create names, addresses, or anything that can be found    under here</li> </ol> </li> <li>Create regex</li> </ol>"},{"location":"advanced/advanced/#foreign-keys-across-data-sets","title":"Foreign keys across data sets","text":"<p>If you have a use case where you require a columns value to match in another data set, this can be achieved in the plan definition. For example, if I have the column <code>account_number</code> in a data source named <code>customer-postgres</code> and column <code>account_id</code> in <code>transaction-cassandra</code>,</p> <pre><code>sinkOptions:\nforeignKeys:\n#The foreign key name with naming convention [dataSourceName].[stepName].[columnName]\n\"customer-postgres.accounts.account_number\":\n#List of columns to match with same naming convention\n- \"transaction-cassandra.transactions.account_id\"\n</code></pre> <p>Sample can be found here. You can define any number of foreign key relationships as you want.</p>"},{"location":"advanced/advanced/#edge-cases","title":"Edge cases","text":"<p>For each given data type, there are edge cases which can cause issues when your application processes the data. This can be controlled at a column level by including the following flag in the generator options:</p> <pre><code>fields:\n- name: \"amount\"\ntype: \"double\"\ngenerator:\ntype: \"random\"\noptions:\nenableEdgeCases: \"true\" </code></pre> <p>If you want to know all the possible edge cases for each data type, can check the documentation here.</p>"},{"location":"advanced/advanced/#scenario-testing","title":"Scenario testing","text":"<p>You can create specific scenarios by adjusting the metadata found in the plan and tasks to your liking. For example, if you had two data sources, a Postgres database and a parquet file, and you wanted to save account data into Postgres and transactions related to those accounts into a parquet file. You can alter the <code>status</code> column in the account data to only generate <code>open</code> accounts and define a foreign key between Postgres and parquet to ensure the same <code>account_id</code> is being used. Then in the parquet task, define 1 to 10 transactions per <code>account_id</code> to be generated.</p> <p>Postgres account generation example task Parquet transaction generation example task Plan</p>"},{"location":"advanced/advanced/#storing-plantasks-in-cloud-storage","title":"Storing plan/task(s) in cloud storage","text":"<p>You can generate and store the plan/task files inside either AWS S3, Azure Blob Storage or Google GCS. This can be controlled via configuration set in the <code>application.conf</code> file where you can set something like the below:</p> <pre><code>folders {\ngeneratedPlanAndTaskFolderPath = \"s3a://my-bucket/data-caterer/generated\"\nplanFilePath = \"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\"\ntaskFolderPath = \"s3a://my-bucket/data-caterer/generated/task\"\n}\n\nspark {\nconfig {\n...\n#S3\n\"spark.hadoop.fs.s3a.directory.marker.retention\" = \"keep\"\n\"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" = \"true\"\n\"spark.hadoop.fs.defaultFS\" = \"s3a://my-bucket\"\n#can change to other credential providers as shown here\n#https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n\"spark.hadoop.fs.s3a.aws.credentials.provider\" = \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n\"spark.hadoop.fs.s3a.access.key\" = \"access_key\"\n\"spark.hadoop.fs.s3a.secret.key\" = \"secret_key\"\n}\n}\n</code></pre>"},{"location":"get-started/docker/","title":"Run Data Caterer","text":""},{"location":"get-started/docker/#docker","title":"Docker","text":""},{"location":"get-started/docker/#quick-start","title":"Quick start","text":"<pre><code>git clone git@github.com:pflooky/data-caterer-docs.git\ncd data-caterer-docs/docs/sample/docker\ndocker-compose up -d datacaterer\n</code></pre> <p>To run for another data source, you can set <code>DATA_SOURCE</code> like below:</p> <pre><code>DATA_SOURCE=postgres docker-compose up -d datacaterer\n</code></pre> <p>Can set it to one of the following:</p> <ul> <li>postgres</li> <li>mysql</li> <li>cassandra</li> <li>solace</li> <li>kafka</li> <li>http</li> </ul> <p>If you want to test it out with your own setup, you can alter the corresponding files under docs/sample/docker/data</p>"},{"location":"get-started/docker/#report","title":"Report","text":"<p>Check the report generated under <code>docs/sample/docker/data/custom/report/index.html</code>.</p> <p>Sample report can also be seen here</p>"},{"location":"get-started/docker/#api","title":"API","text":"<p>A Java and Scala API is available to use to create your own data generation scenario.</p> <p>You can check out the example project found here via:</p> <pre><code>git clone git@github.com:pflooky/data-caterer-example.git\n</code></pre> JavaScala <pre><code>import com.github.pflooky.datacaterer.api.java.PlanRun;\nimport com.github.pflooky.datacaterer.api.model.ArrayType;\nimport com.github.pflooky.datacaterer.api.model.DateType;\nimport com.github.pflooky.datacaterer.api.model.DoubleType;\nimport com.github.pflooky.datacaterer.api.model.IntegerType;\n\nimport java.sql.Date;\n\npublic class DocumentationJavaPlanRun extends PlanRun {\n{\nvar myJson = json(\"account_info\", \"/tmp/data-caterer/json\")\n.numPartitions(1)\n.schema(\nfield().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\nfield().name(\"year\").type(IntegerType.instance()).min(2022).max(2023),\nfield().name(\"name\").expression(\"#{Name.name}\"),\nfield().name(\"amount\").type(DoubleType.instance()).max(1000.0),\nfield().name(\"date\").type(DateType.instance()).min(Date.valueOf(\"2022-01-01\")),\nfield().name(\"status\").oneOf(\"open\", \"closed\"),\nfield().name(\"txn_list\").type(ArrayType.instance())\n.arrayMaxLength(2)\n.schema(schema().addFields(\nfield().name(\"id\").regex(\"ID[0-9]{4}\"),\nfield().name(\"date\").type(DateType.instance()).min(Date.valueOf(\"2022-01-01\")),\nfield().name(\"amount\").type(DoubleType.instance()).max(1000.0)\n))\n)\n.count(count().total(100));\n\nexecute(myJson);\n}\n}\n</code></pre> <pre><code>import com.github.pflooky.datacaterer.api.PlanRun\nimport com.github.pflooky.datacaterer.api.model.{ArrayType, DateType, DoubleType}\n\nimport java.sql.Date\n\nclass ExamplePlanRun extends PlanRun {\n\nval jsonTask = json(\"account_info\", \"/tmp/data-caterer/json\")\n.schema(\nfield.name(\"account_id\"),\nfield.name(\"year\").`type`(IntegerType).min(2022),\nfield.name(\"name\").expression(\"#{Name.name}\"),\nfield.name(\"amount\").`type`(DoubleType).max(1000.0),\nfield.name(\"date\").`type`(DateType).min(Date.valueOf(\"2022-01-01\")),\nfield.name(\"status\").oneOf(\"open\", \"closed\"),\nfield.name(\"txn_list\")\n.`type`(ArrayType)\n.schema(schema.addFields(\nfield.name(\"id\"),\nfield.name(\"date\").`type`(DateType).min(Date.valueOf(\"2022-01-01\")),\nfield.name(\"amount\").`type`(DoubleType),\n))\n)\n.count(count.total(100))\n\nexecute(jsonTask)\n}\n</code></pre>"},{"location":"get-started/docker/#run-with-api","title":"Run with API","text":"<p>Once you have cloned the data-caterer-example repo, it is easiest to run via the <code>run.sh</code> script that helps package up  the API into a jar, then mount it to the docker image, ready to be run. It will prompt you for the class name you want to run for.</p>"},{"location":"get-started/docker/#run-with-multiple-sub-data-sources","title":"Run with multiple sub data sources","text":"<p>In the context of Postgres data sources, tables are sub data sources that data can be generated for.</p> <p>Try to run the following command:</p> <pre><code>PLAN=plan/postgres-multiple-tables docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#run-with-multiple-data-sources","title":"Run with multiple data sources","text":""},{"location":"get-started/docker/#postgres-and-csv-file","title":"Postgres and CSV File","text":"<pre><code>PLAN=plan/scenario-based docker-compose up -d datacaterer\nhead data/custom/csv/transactions/part-00000*\nsample_account=$(head -1 data/custom/csv/transactions/part-00000* | awk -F \",\" '{print $1}')\ndocker exec docker-postgres-1 psql -Upostgres -d customer -c \"SELECT * FROM account.accounts WHERE account_number='$sample_account'\"\n</code></pre> <p>You should be able to see the linked data between Postgres and the CSV file created along with 1 to 10 records per account_id, name combination in the CSV file.</p>"},{"location":"get-started/docker/#run-with-custom-data-sources","title":"Run with custom data sources","text":"<ol> <li>Create/alter plan    under <code>data/custom/plan</code></li> <li>Create/alter tasks    under <code>data/custom/task</code></li> <li>Define your schemas and generator configurations such as record count</li> <li>Create/alter application    configuration <code>data/custom/application.conf</code></li> <li>This is where you define your connection properties and other flags/configurations</li> </ol> <pre><code>DATA_SOURCE=&lt;data source name&gt; docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#generate-plan-and-tasks","title":"Generate plan and tasks","text":"<pre><code>APPLICATION_CONFIG_PATH=/opt/app/custom/application-dvd.conf ENABLE_GENERATE_DATA=false ENABLE_GENERATE_PLAN_AND_TASKS=true DATA_SOURCE=postgresdvd docker-compose up -d datacaterer\ncat data/custom/generated/plan/plan_*\n</code></pre>"},{"location":"get-started/docker/#generate-data-with-record-tracking","title":"Generate data with record tracking","text":"<pre><code>APPLICATION_CONFIG_PATH=/opt/app/custom/application-dvd.conf ENABLE_GENERATE_DATA=true ENABLE_GENERATE_PLAN_AND_TASKS=false ENABLE_RECORD_TRACKING=true DATA_SOURCE=postgresdvd PLAN=generated/plan/$(ls data/custom/generated/plan/ | grep plan | head -1 | awk -F \" \" '{print $NF}' | sed 's/\\.yaml//g') docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#delete-the-generated-data","title":"Delete the generated data","text":"<pre><code>APPLICATION_CONFIG_PATH=/opt/app/custom/application-dvd.conf ENABLE_GENERATE_DATA=false ENABLE_GENERATE_PLAN_AND_TASKS=false ENABLE_DELETE_GENERATED_RECORDS=true DATA_SOURCE=postgresdvd PLAN=generated/plan/$(ls data/custom/generated/plan/ | grep plan | head -1 | awk -F \" \" '{print $NF}' | sed 's/\\.yaml//g') docker-compose up -d datacaterer\n</code></pre>"},{"location":"get-started/docker/#helm","title":"Helm","text":"<p>Link to sample helm on GitHub here</p> <p>Update the configuration to your own data connections and configuration.</p> <pre><code>git clone git@github.com:pflooky/data-caterer-docs.git\nhelm install data-caterer ./data-caterer-docs/helm/data-caterer\n</code></pre>"},{"location":"roadmap/roadmap/","title":"Roadmap","text":"<ul> <li>Support for other data sources<ul> <li>GCP and Azure related data services, Delta, RabbitMQ, ActiveMQ</li> </ul> </li> <li>Metadata discovery for HTTP, JMS</li> <li> API for developers and testers</li> <li>UI for metadata and data generation</li> <li> Report for data generated and validation rules</li> <li>Metadata stored in database</li> <li>Integration with existing metadata services (i.e. amundsen, datahub)</li> <li>Data dictionary<ul> <li>Business definitions of fields that can be referenced for metadata across all data sources</li> </ul> </li> <li> Verification rules after data generation</li> <li> Validation waiting conditions<ul> <li>Webhook</li> <li>File exists</li> <li>Data exists via SQL expression</li> <li>Pause</li> </ul> </li> <li>Alerting</li> <li>Overriding tasks<ul> <li>Can customise tasks without copying whole schema definitions</li> <li>Easier to create scenarios</li> </ul> </li> <li>Integration with Great Expectations<ul> <li>Data validations can reference expectations</li> </ul> </li> <li>Gradle plugin</li> </ul>"},{"location":"sample/","title":"Samples","text":"<p>Below are examples of different types of plans and tasks that can be helpful when trying to create your own. You can use these as a template or to search for something related to your particular use case.</p>"},{"location":"sample/#base-concept","title":"Base Concept","text":"<p>The execution of the data generator is based on the concept of plans and tasks. A plan represent the set of tasks that need to be executed,  along with other information that spans across tasks, such as foreign keys between data sources. A task represent the component(s) of a data source and its associated metadata so that it understands what the data should look like  and how many steps (sub data sources) there are (i.e. tables in a database, topics in Kafka). Tasks can define one or more steps.</p>"},{"location":"sample/#plan","title":"Plan","text":""},{"location":"sample/#foreign-keys","title":"Foreign Keys","text":"<p>Define foreign keys across data sources in your plan to ensure generated data can match Link to associated task 1 Link to associated task 2</p>"},{"location":"sample/#task","title":"Task","text":"Data Source Type Data Source Sample Task Notes Database Postgres Sample Database MySQL Sample Database Cassandra Sample File CSV Sample File JSON Sample Contains nested schemas and use of SQL for generated values File Parquet Sample Partition by year column Kafka Kafka Sample Specific base schema to be used, define headers, key, value, etc. JMS Solace Sample JSON formatted message HTTP PUT Sample JSON formatted PUT body"},{"location":"sample/#configuration","title":"Configuration","text":"<p>Basic configuration</p>"},{"location":"sample/docker/","title":"Data Caterer - Docker Compose","text":"<p>If you want to try out data caterer generating data for various data sources, you do use the following docker-compose file.</p> <p>All you need to do is define which data source you want to run with via a command like below:</p> <pre><code>DATA_SOURCE=postgres docker-compose up -d datacaterer\n</code></pre> <p>You can change <code>DATA_SOURCE</code> to one of the following: - postgres - mysql - cassandra - solace - kafka - http</p>"},{"location":"setup/","title":"Setup","text":"<p>All the configurations and customisation related to Data Caterer can be found under here.</p> <ul> <li> Configurations - Configurations relating to feature flags, folder pathways, metadata analysis</li> <li> Connections - Explore the data source connections available</li> <li> Generators - Choose and configure the type of generator you want used for fields</li> <li> Validations - How to validate data to ensure your system is performing as expected</li> </ul>"},{"location":"setup/configuration/","title":"Configuration","text":"<p>A number of configurations can be made and customised within Data Caterer to help control what gets run and/or where any metadata gets saved.</p> <p>These configurations are defined from within your <code>application.conf</code> file as seen here.</p>"},{"location":"setup/configuration/#flags","title":"Flags","text":"<p>Flags are used to control which processes are executed when you run Data Caterer.</p> Config Default Paid Description <code>enableGenerateData</code> true N Enable/disable data generation <code>enableCount</code> true N Count the number of records generated. Can be disabled to improve performance <code>enableFailOnError</code> true N Whilst saving generated data, if there is an error, it will stop any further data from being generated <code>enableSaveReports</code> true N Enable/disable HTML reports summarising data generated, metadata of data generated (if <code>enableSinkMetadata</code> is enabled) and validation results (if <code>enableValidation</code> is enabled). Sample here <code>enableSinkMetadata</code> true N Run data profiling for the generated data. Shown in HTML reports if <code>enableSaveSinkMetadata</code> is enabled <code>enableValidation</code> false N Run validations as described in plan. Results can be viewed from logs or from HTML report if <code>enableSaveSinkMetadata</code> is enabled. Sample here <code>enableGeneratePlanAndTasks</code> false Y Enable/disable plan and task auto generation based off data source connections <code>enableRecordTracking</code> false Y Enable/disable which data records have been generated for any data source <code>enableDeleteGeneratedRecords</code> false Y Delete all generated records based off record tracking (if <code>enableRecordTracking</code> has been set to true) JavaScalaapplication.conf <pre><code>configuration()\n.enableGenerateData(true)\n.enableCount(true)\n.enableFailOnError(true)\n.enableSaveReports(true)\n.enableSinkMetadata(true)\n.enableValidation(false)\n.enableGeneratePlanAndTasks(false)\n.enableRecordTracking(false)\n.enableDeleteGeneratedRecords(false);\n</code></pre> <pre><code>configuration\n.enableGenerateData(true)\n.enableCount(true)\n.enableFailOnError(true)\n.enableSaveReports(true)\n.enableSinkMetadata(true)\n.enableValidation(false)\n.enableGeneratePlanAndTasks(false)\n.enableRecordTracking(false)\n.enableDeleteGeneratedRecords(false)\n</code></pre> <pre><code>flags {\n  enableCount = false\n  enableCount = ${?ENABLE_COUNT}\n  enableGenerateData = true\n  enableGenerateData = ${?ENABLE_GENERATE_DATA}\n  enableFailOnError = true\n  enableFailOnError = ${?ENABLE_FAIL_ON_ERROR}\n  enableGeneratePlanAndTasks = false\n  enableGeneratePlanAndTasks = ${?ENABLE_GENERATE_PLAN_AND_TASKS}\n  enableRecordTracking = false\n  enableRecordTracking = ${?ENABLE_RECORD_TRACKING}\n  enableDeleteGeneratedRecords = false\n  enableDeleteGeneratedRecords = ${?ENABLE_DELETE_GENERATED_RECORDS}\n}\n</code></pre>"},{"location":"setup/configuration/#folders","title":"Folders","text":"<p>Depending on which flags are enabled, there are folders that get used to save metadata, store HTML reports or track the records generated.</p> <p>These folder pathways can be defined as a cloud storage pathway (i.e. <code>s3a://my-bucket/task</code>).</p> Config Default Paid Description <code>planFilePath</code> /opt/app/plan/customer-create-plan.yaml N Plan file path to use when generating and/or validating data <code>taskFolderPath</code> /opt/app/task N Task folder path that contains all the task files (can have nested directories) <code>validationFolderPath</code> /opt/app/validation N Validation folder path that contains all the validation files (can have nested directories) <code>generatedReportsFolderPath</code> /opt/app/report N Where HTML reports get generated that contain information about data generated along with any validations performed <code>generatedPlanAndTaskFolderPath</code> /tmp Y Folder path where generated plan and task files will be saved <code>recordTrackingFolderPath</code> /opt/app/record-tracking Y Where record tracking parquet files get saved JavaScalaapplication.conf <pre><code>configuration()\n.planFilePath(\"/opt/app/custom/plan/postgres-plan.yaml\")\n.taskFolderPath(\"/opt/app/custom/task\")\n.validationFolderPath(\"/opt/app/custom/validation\")\n.generatedReportsFolderPath(\"/opt/app/custom/report\")\n.generatedPlanAndTaskFolderPath(\"/opt/app/custom/generated\")\n.recordTrackingFolderPath(\"/opt/app/custom/record-tracking\");\n</code></pre> <pre><code>configuration\n.planFilePath(\"/opt/app/custom/plan/postgres-plan.yaml\")\n.taskFolderPath(\"/opt/app/custom/task\")\n.validationFolderPath(\"/opt/app/custom/validation\")\n.generatedReportsFolderPath(\"/opt/app/custom/report\")\n.generatedPlanAndTaskFolderPath(\"/opt/app/custom/generated\")\n.recordTrackingFolderPath(\"/opt/app/custom/record-tracking\")\n</code></pre> <pre><code>folders {\n  planFilePath = \"/opt/app/custom/plan/postgres-plan.yaml\"\n  planFilePath = ${?PLAN_FILE_PATH}\n  taskFolderPath = \"/opt/app/custom/task\"\n  taskFolderPath = ${?TASK_FOLDER_PATH}\n  validationFolderPath = \"/opt/app/custom/validation\"\n  validationFolderPath = ${?VALIDATION_FOLDER_PATH}\n  generatedReportsFolderPath = \"/opt/app/custom/report\"\n  generatedReportsFolderPath = ${?GENERATED_REPORTS_FOLDER_PATH}\n  generatedPlanAndTaskFolderPath = \"/opt/app/custom/generated\"\n  generatedPlanAndTaskFolderPath = ${?GENERATED_PLAN_AND_TASK_FOLDER_PATH}\n  recordTrackingFolderPath = \"/opt/app/custom/record-tracking\"\n  recordTrackingFolderPath = ${?RECORD_TRACKING_FOLDER_PATH}\n}\n</code></pre>"},{"location":"setup/configuration/#metadata","title":"Metadata","text":"<p>When metadata gets generated, there are some configurations that can be altered to help with performance or accuracy related issues. Metadata gets generated from two processes: 1) if <code>enableGeneratePlanAndTasks</code> or 2) if <code>enableSinkMetadata</code> are enabled.</p> <p>During the generation of plan and tasks, data profiling is used to create the metadata for each of the fields defined in the data source. You may face issues if the number of records in the data source is large as data profiling is an expensive task. Similarly, it can be expensive when analysing the generated data if the number of records generated is large.</p> Config Default Paid Description <code>numRecordsFromDataSource</code> 10000 Y Number of records read in from the data source that could be used for data profiling <code>numRecordsForAnalysis</code> 10000 Y Number of records used for data profiling from the records gathered in <code>numRecordsFromDataSource</code> <code>oneOfMinCount</code> 1000 N Minimum number of records required before considering if a field can be of type <code>oneOf</code> <code>oneOfDistinctCountVsCountThreshold</code> 0.2 Y Threshold ratio to determine if a field is of type <code>oneOf</code> (i.e. a field called <code>status</code> that only contains <code>open</code> or <code>closed</code>. Distinct count = 2, total count = 10, ratio = 2 / 10 = 0.2 therefore marked as <code>oneOf</code>) <code>numGeneratedSamples</code> 10 N Number of sample records from generated data to take. Shown in HTML report JavaScalaapplication.conf <pre><code>configuration()\n.numRecordsFromDataSourceForDataProfiling(10000)\n.numRecordsForAnalysisForDataProfiling(10000)\n.oneOfMinCount(1000)\n.oneOfDistinctCountVsCountThreshold(1000)\n.numGeneratedSamples(10);\n</code></pre> <pre><code>configuration\n.numRecordsFromDataSourceForDataProfiling(10000)\n.numRecordsForAnalysisForDataProfiling(10000)\n.oneOfMinCount(1000)\n.oneOfDistinctCountVsCountThreshold(1000)\n.numGeneratedSamples(10)\n</code></pre> <pre><code>metadata {\n  numRecordsFromDataSource = 10000\n  numRecordsForAnalysis = 10000\n  oneOfMinCount = 1000\n  oneOfDistinctCountVsCountThreshold = 0.2\n  numGeneratedSamples = 10\n}\n</code></pre>"},{"location":"setup/configuration/#generation","title":"Generation","text":"<p>When generating data, you may have some limitations such as limited CPU or memory, large number of data sources, or data sources prone to failure under load. To help alleviate these issues or speed up performance, you can control the number of records that get generated in each batch.</p> Config Default Paid Description <code>numRecordsPerBatch</code> 100000 N Number of records across all data sources to generate per batch <code>numRecordsPerStep</code> N Overrides the count defined in each step with this value if defined (i.e. if set to 1000, for each step, 1000 records will be generated) ScalaScalaapplication.conf <pre><code>configuration()\n.numRecordsPerBatch(100000)\n.numRecordsPerStep(1000);\n</code></pre> <pre><code>configuration\n.numRecordsPerBatch(100000)\n.numRecordsPerStep(1000)\n</code></pre> <pre><code>generation {\n  numRecordsPerBatch = 100000\n  numRecordsPerStep = 1000\n}\n</code></pre>"},{"location":"setup/configuration/#spark","title":"Spark","text":"<p>Given Data Caterer uses Spark as the base framework for data processing, you can configure the job as to your  specifications via configuration as seen here.</p> JavaScalaapplication.conf <pre><code>configuration()\n.sparkMaster(\"local[*]\")\n.sparkConfig(Map.of(\"spark.driver.cores\", \"5\"))\n.addSparkConfig(\"spark.driver.memory\", \"10g\");\n</code></pre> <pre><code>configuration\n.sparkMaster(\"local[*]\")\n.sparkConfig(Map(\"spark.driver.cores\" -&gt; \"5\"))\n.addSparkConfig(\"spark.driver.memory\" -&gt; \"10g\")\n</code></pre> <pre><code>spark {\n  master = \"local[*]\"\n  master = ${?SPARK_MASTER}\n  config {\n    \"spark.driver.memory\" = \"2g\"\n    \"spark.executor.memory\" = \"2g\"\n    \"spark.sql.cbo.enabled\" = \"true\"\n    \"spark.sql.adaptive.enabled\" = \"true\"\n    \"spark.sql.cbo.planStats.enabled\" = \"true\"\n    \"spark.sql.legacy.allowUntypedScalaUDF\" = \"true\"\n    \"spark.sql.statistics.histogram.enabled\" = \"true\"\n    \"spark.sql.shuffle.partitions\" = \"4\"\n    \"spark.sql.catalog.postgres\" = \"\"\n    \"spark.sql.catalog.cassandra\" = \"com.datastax.spark.connector.datasource.CassandraCatalog\"\n    \"spark.hadoop.fs.s3a.directory.marker.retention\" = \"keep\"\n    \"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" = \"true\"\n  }\n}\n</code></pre>"},{"location":"setup/connection/connection/","title":"Data Source Connections","text":"<p>Details of all the connection configuration supported can be found in the below subsections for each type of connection.</p> <p>These configurations can be done via API or from configuration. Examples of both are shown for each data source below.</p>"},{"location":"setup/connection/connection/#supported-data-connections","title":"Supported Data Connections","text":"Data Source Type Data Source Database Postgres, MySQL, Cassandra File CSV, JSON, ORC, Parquet Kafka Kafka JMS Solace HTTP GET, PUT, POST, DELETE, PATCH, HEAD, TRACE, OPTIONS"},{"location":"setup/connection/connection/#api","title":"API","text":"<p>All connection details require a name. Depending on the data source, you can define additional options which may be used by the driver or connector for connecting to the data source.</p>"},{"location":"setup/connection/connection/#configuration-file","title":"Configuration file","text":"<p>All connection details follow the same pattern.</p> <pre><code>&lt;connection format&gt; {\n    &lt;connection name&gt; {\n        &lt;key&gt; = &lt;value&gt;\n    }\n}\n</code></pre> <p>Overriding configuration</p> <p>When defining a configuration value that can be defined by a system property or environment variable at runtime, you can define that via the following:</p> <pre><code>url = \"localhost\"\nurl = ${?POSTGRES_URL}\n</code></pre> <p>The above defines that if there is a system property or environment variable named <code>POSTGRES_URL</code>, then that value will be used for the <code>url</code>, otherwise, it will default to <code>localhost</code>.</p>"},{"location":"setup/connection/connection/#data-sources","title":"Data sources","text":"<p>To find examples of a task for each type of data source, please check out this page.</p>"},{"location":"setup/connection/connection/#file","title":"File","text":"<p>Linked here is a list of generic options that can be included as part of your file data source configuration if required. Links to specific file type configurations can be found below.</p>"},{"location":"setup/connection/connection/#csv","title":"CSV","text":"JavaScalaapplication.conf <pre><code>csv(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>csv(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>csv {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?CSV_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for CSV can be found here</p>"},{"location":"setup/connection/connection/#json","title":"JSON","text":"JavaScalaapplication.conf <pre><code>json(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>json(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>json {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?JSON_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for JSON can be found here</p>"},{"location":"setup/connection/connection/#orc","title":"ORC","text":"JavaScalaapplication.conf <pre><code>orc(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>orc(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>orc {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?ORC_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for ORC can be found here</p>"},{"location":"setup/connection/connection/#parquet","title":"Parquet","text":"JavaScalaapplication.conf <pre><code>parquet(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>parquet(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>parquet {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?PARQUET_PATH}\n  }\n}\n</code></pre> <p>Other available configuration for Parquet can be found here</p>"},{"location":"setup/connection/connection/#delta-not-supported-yet","title":"Delta (not supported yet)","text":"JavaScalaapplication.conf <pre><code>delta(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>delta(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>delta {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?DELTA_PATH}\n  }\n}\n</code></pre>"},{"location":"setup/connection/connection/#jdbc","title":"JDBC","text":"<p>Follows the same configuration used by Spark as found here. Sample can be found below</p> JavaScalaapplication.conf <pre><code>postgres(\n\"customer_postgres\",                            #name\n\"jdbc:postgresql://localhost:5432/customer\",    #url\n\"postgres\",                                     #username\n\"postgres\"                                      #password\n)\n</code></pre> <pre><code>postgres(\n\"customer_postgres\",                            #name\n\"jdbc:postgresql://localhost:5432/customer\",    #url\n\"postgres\",                                     #username\n\"postgres\"                                      #password\n)\n</code></pre> <pre><code>jdbc {\n    customer_postgres {\n        url = \"jdbc:postgresql://localhost:5432/customer\"\n        url = ${?POSTGRES_URL}\n        user = \"postgres\"\n        user = ${?POSTGRES_USERNAME}\n        password = \"postgres\"\n        password = ${?POSTGRES_PASSWORD}\n        driver = \"org.postgresql.Driver\"\n    }\n}\n</code></pre> <p>Ensure that the user has write permission, so it is able to save the table to the target tables.</p> SQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre>"},{"location":"setup/connection/connection/#postgres","title":"Postgres","text":"<p>Can see example API or Config definition for Postgres connection above.</p>"},{"location":"setup/connection/connection/#permissions","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> SQL Permission Statements <pre><code>GRANT SELECT ON information_schema.tables TO &lt; user &gt;;\nGRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\nGRANT SELECT ON information_schema.table_constraints TO &lt; user &gt;;\nGRANT SELECT ON information_schema.constraint_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"setup/connection/connection/#mysql","title":"MySQL","text":"JavaScalaapplication.conf <pre><code>mysql(\n\"customer_mysql\",                       #name\n\"jdbc:mysql://localhost:3306/customer\", #url\n\"root\",                                 #username\n\"root\"                                  #password\n)\n</code></pre> <pre><code>mysql(\n\"customer_mysql\",                       #name\n\"jdbc:mysql://localhost:3306/customer\", #url\n\"root\",                                 #username\n\"root\"                                  #password\n)\n</code></pre> <pre><code>jdbc {\n    customer_mysql {\n        url = \"jdbc:mysql://localhost:3306/customer\"\n        user = \"root\"\n        password = \"root\"\n        driver = \"com.mysql.cj.jdbc.Driver\"\n    }\n}\n</code></pre>"},{"location":"setup/connection/connection/#permissions_1","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> SQL Permission Statements <pre><code>GRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.statistics TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"setup/connection/connection/#cassandra","title":"Cassandra","text":"<p>Follows same configuration as defined by the Spark Cassandra Connector as found here</p> JavaScalaapplication.conf <pre><code>cassandra(\n\"customer_cassandra\",   #name\n\"localhost:9042\",       #url\n\"cassandra\",            #username\n\"cassandra\"             #password\n)\n</code></pre> <pre><code>cassandra(\n\"customer_cassandra\",   #name\n\"localhost:9042\",       #url\n\"cassandra\",            #username\n\"cassandra\"             #password\n)\n</code></pre> <pre><code>org.apache.spark.sql.cassandra {\n    customer_cassandra {\n        spark.cassandra.connection.host = \"localhost\"\n        spark.cassandra.connection.host = ${?CASSANDRA_HOST}\n        spark.cassandra.connection.port = \"9042\"\n        spark.cassandra.connection.port = ${?CASSANDRA_PORT}\n        spark.cassandra.auth.username = \"cassandra\"\n        spark.cassandra.auth.username = ${?CASSANDRA_USERNAME}\n        spark.cassandra.auth.password = \"cassandra\"\n        spark.cassandra.auth.password = ${?CASSANDRA_PASSWORD}\n    }\n}\n</code></pre>"},{"location":"setup/connection/connection/#permissions_2","title":"Permissions","text":"<p>Ensure that the user has write permission, so it is able to save the table to the target tables.</p> CQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre> <p>Following permissions are required when generating plan and tasks:</p> CQL Permission Statements <pre><code>GRANT SELECT ON system_schema.tables TO &lt;user&gt;;\nGRANT SELECT ON system_schema.columns TO &lt;user&gt;;\n</code></pre>"},{"location":"setup/connection/connection/#kafka","title":"Kafka","text":"<p>Define your Kafka bootstrap server to connect and send generated data to corresponding topics. Topic gets set at a step level. Further details can be found here</p> JavaScalaapplication.conf <pre><code>kafka(\n\"customer_kafka\",   #name\n\"localhost:9092\"    #url\n)\n</code></pre> <pre><code>kafka(\n\"customer_kafka\",   #name\n\"localhost:9092\"    #url\n)\n</code></pre> <pre><code>kafka {\n    customer_kafka {\n        kafka.bootstrap.servers = \"localhost:9092\"\n        kafka.bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}\n    }\n}\n</code></pre> <p>When defining your schema for pushing data to Kafka, it follows a specific top level schema. An example can be found here. You can define the key, value, headers, partition or topic by following the linked schema.</p>"},{"location":"setup/connection/connection/#jms","title":"JMS","text":"<p>Uses JNDI lookup to send messages to JMS queue. Ensure that the messaging system you are using has your queue/topic registered via JNDI otherwise a connection cannot be created.</p> JavaScalaapplication.conf <pre><code>solace(\n\"customer_solace\",                                      #name\n\"smf://localhost:55554\",                                #url\n\"admin\",                                                #username\n\"admin\",                                                #password\n\"default\",                                              #vpn name\n\"/jms/cf/default\",                                      #connection factory\n\"com.solacesystems.jndi.SolJNDIInitialContextFactory\"   #initial context factory\n)\n</code></pre> <pre><code>solace(\n\"customer_solace\",                                      #name\n\"smf://localhost:55554\",                                #url\n\"admin\",                                                #username\n\"admin\",                                                #password\n\"default\",                                              #vpn name\n\"/jms/cf/default\",                                      #connection factory\n\"com.solacesystems.jndi.SolJNDIInitialContextFactory\"   #initial context factory\n)\n</code></pre> <pre><code>jms {\n    customer_solace {\n        initialContextFactory = \"com.solacesystems.jndi.SolJNDIInitialContextFactory\"\n        connectionFactory = \"/jms/cf/default\"\n        url = \"smf://localhost:55555\"\n        url = ${?SOLACE_URL}\n        user = \"admin\"\n        user = ${?SOLACE_USER}\n        password = \"admin\"\n        password = ${?SOLACE_PASSWORD}\n        vpnName = \"default\"\n        vpnName = ${?SOLACE_VPN}\n    }\n}\n</code></pre>"},{"location":"setup/connection/connection/#http","title":"HTTP","text":"<p>Define any username and/or password needed for the HTTP requests. The url is defined in the tasks to allow for generated data to be populated in the url.</p> JavaScalaapplication.conf <pre><code>http(\n\"customer_api\", #name\n\"admin\",        #username\n\"admin\"         #password\n)\n</code></pre> <pre><code>http(\n\"customer_api\", #name\n\"admin\",        #username\n\"admin\"         #password\n)\n</code></pre> <pre><code>http {\n    customer_api {\n        user = \"admin\"\n        user = ${?HTTP_USER}\n        password = \"admin\"\n        password = ${?HTTP_PASSWORD}\n    }\n}\n</code></pre>"},{"location":"setup/generator/count/","title":"Record Count","text":"<p>There are options related to controlling the number of records generated that can help in generating the scenarios or data required.</p>"},{"location":"setup/generator/count/#total-count","title":"Total Count","text":"<p>Total count is the simplest as you define the total number of records you require for that particular step. For example, in the below step, it will generate 1000 records for the CSV file  </p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(1000);\n\n#or\n\ntask()\n.name(\"csv_file\")\n.steps(\nstep()\n.name(\"transactions\")\n.type(\"csv\")\n.option(\"path\", \"app/src/test/resources/sample/csv/transactions\")\n.count(1000)\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(1000)\n\n#or\n\ntask\n.name(\"csv_file\")\n.step(\nstep\n.name(\"transactions\")\n.`type`(\"csv\")\n.option(\"path\", \"app/src/test/resources/sample/csv/transactions\")\n.count(1000)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\ncount:\ntotal: 1000\n</code></pre>"},{"location":"setup/generator/count/#generated-count","title":"Generated Count","text":"<p>As like most things in data-caterer, the count can be generated based on some metadata. For example, if I wanted to generate between 1000 and 2000 records, I could define that by the below configuration:</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(generator().min(1000).max(2000));\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(generator.min(1000).max(2000))\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\ncount:\ngenerator:\ntype: \"random\"\noptions:\nmin: 1000\nmax: 2000\n</code></pre>"},{"location":"setup/generator/count/#per-column-count","title":"Per Column Count","text":"<p>When defining a per column count, this allows you to generate records \"per set of columns\". This means that for a given set of columns, it will generate a particular amount of records per combination of values for those columns.  </p> <p>One example of this would be when generating transactions relating to a customer. A customer may be defined by columns <code>account_id, name</code>. A number of transactions would be generated per <code>account_id,name</code>.  </p> <p>You can also use a combination of the above two methods to generate the number of records per column.</p>"},{"location":"setup/generator/count/#total","title":"Total","text":"<p>When defining a total count within the <code>perColumn</code> configuration, it translates to only creating <code>(count.total * count.perColumnTotal)</code> records. This is a fixed number of records that will be generated each time, with no variation between runs.</p> <p>In the example below, we have <code>count.total = 1000</code> and <code>count.perColumnTotal = 2</code>. Which means that <code>1000 * 2 = 2000</code> records will be generated for this CSV file every time data gets generated.</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(\ncount()\n.total(1000)\n.perColumnTotal(2, \"account_id\", \"name\")\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(\ncount\n.total(1000)\n.perColumnTotal(2, \"account_id\", \"name\")\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\ncount:\ntotal: 1000\nperColumn:\ntotal: 2\ncolumnNames:\n- \"account_id\"\n- \"name\"\n</code></pre>"},{"location":"setup/generator/count/#generated","title":"Generated","text":"<p>You can also define a generator for the count per column. This can be used in scenarios where you want a variable number of records per set of columns.</p> <p>In the example below, it will generate between <code>(count.total * count.perColumnGenerator.generator.min) = (1000 * 1) = 1000</code> and <code>(count.total * count.perColumnGenerator.generator.max) = (1000 * 2) = 2000</code> records.</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(\ncount()\n.total(1000)\n.perColumnGenerator(generator().min(1).max(2), \"account_id\", \"name\")\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.count(\ncount\n.total(1000)\n.perColumnGenerator(generator.min(1).max(2), \"account_id\", \"name\")\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\ncount:\ntotal: 1000\nperColumn:\ncolumnNames:\n- \"account_id\"\n- \"name\"\ngenerator:\ntype: \"random\"\noptions:\nmax: 2\nmin: 1\n</code></pre>"},{"location":"setup/generator/generator/","title":"Data Generators","text":""},{"location":"setup/generator/generator/#data-types","title":"Data Types","text":"<p>Below is a list of all supported data types for generating data:</p> Data Type Spark Data Type Options Description string StringType <code>minLen, maxLen, expression, enableNull</code> integer IntegerType <code>min, max</code> long LongType <code>min, max</code> short ShortType <code>min, max</code> decimal(precision, scale) DecimalType(precision, scale) <code>min, max</code> double DoubleType <code>min, max</code> float FloatType <code>min, max</code> date DateType <code>min, max, enableNull</code> timestamp TimestampType <code>min, max, enableNull</code> boolean BooleanType binary BinaryType <code>minLen, maxLen, enableNull</code> byte ByteType array ArrayType <code>arrayMinLen, arrayMaxLen, arrayType</code> _ StructType Implicitly supported when a schema is defined for a field"},{"location":"setup/generator/generator/#options","title":"Options","text":""},{"location":"setup/generator/generator/#all-data-types","title":"All data types","text":"<p>Some options are available to use for all types of data generators. Below is the list along with example and descriptions:</p> Option Default Example Description <code>enableEdgeCase</code> false <code>enableEdgeCase: \"true\"</code> Enable/disable generated data to contain edge cases based on the data type. For example, integer data type has edge cases of (Int.MaxValue, Int.MinValue and 0) <code>edgeCaseProbability</code> 0.0 <code>edgeCaseProb: \"0.1\"</code> Probability of generating a random edge case value if <code>enableEdgeCase</code> is true <code>isUnique</code> false <code>isUnique: \"true\"</code> Enable/disable generated data to be unique for that column. Errors will be thrown when it is unable to generate unique data <code>seed</code> <code>seed: \"1\"</code> Defines the random seed for generating data for that particular column. It will override any seed defined at a global level <code>sql</code> <code>sql: \"CASE WHEN amount &lt; 10 THEN true ELSE false END\"</code> Define any SQL statement for generating that columns value. Computation occurs after all non-SQL fields are generated. This means any columns used in the SQL cannot be based on other SQL generated columns. Data type of generated value from SQL needs to match data type defined for the field"},{"location":"setup/generator/generator/#string","title":"String","text":"Option Default Example Description <code>minLen</code> 1 <code>minLen: \"2\"</code> Ensures that all generated strings have at least length <code>minLen</code> <code>maxLen</code> 10 <code>maxLen: \"15\"</code> Ensures that all generated strings have at most length <code>maxLen</code> <code>expression</code> <code>expression: \"#{Name.name}\"</code><code>expression:\"#{Address.city}/#{Demographic.maritalStatus}\"</code> Will generate a string based on the faker expression provided. All possible faker expressions can be found here Expression has to be in format <code>#{&lt;faker expression name&gt;}</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\", \"\u0130yi g\u00fcnler\", \"\u0421\u043f\u0430\u0441\u0438\u0431\u043e\", \"\u039a\u03b1\u03bb\u03b7\u03bc\u03ad\u03c1\u03b1\", \"\u0635\u0628\u0627\u062d \u0627\u0644\u062e\u064a\u0631\", \" F\u00f6rl\u00e5t\", \"\u4f60\u597d\u5417\", \"Nh\u00e0 v\u1ec7 sinh \u1edf \u0111\u00e2u\", \"\u3053\u3093\u306b\u3061\u306f\", \"\u0928\u092e\u0938\u094d\u0924\u0947\", \"\u0532\u0561\u0580\u0565\u0582\", \"\u0417\u0434\u0440\u0430\u0432\u0435\u0439\u0442\u0435\")</p>"},{"location":"setup/generator/generator/#sample","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield()\n.name(\"name\")\n.type(StringType.instance())\n.expression(\"#{Name.name}\")\n.enableNull(true)\n.nullProbability(0.1)\n.minLength(4)\n.maxLength(20)\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield\n.name(\"name\")\n.`type`(StringType)\n.expression(\"#{Name.name}\")\n.enableNull(true)\n.nullProbability(0.1)\n.minLength(4)\n.maxLength(20)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\ntype: \"csv\"\noptions:\npath: \"app/src/test/resources/sample/csv/transactions\"\nschema:\nfields:\n- name: \"name\"\ntype: \"string\"\ngenerator:\noptions:\nexpression: \"#{Name.name}\"\nenableNull: true\nnullProb: 0.1\nminLength: 4\nmaxLength: 20\n</code></pre>"},{"location":"setup/generator/generator/#numeric","title":"Numeric","text":"<p>For all the numeric data types, there are 4 options to choose from: min, max and maxValue. Generally speaking, you only need to define one of min or minValue, similarly with max or maxValue. The reason why there are 2 options for each is because of when metadata is automatically gathered, we gather the statistics of the observed min and max values. Also, it will attempt to gather any restriction on the min or max value as defined by the data source (i.e. max value as per database type).</p>"},{"location":"setup/generator/generator/#integerlongshort","title":"Integer/Long/Short","text":"Option Default Example Description <code>min</code> 0 <code>min: \"2\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> 1000 <code>max: \"25\"</code> Ensures that all generated values are less than or equal to <code>max</code> <p>Edge cases Integer: (2147483647, -2147483648, 0) Edge cases Long: (9223372036854775807, -9223372036854775808, 0) Edge cases Short: (32767, -32768, 0)</p>"},{"location":"setup/generator/generator/#sample_1","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"year\").type(IntegerType.instance()).min(2020).max(2023),\nfield().name(\"customer_id\").type(LongType.instance()),\nfield().name(\"customer_group\").type(ShortType.instance())\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"year\").`type`(IntegerType).min(2020).max(2023),\nfield.name(\"customer_id\").`type`(LongType),\nfield.name(\"customer_group\").`type`(ShortType)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"year\"\ntype: \"integer\"\ngenerator:\noptions:\nmin: 2020\nmax: 2023\n- name: \"customer_id\"\ntype: \"long\"\n- name: \"customer_group\"\ntype: \"short\"\n</code></pre>"},{"location":"setup/generator/generator/#decimal","title":"Decimal","text":"Option Default Example Description <code>min</code> 0 <code>min: \"2\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> 1000 <code>max: \"25\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>numericPrecision</code> 10 <code>precision: \"25\"</code> The maximum number of digits <code>numericScale</code> 0 <code>scale: \"25\"</code> The number of digits on the right side of the decimal point (has to be less than or equal to precision) <p>Edge cases Decimal: (9223372036854775807, -9223372036854775808, 0)</p>"},{"location":"setup/generator/generator/#sample_2","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"balance\").type(DecimalType.instance()).numericPrecision(10).numericScale(5)\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"balance\").`type`(DecimalType).numericPrecision(10).numericScale(5)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"balance\"\ntype: \"decimal\"\ngenerator:\noptions:\nprecision: 10\nscale: 5\n</code></pre>"},{"location":"setup/generator/generator/#doublefloat","title":"Double/Float","text":"Option Default Example Description <code>min</code> 0.0 <code>min: \"2.1\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> 1000.0 <code>max: \"25.9\"</code> Ensures that all generated values are less than or equal to <code>max</code> <p>Edge cases Double: (+infinity, 1.7976931348623157e+308, 4.9e-324, 0.0, -0.0, -1.7976931348623157e+308, -infinity, NaN) Edge cases Float: (+infinity, 3.4028235e+38, 1.4e-45, 0.0, -0.0, -3.4028235e+38, -infinity, NaN)</p>"},{"location":"setup/generator/generator/#sample_3","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"amount\").type(DoubleType.instance())\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"amount\").`type`(DoubleType)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"amount\"\ntype: \"double\"\n</code></pre>"},{"location":"setup/generator/generator/#date","title":"Date","text":"Option Default Example Description <code>min</code> now() - 365 days <code>min: \"2023-01-31\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> now() <code>max: \"2023-12-31\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (0001-01-01, 1582-10-15, 1970-01-01, 9999-12-31) (reference)</p>"},{"location":"setup/generator/generator/#sample_4","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"created_date\").type(DateType.instance()).min(java.sql.Date.valueOf(\"2020-01-01\"))\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"created_date\").`type`(DateType).min(java.sql.Date.valueOf(\"2020-01-01\"))\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"created_date\"\ntype: \"date\"\ngenerator:\noptions:\nmin: \"2020-01-01\"\n</code></pre>"},{"location":"setup/generator/generator/#timestamp","title":"Timestamp","text":"Option Default Example Description <code>min</code> now() - 365 days <code>min: \"2023-01-31 23:10:10\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> now() <code>max: \"2023-12-31 23:10:10\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (0001-01-01 00:00:00, 1582-10-15 23:59:59, 1970-01-01 00:00:00, 9999-12-31 23:59:59)</p>"},{"location":"setup/generator/generator/#sample_5","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"created_time\").type(TimestampType.instance()).min(java.sql.Timestamp.valueOf(\"2020-01-01 00:00:00\"))\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"created_time\").`type`(TimestampType).min(java.sql.Timestamp.valueOf(\"2020-01-01 00:00:00\"))\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"created_time\"\ntype: \"timestamp\"\ngenerator:\noptions:\nmin: \"2020-01-01 00:00:00\"\n</code></pre>"},{"location":"setup/generator/generator/#binary","title":"Binary","text":"Option Default Example Description <code>minLen</code> 1 <code>minLen: \"2\"</code> Ensures that all generated array of bytes have at least length <code>minLen</code> <code>maxLen</code> 20 <code>maxLen: \"15\"</code> Ensures that all generated array of bytes have at most length <code>maxLen</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\", -128, 127)</p>"},{"location":"setup/generator/generator/#sample_6","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"payload\").type(BinaryType.instance())\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"payload\").`type`(BinaryType)\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"payload\"\ntype: \"binary\"\n</code></pre>"},{"location":"setup/generator/generator/#array","title":"Array","text":"Option Default Example Description <code>arrayMinLen</code> 0 <code>arrayMinLen: \"2\"</code> Ensures that all generated arrays have at least length <code>arrayMinLen</code> <code>arrayMaxLen</code> 5 <code>arrayMaxLen: \"15\"</code> Ensures that all generated arrays have at most length <code>arrayMaxLen</code> <code>arrayType</code> <code>arrayType: \"double\"</code> Inner data type of the array. Optional when using Java/Scala API. Allows for nested data types to be defined like struct <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true"},{"location":"setup/generator/generator/#sample_7","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield().name(\"last_5_amounts\").type(ArrayType.instance()).arrayType(\"double\")\n);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n.schema(\nfield.name(\"last_5_amounts\").`type`(ArrayType).arrayType(\"double\")\n)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n- name: \"transactions\"\n...\nschema:\nfields:\n- name: \"last_5_amounts\"\ntype: \"array&lt;double&gt;\"\n</code></pre>"},{"location":"setup/generator/report/","title":"Report","text":"<p>Data Caterer can be configured to produce a report of the data generated to help users understand what was run, how much  data was generated, where it was generated, validation results and any associated metadata. </p>"},{"location":"setup/generator/report/#sample","title":"Sample","text":"<p>Once run, it will produce a report like this.</p>"},{"location":"setup/validation/validation/","title":"Validations","text":"<p>Validations can be used to run data checks after you have run the data generator or even as a standalone task. A report  summarising the success or failure of the validations is produced and can be examined for further investigation.</p> <p>Currently, SQL expression validations are supported (can see here  for reference what other expressions are valid), but will later be extended out to supported other validations such as  aggregates (group by account_number, sum of amounts should be greater than 100), ordering (transaction dates should be  in descending order), relationships (at least one transaction per account_number) or data profiling (how close produced  data profile is to expected data profile).</p>"},{"location":"setup/validation/validation/#sample","title":"Sample","text":"JavaScalaYAML <pre><code>validationConfig()\n.name(\"account_checks\")\n.description(\"Check account related fields have gone through system correctly\")\n.addValidations(\n\"accountJson\",                          #data source name\nMap.of(\"path\", \"sample/json/txn-gen\"),  #data source options\nvalidation().expr(\"amount &lt; 100\"),      #validations\nvalidation().expr(\"year == 2021\").errorThreshold(0.1),\nvalidation().expr(\"regexp_like(name, 'Peter .*')\").errorThreshold(200).description(\"Should be lots of Peters\")\n);\n</code></pre> <pre><code>validationConfig\n.name(\"account_checks\")\n.description(\"Check account related fields have gone through system correctly\")\n.addValidations(\n\"accountJson\",                        #data source name\nMap(\"path\" -&gt; \"sample/json/txn-gen\"), #data source options\nvalidation.expr(\"amount &lt; 100\"),      #validations\nvalidation.expr(\"year == 2021\").errorThreshold(0.1),\nvalidation.expr(\"regexp_like(name, 'Peter .*')\").errorThreshold(200).description(\"Should be lots of Peters\")\n)\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndescription: \"Check account related fields have gone through system correctly\"\ndataSources:\naccountJson:\noptions:\npath: \"sample/json/txn-gen\"\nvalidations:\n- expr: \"amount &lt; 100\"\n- expr: \"year == 2021\"\nerrorThreshold: 0.1   #equivalent to if error percentage is &gt;= 10%, then fail\n- expr: \"regexp_like(name, 'Peter .*')\"\nerrorThreshold: 200   #equivalent to if number of errors is &gt;= 200, then fail\ndescription: \"Should be lots of Peters\"\n</code></pre> <p>Once run, it will produce a report like this.</p>"},{"location":"use-case/business-value/","title":"Business Value","text":"<p>Below is a list of the business related benefits from using Data Caterer which may be applicable for your use case.</p> Problem Data Caterer Solution Resources Effects Reliable test data creation - Profile existing data- Create scenarios- Generate data Software Engineers, QA, Testers Cost reduction in labor, more time spent on development, more bugs caught before production Faster development cycles - Generate data in local, test, UAT, pre-prod- Run different scenarios Software Engineers, QA, Testers More defects caught in lower environments, features pushed to production faster, common framework used across all environments Data compliance - Profiling existing data- Generate based on metadata- No complex masking- No production data used in lower environments Audit and compliance No chance for production data breaches Storage costs - Delete generated data- Test specific scenarios Infrastructure Lower data storage costs, less time spent on data management and clean up Schema evolution - Create metadata from data sources- Generate data based off fresh metadata Software Engineers, QA, Testers Less time spent altering tests due to schema changes, ease of use between environments and application versions"},{"location":"use-case/use-case/","title":"Use cases","text":""},{"location":"use-case/use-case/#replicate-production-in-lower-environment","title":"Replicate production in lower environment","text":"<p>Having a stable and reliable test environment is a challenge for a number of companies, especially where teams are asynchronously deploying and testing changes at faster rates. Data Caterer can help alleviate these issues by doing the following:</p> <ol> <li>Generates data with the latest schema changes and production like field values</li> <li>Run as a job on a daily/regular basis to replicate production traffic or data flows</li> <li>Validate data to ensure your system runs as expected</li> <li>Clean up data to avoid build up of generated data</li> </ol> <p></p>"},{"location":"use-case/use-case/#local-development","title":"Local development","text":"<p>Similar to the above, being able to replicate production like data in your local environment can be key to developing more reliable code as you can test directly against data in your local computer. This has a number of benefits including:</p> <ol> <li>Fewer assumptions or ambiguities when the developer codes</li> <li>Direct feedback loop in local computer rather than waiting for test environment for more reliable test data</li> <li>No domain expertise required to understand the data</li> <li>Easy for new developers to be onboarded and developing/testing code for jobs/services</li> </ol>"},{"location":"use-case/use-case/#systemintegration-testing","title":"System/integration testing","text":"<p>When working with third-party, external or internal data providers, it can be difficult to have all setup ready to produce reliable data that abides by relationship contracts between each of the systems. You have to rely on these data providers in order for you to run your tests which may not align to their priorities. With Data Caterer, you can generate the same data that they would produce, along with maintaining referential integrity across the data providers, so that you can run your tests without relying on their systems being up and reliable in their corresponding lower environments.</p>"},{"location":"use-case/use-case/#scenario-testing","title":"Scenario testing","text":"<p>If you want to set up particular data scenarios, you can customise the generated data to fit your scenario. Once the data gets generated and is consumed, you can also run validations to ensure your system has consumed the data correctly. These scenarios can be put together from existing tasks or data sources can be enabled/disabled based on your requirement. Built into Data Caterer and controlled via feature flags, is the ability to test edge cases based on the data type of the fields used for data generation (<code>enableEdgeCases</code> flag within <code>&lt;field&gt;.generator.options</code>, see more here).</p>"},{"location":"use-case/use-case/#data-debugging","title":"Data debugging","text":"<p>When data related issues occur in production, it may be difficult to replicate in a lower or local environment. It could be related to specific fields not containing expected results, size of data is too large or missing corresponding referenced data. This becomes key to resolving the issue as you can directly code against the exact data scenario and have confidence that your code changes will fix the problem. Data Caterer can be used to generate the appropriate data in whichever environment you want to test your changes against.</p>"},{"location":"use-case/use-case/#data-profiling","title":"Data profiling","text":"<p>When using Data Caterer with the feature flag <code>enableGeneratePlanAndTasks</code> enabled (see here), metadata relating all the fields defined in the data sources you have configured will be generated via data profiling. You can run this as a standalone job (can disable <code>enableGenerateData</code>)  so that you can focus on the profile of the data you are utilising. This can be run against your production data sources  to ensure the metadata can be used to accurately generate data in other environments. This is a key feature of Data  Caterer as no direct production connections need to be maintained to generate data in other environments (which can  lead to serious concerns about data security as seen here).</p>"},{"location":"use-case/use-case/#schema-gathering","title":"Schema gathering","text":"<p>When using Data Caterer with the feature flag <code>enableGeneratePlanAndTasks</code> enabled (see here), all schemas of the data sources defined will be tracked in a common format (as tasks). This data, along with the data profiling metadata, could then feed back into your schema registries to help keep them up to date with your system.</p>"},{"location":"use-case/use-case/#comparison-to-similar-tools","title":"Comparison to similar tools","text":"Tool Description Features Pros Cons DBLDatagen Python based data generation tool - Scalable and predictable data generation across data scenarios- Plugin third-party libraries- Generate from existing data- Generate based on combination of other fields - Open source- Good documentation- Customisable and scalable- Generate from existing data/schemas - Limited support if issues- Code required- No clean up- No validation DataCebo Synthetic Data Vault Python based data generation tool with focus on ML generation, evaluating generated data - Create synthetic data using machine learning- Evaluate and visualize synthetic data- Preprocess, anonymize and define constraints Tonic Platform solution for generating data - Integration with many data sources- UI with RBAC- Quality and security checks- Auditing and alerting- Dashboards and reporting Datafaker Realistic data generation library - Generate realistic data- Push to CSV/JSON format- Create your own data providers- Performant - Simple, easy to use- Extensible- Open source- Realistic values - Have to code for and manage data source connections- No data clean up- No validation- No foreign keys support Gatling HTTP API load testing tool - Load testing- Validating data and responses- Scenario testing- Reporting- Extensive API support- Integration with CI/CD tools - Widely used- Clear documentation- Extensive testing/validation support- Customisable scenarios - Only supports HTTP, JMS and JDBC- No data clean up- Data feeders not based off metadata Tricentis - Data integrity Testing tool that focuses on data integrity - Data testing- Pre-screening data- Reconciliation, profiling and report testing- Support SQL DB, noSQL DB, files, API Broadcom - Test data manager Test data provisioning tool with PII detection and reusable datasets - Identify sensitive data- Generate synthetic data- Store and reuse existing data- Create virtual copies of data"}]}